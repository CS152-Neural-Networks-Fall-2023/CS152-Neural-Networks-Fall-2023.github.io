[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "In this course, we will introduce neural networks as a tool for machine learning and function approximation. We will start with the fundamentals of how to build and train a neural network from scratch. We will work our way towards discussing state-of-the-art neural networks including networks that power applications like object recognition, image generation and large language models. Throughout the course we will keep a strong focus on the implications of these models and how to apply them responsibly.\n\n\n\n\n\n\n\nProf. Gabe Hope (he/him)\nEmail: ghope@g.hmc.edu\nOffice: MacGregor 322\nCourse Slack: https://join.slack.com/t/slack-hox8054/shared_invite/zt-220qv4i92-wFrivrGQBQbtzy0fSElV~w\n\n\n\nYou can call me any combination of Prof./Professor/Dr. and Hope/Gabe. My full name is actually John Gabriel Hope.\nI am originally from New York City\nI have a Bachelor of Science (BS) in computer science and finance from Washington University in St. Louis.\nI have a Master of Science (MS) from Brown University (This was actually the start of my Ph.D.)\nI earned my Ph.D. from the University of California, Irvine advised by Erik Sudderth. I graduated just this June!\nMy research focuses on using neural networks to find interpretable structures in data. I mainly focus on image data, though I have also worked on analyzing motion-capture, audio and climate data among other things!\n\n\n\n\n\n\nMax McKnight (he/they)\nEmail: mmcknigh@pitzer.edu\n\n\n\n\nLinear and logistic regression\nGradient descent and optimization\nFeature transforms and feed-forward networks\nPerformance tuning and analysis for neural networks\nConvolutional neural networks\nRegularization and normalization\nResidual networks and large-scale architectures\nAttention and transformers\nBiases and fairness in machine learning\n\n\n\n\nProbabilistic Machine Learning by Kevin Murphy. Full text for book 1 and book 2 are available for free online.\n\n\n\nMondays 4-5:30pm MacGregor 322\n8/31 ONLY: Thursday 4-5:30pm MacGregor 322\nAdditional instructor and grutor office hours TBD\nOpen door policy: If my door is open, feel free to stop in, say hi and ask questions about the course, research or any other academic issues. If the door is closed, feel free to knock. I often like to close my door to focus, but it does not always mean I am unable to talk. If I don’t answer promptly I am either out of office or in a meeting and am unable to talk. If in doubt, feel free contact me on slack. Note that I generally prefer to keep longer discussion of course materials to designated office hours."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Course Syllabus",
    "section": "",
    "text": "Prof. Gabe Hope (he/him)\nEmail: ghope@g.hmc.edu\nOffice: MacGregor 322\nCourse Slack: https://join.slack.com/t/slack-hox8054/shared_invite/zt-220qv4i92-wFrivrGQBQbtzy0fSElV~w\n\n\n\nYou can call me any combination of Prof./Professor/Dr. and Hope/Gabe. My full name is actually John Gabriel Hope.\nI am originally from New York City\nI have a Bachelor of Science (BS) in computer science and finance from Washington University in St. Louis.\nI have a Master of Science (MS) from Brown University (This was actually the start of my Ph.D.)\nI earned my Ph.D. from the University of California, Irvine advised by Erik Sudderth. I graduated just this June!\nMy research focuses on using neural networks to find interpretable structures in data. I mainly focus on image data, though I have also worked on analyzing motion-capture, audio and climate data among other things!"
  },
  {
    "objectID": "index.html#grutor",
    "href": "index.html#grutor",
    "title": "Course Syllabus",
    "section": "",
    "text": "Max McKnight (he/they)\nEmail: mmcknigh@pitzer.edu"
  },
  {
    "objectID": "index.html#topics-covered-tentative",
    "href": "index.html#topics-covered-tentative",
    "title": "Course Syllabus",
    "section": "",
    "text": "Linear and logistic regression\nGradient descent and optimization\nFeature transforms and feed-forward networks\nPerformance tuning and analysis for neural networks\nConvolutional neural networks\nRegularization and normalization\nResidual networks and large-scale architectures\nAttention and transformers\nBiases and fairness in machine learning"
  },
  {
    "objectID": "index.html#textbook-not-required",
    "href": "index.html#textbook-not-required",
    "title": "Course Syllabus",
    "section": "",
    "text": "Probabilistic Machine Learning by Kevin Murphy. Full text for book 1 and book 2 are available for free online."
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Course Syllabus",
    "section": "",
    "text": "Mondays 4-5:30pm MacGregor 322\n8/31 ONLY: Thursday 4-5:30pm MacGregor 322\nAdditional instructor and grutor office hours TBD\nOpen door policy: If my door is open, feel free to stop in, say hi and ask questions about the course, research or any other academic issues. If the door is closed, feel free to knock. I often like to close my door to focus, but it does not always mean I am unable to talk. If I don’t answer promptly I am either out of office or in a meeting and am unable to talk. If in doubt, feel free contact me on slack. Note that I generally prefer to keep longer discussion of course materials to designated office hours."
  },
  {
    "objectID": "index.html#vscode-optional",
    "href": "index.html#vscode-optional",
    "title": "Course Syllabus",
    "section": "VSCode (Optional)",
    "text": "VSCode (Optional)\nVisual Studio Code is a free development environment developed by Microsoft. It is available for Mac, Windows and Linux, and provides convenient tools for working with Python, Git and Jupyter. It is what I use to develop the materials for this course, and it is what I would recommend using for homework assignments. This is completely optional however. You are welcome to use whatever environment you feel most comfortable with.\nHere are resources for getting started:\n\nRecommended extensions for data science and working with Jupyter notebooks are listed here.\nInstructions for setting up Python in VSCode are here.\nInstructions for working with Jupyter notebooks in VSCode are here.\nInstructions for setting up Git in VSCode are here."
  },
  {
    "objectID": "index.html#python",
    "href": "index.html#python",
    "title": "Course Syllabus",
    "section": "Python",
    "text": "Python\nAssignments and projects in this course will be based on Python 3. We will be using the following packages throughout this course:\n\nNumpy: The industry-standard Python library for working with vectors, matrices and general numerical computing.\nMatplotlib: The most popular and widely supported library for data visualization in Python.\nSciKit-Learn: A popular library for basic machine learning.\nPyTorch: A deep learning library. Currently the most popular library for neural networks research and competitive with TensorFlow in terms of industry deployment.\n\nYou can find this course’s requirements file here. It will also be included in homework distributions. You can install the full set of packages using the command:\npip install -r requirements.txt"
  },
  {
    "objectID": "index.html#jupyter",
    "href": "index.html#jupyter",
    "title": "Course Syllabus",
    "section": "Jupyter",
    "text": "Jupyter\nMost homework assignments will be distributed and submitted as Jupyter notebooks. Jupyterlab is included in the course requirements.txt file, but instructions for installing it are also available here. Once installed, you can launch a JupyterLab server locally on you computer using the command:\njupyter lab\nThis will open the Jupyter Lab application in a web browser. From there you can navigate to the homework’s main.ipynb file. Resources and documentation for working with Jupyter notebooks are available here."
  },
  {
    "objectID": "index.html#latex-style-equations",
    "href": "index.html#latex-style-equations",
    "title": "Course Syllabus",
    "section": "Latex (style) equations",
    "text": "Latex (style) equations\nHomework assignments will ask you to submit answers requiring mathematical derivations as typeset Latex-style equations. Latex equations are supported directly within Jupyter. To write an equation in a text/markdown cell, simply surround the equation with $ symbols as: $y = x^2 + 1$, which produces the output: \\(y=x^2 +1\\). You can write block equation using double dollar-signs as $$y = x^2 + 1$$, which puts the equation on its own centered line.\nAn extensive reference for Latex equations is available here.\nIn general, only the final answer to such problems will be required to be submitted in this way, intermediate steps in derivations can be submitted separately as handwritten notes. To do this, scan or photograph (clearly!) the handwritten derivations and include them as a derivations.pdf file in the homework repository. You may also include separate files for each question with the format derivations-q1.pdf. PNG and JPG files are also acceptable. You may also omit intermediate steps altogether, but this is not recommended as it may limit your ability to earn partial credit if your answer is incorrect."
  },
  {
    "objectID": "index.html#git-and-github",
    "href": "index.html#git-and-github",
    "title": "Course Syllabus",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nAssignments for this course will be distributed and collected via GitHub. You can sign up for a GitHub account at the link. If you have an existing GitHub account you may use it; you are not required to use an account associated with your Harvey Mudd (or 5Cs) email. As soon as possible please provide me with your Github username using this form.\nAfter the first class you will receive an email inviting you to a GitHub organization for this course. Please accept the invite as soon as possible. Once you have joined the organization, personal repositories for each homework will appear in your GitHub account under the organization (this link should take you to a list of your repositories for each homework, provided you are logged into GitHub).\nThe workflow for each homework should be as follows:\n\nClone the repository: You can find your repository for each homework using the link above. Running git clone REPOSITORY_URL will copy the repository to your computer. In VSCode, you can simply select Clone Git Repository...under the start menu.\nComplete the assignment: Typically the only file you will need to edit will be main.ipynb in each homework’s repository. This is a Jupyter notebook which will contain instructions for where to answer each question. Other supporting files may be distributed in the repository as well.\nCommit your changes: From the command line in the repository directory run the command: git add main.ipynbto stage the changes to made to the assignment with your answers. Then run the command git commit -m \"COMMIT_MESSAGE\" to save a checkpoint of your changes with the provided message. You can make as many commits as you want. In VSCode you can commit by navigating to the “source control” menu in the toolbar (typically on the left side of the window) and clicking the commit or check mark button.\nPush your changes: Before the deadline, submit your assignment by running git push origin main in the assignment directory. You may push updated solutions multiple times, but please do not push after the deadline. Your last push before the deadline will be considered your submission. Submitting after the deadline will cause issues for me and the gruders. In VSCode you can similarly push your assignment from the source control panel and selecting “push”."
  },
  {
    "objectID": "index.html#course-gpu-server",
    "href": "index.html#course-gpu-server",
    "title": "Course Syllabus",
    "section": "Course GPU Server",
    "text": "Course GPU Server\nWe have a GPU server for this course that will be available to you for your final projects. (Thank you to our system administrator Tim Buchheim for setting this up!). The server is located at teapot.cs.hmc.edu (named for the Utah teapot). We will discuss how to allocate resources on this machine at the start of the course project. You will not need GPU access for most homework assignments."
  },
  {
    "objectID": "index.html#homework-assignments-40-of-course-grade",
    "href": "index.html#homework-assignments-40-of-course-grade",
    "title": "Course Syllabus",
    "section": "Homework assignments (40% of course grade)",
    "text": "Homework assignments (40% of course grade)\nFrequency: Homeworks will be assigned on a weekly basis throughout the semester, with the exception of weeks where midterms and final projects are due.\nDue dates and late policy: Homeworks are assigned on Mondays and must be submitted by the end of the following Tuesday (11:59pm Tuesday). Late assignments will not be accepted, but the lowest 2 homework scores will be dropped unconditionally. For cases of illness, injury, family emergency or other extenuating circumstances, please contact me for exceptions to this policy.\nSubmission Homework assignments are submitted by pushing to the corresponding Github repository. For each homework, I will consider the last push before the submission deadline to be your submission."
  },
  {
    "objectID": "index.html#midterm-exam-20-of-course-grade",
    "href": "index.html#midterm-exam-20-of-course-grade",
    "title": "Course Syllabus",
    "section": "Midterm exam (20% of course grade)",
    "text": "Midterm exam (20% of course grade)\nThis course will have a single in-class midterm exam on Wednesday, November 8th. Details will be discussed closer to the exam date."
  },
  {
    "objectID": "index.html#participation-10-of-course-grade",
    "href": "index.html#participation-10-of-course-grade",
    "title": "Course Syllabus",
    "section": "Participation (10% of course grade)",
    "text": "Participation (10% of course grade)\nThis course is not generally a discussion-based class, however there will be certain lectures with open-ended discussions throughout the semester. The participation grade will be based on the following factors:\n\nParticipation in open-ended discussion sessions during class\nContriubting to the learning environment by asking or answering questions during regular lectures\nFollowing the guidelines for respectful discussion (as outlined in course policies)\nClass attendance\nAttending office hours outside class\n\nEarning a perfect participation grade will not require full marks for all of these criteria. A perfect participation grade will be earned by any student who: attends class regularly (&gt; 80% of the time) and participates respectfully in class at least every 1-2 weeks. Attending office hours is not strictly required, but if you are struggling to participate in class, I will assign bonus points to your participation grade for attending. If you have questions about your participation grade at any point, please contact me."
  },
  {
    "objectID": "index.html#final-project-30-of-course-grade",
    "href": "index.html#final-project-30-of-course-grade",
    "title": "Course Syllabus",
    "section": "Final Project (30% of course grade)",
    "text": "Final Project (30% of course grade)\nThe culmination of this course will be a final project completed in teams of 2-4 students. Full project description to follow. Your grade for the final project will depend on:\n\nThe strength of your team’s final presentation and write-up\nYour strength as a team-member (determined by self, peer and instructor evaluations)\nInitial project proposal\nMid-project check-ins\n\nStudents enter this class with highly varying backgrounds and prior experiences with neural networks, so I will help each team determine an appropriate scope for their project. Grades will be evaluated for each team individually based on how the team approached, analyzed and executed on the goals of the project. The relative technical sophistication of other teams projects will not be considered."
  },
  {
    "objectID": "index.html#letter-grade-assignments",
    "href": "index.html#letter-grade-assignments",
    "title": "Course Syllabus",
    "section": "Letter grade assignments",
    "text": "Letter grade assignments\nAs this course is still under active development I cannot yet guarantee exact cutoffs for each grade. Harvey Mudd does not impose expectations for the grade distribution, so every student that meets the requirements for a given grade will earn it. The following is the maximum cutoff of each letter grade, the actual cutoff for each grade may be lower that what is listed below:\n\n&gt;90%: A\n&gt;80%: B\n&gt;70%: C\n&gt;60%: D\n\nAs the semester progresses, I will update this guide to provide a clearer picture of how grades will be assigned."
  },
  {
    "objectID": "index.html#course-feedback",
    "href": "index.html#course-feedback",
    "title": "Course Syllabus",
    "section": "Course feedback",
    "text": "Course feedback\nThis is my first time teaching a college course, so I will need your help! I want to make sure that we go through the material at an appropriate pace and that concepts are presented in a clear and understandable way. To do this, I will be continuously soliciting feedback from you throughout the semester on both the lectures and assignments. I ask that you provide feedback honestly, but kindly. There are three mechanisms I will use for feedback:\nIn-class: In class we will use a thumbs-up, thumbs down system. When I ask if people are following the lecture you can put your thumb up to indicate that you feel you are understanding the material being presented, down to indicate that you are lost or the lecture is confusing, and sideways to indicate that you followed some parts, but not all. You are, of course, also encouraged to give verbal feedback if appropriate.\nWith homework: Each homework will include a link to a survey to give feedback on that week’s assignment and lectures. Submitting this form is a required part of the homework, but your answers will not be tracked or accounted for in grades. This gives you a chance to indicate any issues (or things you like) with the class.\nGeneral anonymous feedback: If you have an issue with the course that you would like me to be aware of, but do not want your name to be associated with, you can use this form to submit an anonymous issue. Please continue to remain constructive and kind when submitting feedback in this way."
  },
  {
    "objectID": "index.html#academic-issues-and-wellness",
    "href": "index.html#academic-issues-and-wellness",
    "title": "Course Syllabus",
    "section": "Academic issues and wellness",
    "text": "Academic issues and wellness\nMy primary goal is for every student to understand the material to the best extent possible and hopefully enjoy learning the material at the same time. If at any point you are concerned about your grade or feel you are struggling for any reason I encourage you to reach out to me either via slack/email or during office hours. I will also try to reach out to you if I notice you are struggling with the material or are not on track to pass the class.\nI also want you to prioritize your mental and physical well-being. The college has several resources that can help you with this. The academic deans (academicdeans@g.hmc.edu) can help you keep on top of your academic progress. The office of health and wellness (https://www.hmc.edu/health-wellness/) can help you with a wide range of physical and metal health issues. I encourage you to be proactive, if you are starting to feel anxious, overwhelmed or depressed seeking help early can be extremely valuable. If you are unsure where to go, I can help guide you to the appropriate resource. The Claremont Care Guide, provides a useful guide if you or someone you know is in urgent distress."
  },
  {
    "objectID": "index.html#accommodations",
    "href": "index.html#accommodations",
    "title": "Course Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\nIf you have a disability (for example, mental health, learning, chronic health, physical, hearing, vision, neurological, etc.) and expect barriers related to this course, it is important to request accommodations and establish a plan. Requests for accommodations must go through the Office of Accessible Education. I am happy to work with them to establish an appropriate plan for this course. I also encourage reaching out to them if you are unsure of your eligibility for accommodations, they can help determine what is appropriate for you.\nRemember that requests for accommodations must be made each semester. If you are not already registered this process can take time and accommodations cannot be applied retroactively, so starting the process early is important."
  },
  {
    "objectID": "index.html#attendence",
    "href": "index.html#attendence",
    "title": "Course Syllabus",
    "section": "Attendence",
    "text": "Attendence\nAttendence is strongly encouraged as it is beneficial for both your own learning and that of your peers who may learn from your knowledge and viewpoints. Not only is attendance reflected in your participation grade, it is also highly correlated with performance on exams and homework. That said, I understand that there are times where student may miss class for a variety of reasons. If you miss a class (or several) please contact me by email or slack and we can work out a plan to catch you up on the material. Up to 1 unexcused absence per month will not affect your participation grade, neither will excused absences due to illness, injury, etc."
  },
  {
    "objectID": "index.html#guidelines-for-respectful-class-discussion",
    "href": "index.html#guidelines-for-respectful-class-discussion",
    "title": "Course Syllabus",
    "section": "Guidelines for respectful class discussion",
    "text": "Guidelines for respectful class discussion\nThe goal of in-class discussions to understand each others perspectives and to contribute to both our own learning and that of our peers. To make sure that in-class discussions are aligned with these goals please be mindful of the following guidelines:\n\nAvoid judgment: Students enter this class with a variety of backgrounds, experience and viewpoints. Be positive and encouraging to your peers even if you feel they are incorrect. Strive to make sure those around you feel comfortable answering questions even if they are not completely sure of their answer and give opinions that they are not sure others will agree with. Remember that giving an answer different from what the instructor was looking for can lead to productive and informative discussions.\nAllow everyone a chance to speak: We want to give every student a chance to participate in the class and in discussions. If you find yourself speaking, answering or asking questions far more than your peers, consider encouraging others to speak instead. Remember that in-class time is not your only opportunity to discuss this material and you are welcome to ask more questions in office hours.\nPractice active listening: When having in-class discussions make sure to acknowledge the answers and opinions of others before offering your own. Avoid interrupting others. Your thoughts deserve to be heard and understood, so it’s important that we work together to make sure everyone’s contributions are considered.\nBe kind: Do not use harsh or disparaging language in class. Avoid blame or speculation about other students. Aim to be charitable in your interpretations of other peoples words. Respect the boundaries set by others.\nBe inclusive: Be respectful of everyone’s background and identity. Avoid making assumptions or generalizations based on someone’s (perceived) social group. Do not ask individuals to speak for their social group."
  },
  {
    "objectID": "index.html#collaboration-policy",
    "href": "index.html#collaboration-policy",
    "title": "Course Syllabus",
    "section": "Collaboration policy",
    "text": "Collaboration policy\nYou are encouraged to discuss homework assignments with other students, but the final work that you submit for each assignment must be still be your own. This means you may:\n\nDiscuss published course materials and topics\nDiscuss approaches to problems with other students, including while working on the assignments\nShare helpful examples and resources with other students\nHelp other students with technical issues such as setting up GitHub and Python environments.\nView another student’s code for the purpose of debugging small technical issues (exceptions, syntax errors etc.)\n\nYou may not:\n\nCopy/paste another student’s answers to any problem or allow another student to copy/paste your answers\nShare answers to completed problems with other students\nDistribute or post online any assignments, problems and/or solutions.\n\nThis collaboration policy is covered by the Harvey Mudd honor code and violations will be referred to the honor code board.\nEach homework will have space for you to indicate who you discussed the assignment with. If you would like help finding other students to study with, please let me know and I can work to set you up with a study group."
  },
  {
    "objectID": "index.html#ai-policy",
    "href": "index.html#ai-policy",
    "title": "Course Syllabus",
    "section": "AI Policy",
    "text": "AI Policy\nIn this course we will be learning the fundamental tools for building large language models and chat AIs, such as ChatGPT. Therefore I encourage you to experiment with ChatGPT and it’s competitors during this course. However, I consider these models to be covered by the above collaboration policy. That means you may interact with them to discuss course materials, but you may not share assignment problems with them, nor may you copy/paste answers from them. If you have any questions about what is appropriate, please reach out to me."
  },
  {
    "objectID": "index.html#covid-safety",
    "href": "index.html#covid-safety",
    "title": "Course Syllabus",
    "section": "COVID Safety",
    "text": "COVID Safety\nCollege policy states that masks are no longer required indoors for the upcoming semester. I will not require masks in class, but students who prefer to continue wearing masks are should do so. If you are feeling sick please stay home and let me know so that I can provide you with the relevant course materials."
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "Course Syllabus",
    "section": "Courses",
    "text": "Courses\n\nfastai (website)\nDeep Learning Specialization (Coursera MOOC)\nDeep Learning (Stanford CS230 course)\nConvolutional Neural Networks for Visual Recognition (Stanford CS231n course)\nIntroduction to Deep Learning (MIT 6.S191 course)\nMIT Deep Learning and Artificial Intelligence Lectures (MIT course)\nDeep Reinforcement Learning (Berkeley CS 285 course)\nDeep Reinforcement Learning (free online course)\nDeep Learning Systems"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "Course Syllabus",
    "section": "Books",
    "text": "Books\n\nDive into Deep Learning (UC Berkeley book)\nDeep Learning (free book)\nFirst steps towards Deep Learning with PyTorch (free book)\nNeural Networks and Deep Learning (free book)\nDeep Learning With PyTorch (pdf)\nAnnotated Algorithms in Python (free book)\nLearn Python the Right way (free book)\nThe Linux Command Line by William Shotts"
  },
  {
    "objectID": "index.html#math",
    "href": "index.html#math",
    "title": "Course Syllabus",
    "section": "Math",
    "text": "Math\n\nThe Matrix Calculus You Need For Deep Learning (website)\nThe Mechanics of Machine Learning (free book)\nMathematics for Machine Learning (free book)\nSeeing Theory: A Visual Introduction To Probability And Statistics (free book)"
  },
  {
    "objectID": "index.html#extras",
    "href": "index.html#extras",
    "title": "Course Syllabus",
    "section": "Extras",
    "text": "Extras\n\nCheatsheet (website)\nTensorFlow 2.0 Complete Course - Python Neural Networks for Beginners Tutorial (YouTube)\nNeural Networks (3blue1brown YouTube)\nMachine Learning From Scratch (website)\nA visual introduction to machine learning (website)"
  },
  {
    "objectID": "index.html#python-1",
    "href": "index.html#python-1",
    "title": "Course Syllabus",
    "section": "Python",
    "text": "Python\n\nGoogle’s Python Class\nIntroduction to Python | Microsoft Learn\nList of free free Python books\nPython Programming Tutorials\nLearn Python - Full Course for Beginners (YouTube)\nPython In 1 Minute (YouTube)\nAutomate the Boring Stuff With Python (book)\nIntroduction to Python Programming (free course)\nA Whirlwind Tour of Python (Jupyter Notebooks)\nPython for Everybody Specialization (free course)\nIntroduction to Computer Science and Programming Using Python (MIT course)"
  },
  {
    "objectID": "index.html#ethics",
    "href": "index.html#ethics",
    "title": "Course Syllabus",
    "section": "Ethics",
    "text": "Ethics\n\nAwful AI\nLearning from the past to create Responsible AI\nPractical Data Ethics\nFair ML Book\nMachine Ethics Podcast\nACM Code of Ethics and Professional Conduct\nIEEE Code of Ethics\nCode of Conduct for Professional Data Scientists"
  },
  {
    "objectID": "index.html#librariesframeworkstools",
    "href": "index.html#librariesframeworkstools",
    "title": "Course Syllabus",
    "section": "Libraries/Frameworks/Tools",
    "text": "Libraries/Frameworks/Tools\n\nMlxtend (machine learning extensions)\nStreamlit (Turn data scripts into sharable web apps in minutes)\nDeepnote (The notebook you’ll love to use)"
  },
  {
    "objectID": "misc/hw1-hint.html",
    "href": "misc/hw1-hint.html",
    "title": "Hint for homework 1: Q7",
    "section": "",
    "text": "For this question we are interested in simplifying an expression into matrix/vector notation. In order to do this it may be first helpful to think about how we went the other direction: matrix/vector notation to expanded notation.\nRecall that a dot product between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), can be written explicitly from its definition as:\n\\[\n\\mathbf{x}^T\\mathbf{y} = \\sum_{i=1}^n x_iy_i\n\\]\nTherefore if we see something like the summation on the right in an expression, we can replace it with the more compact dot product notation.\nIf we have the expression \\(\\mathbf{A} \\mathbf{x}\\), where \\(\\mathbf{A}\\) is a matrix and \\(\\mathbf{x}\\) is a vector, we know that the result of this multiplication will be a vector. Let’s call this vector \\(\\mathbf{c}\\), so that \\(\\mathbf{A}\\mathbf{x}=\\mathbf{c}\\).\nWe know from the definition of matrix-multiplication that each element of \\(\\mathbf{c}\\) can be written as the following summation:\n\\[\nc_i = \\sum_{j=1}^n A_{ij}x_j\n\\]\nTherefore, if we saw such a summation in an expression, we could temporarily replace it with \\(c_i\\), knowing that we defined \\(\\mathbf{c}\\) as \\(\\mathbf{c}=\\mathbf{A}\\mathbf{x}\\). Try doing this as the first step in the homework, then try repeating this idea until you have something that you can write compactly in matrix/vector notation.\nWhen writing you final answer make sure to write the final expression in terms of the original variables \\((\\mathbf{A}, \\mathbf{x}, \\mathbf{b})\\). E.g. if you substituted \\(\\mathbf{c}\\) for \\(\\mathbf{A}\\mathbf{x}\\), make sure to substitute it back in the answer."
  },
  {
    "objectID": "calendar/calendar.html",
    "href": "calendar/calendar.html",
    "title": "Course Calendar",
    "section": "",
    "text": "Lecture\nDate\nTopics\nTextbook\nMaterials\nHomework\n\n\n\n\n1\n8/28\nCourse introduction, linear algebra and calculus review\nBook 1: 7.1.1-7.2.4, 7.8.1, 7.8.2\nNotes\nSlides\nHomework 1 assigned (Github) Due: 9/5 11:59pm\n\n\n2\n8/30\nLinear regression\nBook 1: 4.2.1, 4.2.2, 11.1-11.2.4\nNotes\nSlides\n\n\n\n\n9/4\nNo class (Labor day)\n\n\n\n\n\n3\n9/6\n\n\n\nHomework 2 assigned (Github) Due: 9/12 11:59pm\n\n\n4\n9/11\n\n\n\n\n\n\n5\n9/13\n\n\n\nHomework 3 assigned (Github) Due: 9/19 11:59pm\n\n\n6\n9/18\n\n\n\n\n\n\n7\n9/20\n\n\n\nHomework 4 assigned (Github) Due: 9/26 11:59pm\n\n\n8\n9/25\n\n\n\n\n\n\n9\n9/27\n\n\n\nHomework 5 assigned (Github) Due: 10/3 11:59pm\n\n\n10\n10/2\n\n\n\n\n\n\n11\n10/4\n\n\n\nHomework 6 assigned (Github) Due: 10/10 11:59pm\n\n\n12\n10/9\n\n\n\n\n\n\n13\n10/11\n\n\n\nHomework 7 assigned (Github) Due: 10/19 11:59pm\n\n\n\n10/16\nNo class (Fall break)\n\n\n\n\n\n14\n10/18\n\n\n\nHomework 8 assigned (Github) Due: 10/24 11:59pm\n\n\n15\n10/23\n\n\n\n\n\n\n16\n10/25\n\n\n\nHomework 9 assigned (Github) Due: 9/5 11:59pm\n\n\n17\n10/30\n\n\n\n\n\n\n18\n11/1\n\n\n\nHomework 10 assigned (Github) Due: 11/7 11:59pm\n\n\n19\n11/6\n\n\n\n\n\n\n\n11/8\n\n\n\n\n\n\n20\n11/13\n\n\n\n\n\n\n21\n11/15\n\n\n\nHomework 11 assigned (Github) Due: 11/20 11:59pm\n\n\n22\n11/20\n\n\n\n\n\n\n\n11/22\nNo class (Thanksgiving break)\n\n\nHomework 12 assigned (Github) Due: 12/5 11:59pm\n\n\n23\n11/27\n\n\n\n\n\n\n24\n11/29\n\n\n\n\n\n\n25\n12/5\n\n\n\n\n\n\n26\n12/6"
  },
  {
    "objectID": "plugins/drawer/Readme.html",
    "href": "plugins/drawer/Readme.html",
    "title": "RevealJS drawer plugin (3.2KB gzipped)",
    "section": "",
    "text": "Allows you to draw over your slides. Drawings are saved per slide and kept when slide is changed. Demo. Works with RevealJS Pointer Plugin.\n\nThis plugin only works with RevealJS v4.x or higher.\n\nNo external dependencies, only 7.5KB | &lt;3.2KB gzipped.\n\n\n\nCopy dist/drawer.js into plugins/drawer/drawer.js and import script:\n[...]\n&lt;script src=\"plugin/drawer/drawer.js\"&gt;&lt;/script&gt;\n[...]\nCopy dist/drawer.css into plugins/drawer/drawer.css and import style in &lt;head&gt;&lt;/head&gt;:\n[...]\n&lt;link rel=\"stylesheet\" href=\"plugin/drawer/drawer.css\" /&gt;\n[...]\nAdd RevealDrawer into your plugins initialization:\nplugins: [RevealDrawer];\n\n\n\n\n\nT - toggle drawing board\nD - toggle mode (drawing or not drawing)\nCtrl + Z - remove last line from current slide\n\"1\", \"2\", \"3\", \"4\" - change selected color (base on the order)\n\nIf you’re not changing anything in the Config then you should be able to show drawing board just by hitting T. By default the drawing is enabled.\n\nIf you hit D then drawing mode is toggled and it is going to switch to disabled mode (the pen icon is grayed out).\n\nIn drawing mode you’re not able to interact with other elements (like code) because it would disturb your drawing. That’s why switching between drawing and not drawing mode is important.\nEach time you draw sth, it is saved for this particular slide (slide includes all fragments). You can switch between slides and have a different drawing on each one. Ctrl + Z is available if you make a mistake in your drawing. It also works per slide even if you’re coming back from the different slide.\nYou’re able to change between colors using color icons or numbers on the keyboard. Each color has a number assigned to it and if you have 4 colors then numbers 1,2,3,4 on your keyboard are responsible for switching between them (default option). If you change default colors then numbers are assigned to new ones (base on how many colors you have). E.g. you’ve decided to have simpler colors, so your list looks like ['#FF0000', '#00FF00', '#0000FF'], now only 1,2,3 keys are available.\n\n\n\n\nYou can configure drawer key and tail length in plugin config.\nReveal.initialize({\n  drawer: {\n    toggleDrawKey: \"d\", // (optional) key to enable drawing, default \"d\"\n    toggleBoardKey: \"t\", // (optional) key to show drawing board, default \"t\"\n    colors: [\"#fa1e0e\", \"#8ac926\", \"#1982c4\", \"#ffca3a\"], // (optional) list of colors avaiable (hex color codes)\n    color: \"#FF0000\", // (optional) color of a cursor, first color from `codes` is a default\n    pathSize: 4, // (optional) path size in px, default 4\n  }\n})\nList of available keys:\n\n[“0”, “1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “backspace”, “tab”, “enter”, “shift”, “ctrl”, “alt”, “pausebreak”, “capslock”, “esc”, “space”, “pageup”, “pagedown”, “end”, “home”, “leftarrow”, “uparrow”, “rightarrow”, “downarrow”, “insert”, “delete”, “a”, “b”, “c”, “d”, “e”, “f”, “g”, “h”, “i”, “j”, “k”, “l”, “m”, “n”, “o”, “p”, “q”, “r”, “s”, “t”, “u”, “v”, “w”, “x”, “y”, “z”, “leftwindowkey”, “rightwindowkey”, “selectkey”, “numpad0”, “numpad1”, “numpad2”, “numpad3”, “numpad4”, “numpad5”, “numpad6”, “numpad7”, “numpad8”, “numpad9”, “multiply”, “add”, “subtract”, “decimalpoint”, “divide”, “f1”, “f2”, “f3”, “f4”, “f5”, “f6”, “f7”, “f8”, “f9”, “f10”, “f11”, “f12”, “numlock”, “scrolllock”, “semicolon”, “equalsign”, “comma”, “dash”, “period”, “forwardslash”, “graveaccent”, “openbracket”, “backslash”, “closebracket”, “singlequote”]\n\n\n\n\n\nMake changes in src/plugin.js and run:\nnpm run build\nThis is going to produce dist/drawer.js with bundled iife file."
  },
  {
    "objectID": "plugins/drawer/Readme.html#installation",
    "href": "plugins/drawer/Readme.html#installation",
    "title": "RevealJS drawer plugin (3.2KB gzipped)",
    "section": "",
    "text": "Copy dist/drawer.js into plugins/drawer/drawer.js and import script:\n[...]\n&lt;script src=\"plugin/drawer/drawer.js\"&gt;&lt;/script&gt;\n[...]\nCopy dist/drawer.css into plugins/drawer/drawer.css and import style in &lt;head&gt;&lt;/head&gt;:\n[...]\n&lt;link rel=\"stylesheet\" href=\"plugin/drawer/drawer.css\" /&gt;\n[...]\nAdd RevealDrawer into your plugins initialization:\nplugins: [RevealDrawer];\n\n\n\n\n\nT - toggle drawing board\nD - toggle mode (drawing or not drawing)\nCtrl + Z - remove last line from current slide\n\"1\", \"2\", \"3\", \"4\" - change selected color (base on the order)\n\nIf you’re not changing anything in the Config then you should be able to show drawing board just by hitting T. By default the drawing is enabled.\n\nIf you hit D then drawing mode is toggled and it is going to switch to disabled mode (the pen icon is grayed out).\n\nIn drawing mode you’re not able to interact with other elements (like code) because it would disturb your drawing. That’s why switching between drawing and not drawing mode is important.\nEach time you draw sth, it is saved for this particular slide (slide includes all fragments). You can switch between slides and have a different drawing on each one. Ctrl + Z is available if you make a mistake in your drawing. It also works per slide even if you’re coming back from the different slide.\nYou’re able to change between colors using color icons or numbers on the keyboard. Each color has a number assigned to it and if you have 4 colors then numbers 1,2,3,4 on your keyboard are responsible for switching between them (default option). If you change default colors then numbers are assigned to new ones (base on how many colors you have). E.g. you’ve decided to have simpler colors, so your list looks like ['#FF0000', '#00FF00', '#0000FF'], now only 1,2,3 keys are available.\n\n\n\n\nYou can configure drawer key and tail length in plugin config.\nReveal.initialize({\n  drawer: {\n    toggleDrawKey: \"d\", // (optional) key to enable drawing, default \"d\"\n    toggleBoardKey: \"t\", // (optional) key to show drawing board, default \"t\"\n    colors: [\"#fa1e0e\", \"#8ac926\", \"#1982c4\", \"#ffca3a\"], // (optional) list of colors avaiable (hex color codes)\n    color: \"#FF0000\", // (optional) color of a cursor, first color from `codes` is a default\n    pathSize: 4, // (optional) path size in px, default 4\n  }\n})\nList of available keys:\n\n[“0”, “1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”, “backspace”, “tab”, “enter”, “shift”, “ctrl”, “alt”, “pausebreak”, “capslock”, “esc”, “space”, “pageup”, “pagedown”, “end”, “home”, “leftarrow”, “uparrow”, “rightarrow”, “downarrow”, “insert”, “delete”, “a”, “b”, “c”, “d”, “e”, “f”, “g”, “h”, “i”, “j”, “k”, “l”, “m”, “n”, “o”, “p”, “q”, “r”, “s”, “t”, “u”, “v”, “w”, “x”, “y”, “z”, “leftwindowkey”, “rightwindowkey”, “selectkey”, “numpad0”, “numpad1”, “numpad2”, “numpad3”, “numpad4”, “numpad5”, “numpad6”, “numpad7”, “numpad8”, “numpad9”, “multiply”, “add”, “subtract”, “decimalpoint”, “divide”, “f1”, “f2”, “f3”, “f4”, “f5”, “f6”, “f7”, “f8”, “f9”, “f10”, “f11”, “f12”, “numlock”, “scrolllock”, “semicolon”, “equalsign”, “comma”, “dash”, “period”, “forwardslash”, “graveaccent”, “openbracket”, “backslash”, “closebracket”, “singlequote”]"
  },
  {
    "objectID": "plugins/drawer/Readme.html#developing",
    "href": "plugins/drawer/Readme.html#developing",
    "title": "RevealJS drawer plugin (3.2KB gzipped)",
    "section": "",
    "text": "Make changes in src/plugin.js and run:\nnpm run build\nThis is going to produce dist/drawer.js with bundled iife file."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html",
    "href": "lecture2-linear-regression/notes.html",
    "title": "Lecture 2: Linear regression",
    "section": "",
    "text": "Manim Community v0.17.3"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#functions-revisited",
    "href": "lecture2-linear-regression/notes.html#functions-revisited",
    "title": "Lecture 2: Linear regression",
    "section": "Functions revisited",
    "text": "Functions revisited\nIn the previous lecture we reviewed the concept of a function, which is a mapping from a set of possible inputs to a corresponding set of outputs. Here we’ll consider functions with vector inputs and scalar outputs.\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n\\]\nMathematically, we can easily definite a function using a sequence of basic operations.\n\n\n\n\n\nThis function gives us the relationship between inputs \\(\\mathbf{x}\\) and outputs \\(f(\\mathbf{x})\\). That is, for any given input \\(x\\), we can find the corresponding output \\(y\\) by applying our function \\(f(\\mathbf{x})\\)."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-functions",
    "href": "lecture2-linear-regression/notes.html#linear-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nA linear function is any function \\(f\\) where the following conditions always hold: \\[ f(\\mathbf{x} + \\mathbf{y}) =f(\\mathbf{x}) + f(\\mathbf{y})\\] and \\[ f(a\\mathbf{x}) = a f(\\mathbf{x})\\] For a linear function, the output can be defined as a weighted sum of the inputs. In other words a linear function of a vector can always be written as:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i + b\n\\]\nHere, \\(w_i\\) and \\(b\\) are constants that define the particular linear function. We will refer to these as the parameters of the function. We can also write this using a dot-product between our input \\(\\mathbf{x}\\) and parameter vector \\(\\mathbf{w}\\) as:\n\\[\nf(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b \\quad \\text{or} \\quad f(\\mathbf{x}) = \\mathbf{x}^T  \\mathbf{w} + b\n\\]\nWe typically refer to \\(\\mathbf{w}\\) specifically as the weight vector (or weights) and \\(b\\) as the bias. To summarize:\n\\[\n\\textbf{Linear function:  }f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}+b,\\quad \\textbf{Parameters:}\\quad \\big(\\text{Weights:  } \\mathbf{w},\\ \\text{Bias:  } b \\big)\n\\]\nIn one dimension, a linear function is always a line, for example:\n\n\n\n\n\nIn higher dimensions, it is a plane or hyperplane:\n\n\n\n\n\nIn numpy we can easily write a linear function of this form:\n\ndef f(x):\n    w = np.array([-0.6, -0.2])\n    b = -1\n    return np.dot(x, w) + b"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#handling-bias-compactly",
    "href": "lecture2-linear-regression/notes.html#handling-bias-compactly",
    "title": "Lecture 2: Linear regression",
    "section": "Handling bias compactly",
    "text": "Handling bias compactly\nNotationally, it can be tedious to always write the bias term. A common approach to compactly describing linear functions is to use augmented inputs and weights, such that for \\(\\mathbf{x}\\) and \\(\\mathbf{w}\\in \\mathbb{R}^n\\), we add \\(x_{n+1}=1\\) and \\(w_{n+1}=b\\). So:\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\longrightarrow \\mathbf{x}_{aug}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\longrightarrow \\mathbf{w}_{aug}=  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ b \\end{bmatrix}\n\\]\nWe can easily see then that using this notation:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{w} +b = \\mathbf{x}_{aug}^T \\mathbf{w}_{aug}\n\\]\nThis approach is common enough that we typically won’t bother with the \\(aug\\) notation and just assume that any linear function defined as \\(f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w}\\) can be defined to include a bias implicitly.\nIn numpy this is similarly straightforward:\n\ndef f(x):\n    w = np.array([-0.6, -0.2, -1])\n    x = np.pad(x, ((0,1),), constant_values=1)\n    return np.dot(x, w)"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#datasets-and-observations",
    "href": "lecture2-linear-regression/notes.html#datasets-and-observations",
    "title": "Lecture 2: Linear regression",
    "section": "Datasets and observations",
    "text": "Datasets and observations\nIn the real-world we often have access to inputs and outputs in the form of data, but not to an actual function that we can evaluate.\nSpecifically we will say that we have access to a dataset \\(\\mathcal{D}\\) made up of \\(N\\) pairs of inputs ( \\(\\mathbf{x}\\) ) and outputs ( \\(y\\) ):\n\\[\n\\mathcal{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\}\n\\]\nWe call each of these pairs an observation. Let’s take a look at a real world example of a dataset.\n\nFuel efficiency\nLet’s imagine we’re designing a car and we would like to know what the fuel efficiency of the car we’re designing will be in miles per gallon (MPG). We know some properties of our current design, such as the weight and horsepower, that we know should affect the efficiency. Ideally we would have access to a function that would give us the MPG rating if we provide these features.\n\\[\n\\text{mpg} = f(\\text{weight},\\ \\text{horsepower}...)\n\\]\nUnfortunately we don’t know the exact relationship between a car’s features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 33mpg}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 21mpg}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]\nOur dataset will be this collection of data that we have for all other cars. In general, each observation in this dataset will correspond to a car.\n\\[\n\\text{Dataset: } \\mathcal{D}=\\{(\\mathbf{x}_i,\\ y_i) \\text{  for  } i\\in 1...N\\}\n\\]\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = MPG\n\\]\nJust as with a known function, we can plot the inputs vs the outputs, however in this case, we only know the outputs for the inputs we’ve seen in our dataset. Let’s take a look at a single feature: the weight of a car."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#prediction-functions",
    "href": "lecture2-linear-regression/notes.html#prediction-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Prediction functions",
    "text": "Prediction functions\nOur dataset gives us a set of known inputs and outputs for our unknown functions. The central question we will address in this course is then:\n\nHow do we predict the output for an input that we haven’t seen before?\nFor example, in our car scenario, we might know that the car that we’re designing will weigh 3100 lbs. In our dataset we’ve haven’t seen a car that weighs exactly 3100 lbs, so we need a way to predict the output of the function at input 3100 lbs.\n\n\n\n\n\nIn general, our approach to this problem will be to model our unknown function with a known function that we can evaluate at any input. We want to chose a function \\(f\\) such that for any observation our dataset, the output of this function approximates the true target output that we observed.\n\\[\nf(\\mathbf{x}_i) \\approx y_i, \\quad \\forall (\\mathbf{x}_i, y_i) \\in \\mathcal{D}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-interpolation",
    "href": "lecture2-linear-regression/notes.html#linear-interpolation",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation\nOne reasonable approach we might consider is linear interpolation. In this approach, we simply connect all the neighboring points with straight lines:\n\n\n\n\n\nIn some cases this can be a reasonable approach! In fact it’s how the plt.plot function works. Real data however tends to be messy. The measurements in our dataset might not be 100% accurate or they might even conflict! What do we do if we have two observations with the same input and different outputs?\n\\[(\\text{Weight: }3100, \\text{MPG: } 34), \\quad (\\text{Weight: }3100, \\text{MPG: } 23) \\longrightarrow f(3100) = ?\\]\nAs the size and number of features in our inputs gets larger, this become even more complex. We can see this if we try to apply interpolation to a much larger MPG dataset:"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-regression-1",
    "href": "lecture2-linear-regression/notes.html#linear-regression-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression is the approach of modeling an unknown function with a linear function. From our discussion of linear functions, we know that this means that we will make predictions using a function of the form:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n\\]\nMeaning that the output will be a weighted sum of the features of the input. In the case of our car example, we will make predictions as:\n\\[\n\\text{Predicted MPG} = f(\\mathbf{x})=\n\\]\n\\[\n(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n\\]\nOr in matrix notation:\n\\[\nf(\\mathbf{x})= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix}\n\\]\nWe see that under this approach each weight \\((w_1, w_2…)\\) tells us how much our prediction changes as we change the corresponding feature. For example, if we were to increase the weight of our car by 1 lb, the predicted MPG would change by \\(w_1\\).\nThe set of weights defines the particular linear regression function. In numpy we can define a generic class for linear regression:\n\nclass Regression:\n    def __init__(self, weights):\n        self.weights = weights\n    \n    def predict(self, x):\n        return np.dot(x, self.weights)\n\nmodel = Regression(np.array([1, 1, 1, 1, 1]))\nmodel.predict(np.array([5, 2, 3, 3, 1]))\n\n14\n\n\nIf we again look at our plot of weight vs. MPG, we see we could chose many different linear functions to make predictions:"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#residuals-and-error",
    "href": "lecture2-linear-regression/notes.html#residuals-and-error",
    "title": "Lecture 2: Linear regression",
    "section": "Residuals and error",
    "text": "Residuals and error\nThe residual or error of a prediction is the difference between the prediction and the true output:\n\\[\ne_i = y_i - f(\\mathbf{x}_i)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#mean-squared-error",
    "href": "lecture2-linear-regression/notes.html#mean-squared-error",
    "title": "Lecture 2: Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nIn deciding what linear function to use, we need a measure of error for the entire dataset. A computationally convenient measure is mean squared error (MSE). The mean squared error is the averaged of the error squared for each observation in our dataset:\n\\[\nMSE = \\frac{1}{N}\\sum_{i=1}^N (f(\\mathbf{x}_i) - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]It follows that the best choice of linear function \\(f^*\\) is the one that minimizes the mean squared error for our dataset. Since each linear function is defined by a parameter vector \\(\\mathbf{w}\\), this is equivalent to finding \\(\\mathbf{w}^*\\), the parameters vector that minimizes the mean squared error. \\[\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#loss-functions",
    "href": "lecture2-linear-regression/notes.html#loss-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Loss functions",
    "text": "Loss functions\nNote that the mean squared error depends on the data inputs \\((\\mathbf{x}_1,…,\\mathbf{x}_N)\\), the data targets \\((y_1,…,y_N)\\) and the parameters \\((\\mathbf{w})\\). So we can express the MSE as a function of all three:\n\\[\nMSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nHere we have used \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to refer to the entire collection of inputs and outputs from our dataset \\((\\mathcal{D})\\) respectively, so:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n}\\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nn} \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n\\]\nThis is an example of loss function, for our given dataset this function tells us how much error (loss) we are incurring for a given choice of \\(\\mathbf{w}\\). If we assume our dataset is fixed we can drop the explicit dependence on \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), looking at the loss as purely a function of our choice of parameters:\n\\[\n\\textbf{Loss}(\\mathbf{w})= MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nAgain, if our goal is to minimize error, we want to choose the parameters \\(\\mathbf{w}^*\\) that minimize this loss:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{Loss}(\\mathbf{w})= \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#visualizing-loss",
    "href": "lecture2-linear-regression/notes.html#visualizing-loss",
    "title": "Lecture 2: Linear regression",
    "section": "Visualizing loss",
    "text": "Visualizing loss\nIf we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector \\(\\mathbf{w}\\) only has 2 entries: \\(w_1\\) and \\(b\\). In this case, we can actually plot our loss function directly!\n\n\n\n\n\n\n\n\n\n\nWe see that point where the loss is lowest, corresponds to the line that best fits our data!"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#gradient-descent",
    "href": "lecture2-linear-regression/notes.html#gradient-descent",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nNow that we have a way to determine the quality of a choice of parameters \\(\\mathbf{w}\\), using our loss function, we need a way to actually find the \\(\\mathbf{w}^*\\) that minimizes our loss. To do this we will turn to an algorithm called gradient descent. In this lecture we will introduce gradient descent, but we will go into much more depth in a future lecture.\nWe’ll introduce gradient descent as a method to find the minimum of a generic function. We have some function \\(f(\\mathbf{\\cdot})\\) and we would like find the input \\(\\mathbf{w}^*\\) that minimizes the output of the function:\n\\[\n\\text{Find: } \\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ f(\\mathbf{w})\n\\]\nWe don’t know how to find \\(\\mathbf{w}^*\\) directly, but if we have an initial guess \\(\\mathbf{w}^{(0)}\\), we can try to update our guess to improve it.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} + \\mathbf{g}\n\\]\n\n\nAutograd ArrayBox with value [[-5.0059363  -4.47709947 -2.89058898 -4.09935888 -3.04168522 -2.36175215\n  -2.52795801 -2.13510779]\n [-4.33221987 -3.80338304 -2.21687255 -3.42564245 -2.36796879 -1.68803572\n  -1.85424158 -1.46139136]\n [-4.04994516 -3.52110833 -1.93459784 -3.14336774 -2.08569408 -1.40576101\n  -1.57196687 -1.17911665]\n [-3.58364606 -3.05480923 -1.46829873 -2.67706863 -1.61939497 -0.9394619\n  -1.10566776 -0.71281755]\n [-3.13138271 -2.60254588 -1.01603539 -2.22480529 -1.16713163 -0.48719856\n  -0.65340442 -0.2605542 ]\n [-2.54188083 -2.013044   -0.42653351 -1.63530341 -0.57762975  0.10230332\n  -0.06390254  0.32894768]\n [-1.87596204 -1.34712521  0.23938528 -0.96938462  0.08828904  0.76822211\n   0.60201625  0.99486647]\n [-0.98703064 -0.45819381  1.12831668 -0.08045321  0.97722045  1.65715351\n   1.49094765  1.88379787]]\n\n\n\n\n\nHere we are changing \\(\\mathbf{w}^{(0)}\\) by moving in the direction of \\(\\mathbf{g}\\). If we recall that the gradient of a function at point \\(\\mathbf{x}\\) corresponds to the slope of \\(f\\) at \\(\\mathbf{w}\\), or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} - \\nabla f(\\mathbf{w}^{(0)})\n\\]\nNote that because the gradient corresponds to the direction that maximally increases \\(f(\\mathbf{w})\\), we actually need to subtract the gradient in order to minimize our function. We can repeat this process many times, continuously updating our estimate.\n\\[\n\\text{For }i \\text{ in 1,...,T}\\text{ :} \\\\\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#gradient-descent-convergence",
    "href": "lecture2-linear-regression/notes.html#gradient-descent-convergence",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent convergence",
    "text": "Gradient descent convergence\nRecall that it’s minimum value \\(\\mathbf{w}^*\\), a function \\(f\\) must have a gradient of \\(\\mathbf{0}\\).\n\\[\n\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\n\\]\nIt follows that:\n\\[\n\\mathbf{w}^{*} = \\mathbf{w}^{*} - \\nabla f(\\mathbf{w}^{*})\n\\]\nThis means that if our gradient descent reaches the minimum, it will stop updating the guess and we know that we can stop our iteration. So we could write our algorithm to account for this:\n\\[\n\\text{While } \\nabla f(\\mathbf{w}^{(i)}) \\neq \\mathbf{0} \\text{ :} \\\\\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]\nIn practice though, it could take infinitely many updates to find the exact minimum. A more common approach is to define a convergence criteria that stops the iteration when the gradient magnitude is sufficiently small:\n\\[\n\\text{While } ||\\nabla f(\\mathbf{w}^{(i)})||_2 &gt; \\epsilon \\text{ :} \\\\\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#step-sizes",
    "href": "lecture2-linear-regression/notes.html#step-sizes",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nNotice that the gradient descent algorithm we’ve defined so far not only says that we want to update our guess in the direction of the gradient, it also say that we want to move in that direction a distance equal to the magnitude of the gradient. It turns out this is often a very bad idea!\nThis approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa. This is desirable for many functions as it means when we’re far from the minimum we take large steps, moving toward the minimum more quickly. While when we’re close to the minimum we take small steps to refine our guess more precisely.\n\n\n\n\n\nHowever, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to divergence, where gradient descent overshoots the minimum more and more at each step.\n\n\n\n\n\nRemember that the gradient is making a linear approximation to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be.\n\n\n\n\n\nAlso remember that the gradient gives us the direction of maximum change of the function, but this is only true in the limit of a very small step.\n\\[\n\\frac{df}{d\\mathbf{w}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{w} + \\mathbf{\\epsilon}) - f(\\mathbf{w})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]\nSo in higher dimensions, the gradient may not point directly to the minimum.\nAll of these issues motivate the need to control the size of our updates. We will typically do this by introducing an additional control to our algorithm: a step size or learning rate. This is a small constant \\(\\alpha\\), that we will multiply the gradient by in each of our updates.\n\\[\n\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}^{(i)})\n\\]\nUsing a small learning rate \\((\\alpha &lt;&lt; 1)\\) will make gradient descent slower, but much more reliable. Later on in the semester we will explore how to choose \\(\\alpha\\) (and even update it during optimization)."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#optimizing-linear-regression",
    "href": "lecture2-linear-regression/notes.html#optimizing-linear-regression",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression",
    "text": "Optimizing linear regression\nWe can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss. To do this we need to find the gradient of the mean squared error with respect to the parameters:\n\\[\n\\nabla_{\\mathbf{w}} \\textbf{MSE}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\bigg)\n\\]\n\\[\n= \\frac{2}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWith this gradient our gradient descent update becomes:\n\\[\n\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha\\bigg(\\frac{2 }{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w}^{(i)} - y_i)\\mathbf{x}_i\n\\]\nWe can see that this update is a sum of all the inputs weighted by their corresponding residual given the current value of the parameters.\nWe can see how the the parameters of our regression model change as we run gradient descent."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#optimizing-linear-regression-directly",
    "href": "lecture2-linear-regression/notes.html#optimizing-linear-regression-directly",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\nGradient descent works well for finding the optimal parameters for a linear regression model, but in fact we can actually find the optimal set of parameters directly, without needing to run an iterative algorithm.\nWe know that at the minimum, the gradient must be \\(\\mathbf{0}\\), so the following condition must hold:\n\\[\n\\mathbf{0} = \\bigg( \\frac{2}{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWe can solve for a \\(\\mathbf{w}\\) that satisfied this condition by first dropping the constant \\(\\frac{2}{N}\\).\n\\[\n\\mathbf{0} = \\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\n\\[\n\\mathbf{0} = \\sum_{i=1}^N \\big( \\mathbf{x}_i\\mathbf{x}_i^T\\mathbf{w} - y_i \\mathbf{x}_i \\big)\n\\]\n\\[\n\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w}\n\\]\nNote that \\(\\mathbf{x}_i \\mathbf{x}_i^T\\) is a vector outer product:\n\\[\n\\mathbf{x}_i \\mathbf{x}_i^T = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\  x_{in}\\end{bmatrix} \\begin{bmatrix} x_{i1} & x_{i2} & \\dots &  x_{in}\\end{bmatrix} =\n\\begin{bmatrix} x_{i1} x_{i1} & x_{i1} x_{i2} & \\dots & x_{i1} x_{in} \\\\\nx_{i2} x_{i1} & x_{i2} x_{i2} & \\dots & x_{i2} x_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{in} x_{i1} & x_{in} x_{i2} & \\dots & x_{in} x_{in} \\\\\n\\end{bmatrix}\n\\]\nThus \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)\\) is a matrix. Multiplying both sides by the inverse \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1}\\) we get:\n\\[\n\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^*\n\\]\nWe can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.\n\\[\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} \\\\ \\mathbf{x}_{2} \\\\ \\vdots \\\\  \\mathbf{x}_{N} \\end{bmatrix},\\quad \\mathbf{y} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\  y_{N} \\end{bmatrix}\n\\]\nIn this case, the expression becomes:\n\\[\\mathbf{w}^* = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\big(\\mathbf{y}\\mathbf{X}\\big)\\]"
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#normal-distributions",
    "href": "lecture2-linear-regression/notes.html#normal-distributions",
    "title": "Lecture 2: Linear regression",
    "section": "Normal distributions",
    "text": "Normal distributions\nThe Normal distribution (also known as the Gaussian distribution) is a continuous probability distribution with the following probability density function:\n\\[\np(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n\\]\nThe normal distribution shows up almost everywhere in probability and statistics. Most notably, the central limit theorem tells us that the mean of many independent and identically distributed random outcomes tends towards a normal distribution."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#linear-regression-as-a-probabilistic-model",
    "href": "lecture2-linear-regression/notes.html#linear-regression-as-a-probabilistic-model",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression as a probabilistic model",
    "text": "Linear regression as a probabilistic model\nOur function approximation view of linear regression says that we can approximate an unknown function with a linear function. An alternate approach is to define a distribution over the output \\(y_i\\) of our unknown function given an input \\(\\mathbf{x}_i\\). In particular, the probabilistic model for linear regression will make the assumption that the output is normally distributed conditioned on the input:\n\\[\ny_i \\sim \\mathcal{N}\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n\\]\nHere we see the assumption we’re making is that the mean of the distribution is a linear function of the input, while the variance is fixed. Under this model, we can write the conditional probability or likelihood of an output as:\n\\[\np(y_i\\mid\\mathbf{x}_i, \\mathbf{w}) =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\n\\]\nWhy view linear regression as a probabilistic model? Well, generally for real data we can’t know if there actually is a function that perfectly maps inputs to outputs. It could be that there are variables we’re not accounting for, that there errors in our measurements for the data we collected or simply that there is some inherent randomness in the outputs. This view of linear regression makes the uncertainty in our predictions explicit."
  },
  {
    "objectID": "lecture2-linear-regression/notes.html#maximum-likelihood-estimation-1",
    "href": "lecture2-linear-regression/notes.html#maximum-likelihood-estimation-1",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWith this view of linear regression in mind, let’s ask again how we find the optimal value for \\(\\mathbf{w}\\). Possibly the most widely used approach to this is to simply choose the \\(\\mathbf{w}\\) that maximizes the likelihood (conditional probability) of all of the outputs in our dataset:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) \\]\nGenerally our model also assumes conditional independence across observations so:\n\\[\np(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\nFor convenience, it is typical to frame the optimal value in terms of the negative log-likelihood rather than the likelihood, but the two are equivalent.\n\\[\n\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nWe see that the negative log-likelihood is a natural loss function to optimize to find \\(\\mathbf{w}^*\\).\n\\[\n\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]\nWe can write out the negative log-likelihood explicitly using the normal PDF:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N\\log\\bigg[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\\bigg]\n\\]\n\\[\n= \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n\\]\nWe see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.\n\\[\n\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}} \\bigg)\n\\]\n\\[\n= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nAs we saw in the MSE case, the optimal value \\(\\mathbf{w}^*\\) does not depend on the constant value outside the summation. This means that the optimal value for \\(\\mathbf{w}\\) is the same for both MSE and negative log-likelihood and the optimal value does not depend on \\(\\sigma^2\\)!\n\\[\n\\underset{\\mathbf{w}}{\\text{argmin}}\\  MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html",
    "href": "lecture2-linear-regression/slides.html",
    "title": "Lecture 2: Linear regression",
    "section": "",
    "text": "Manim Community v0.17.3\n\n\n\n\n\n\n\nHomework 1 due next Tuesday 9/5 at 11:59pm\nFill out Github username survey!\nOffice hours tomorrow 4-5:30pm in MacGregor 322\nCourse calendar added to website with lecture notes.\nLecture notes are open source!"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#logistics-1",
    "href": "lecture2-linear-regression/slides.html#logistics-1",
    "title": "Lecture 2: Linear regression",
    "section": "Logistics",
    "text": "Logistics\n\nHomework 1 due next Tuesday 9/5 at 11:59pm\nFill out Github username survey!\nOffice hours tomorrow 4-5:30pm in MacGregor 322\nCourse calendar added to website with lecture notes.\nLecture notes are open source!"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#functions-revisited",
    "href": "lecture2-linear-regression/slides.html#functions-revisited",
    "title": "Lecture 2: Linear regression",
    "section": "Functions revisited",
    "text": "Functions revisited\n\\[\ny=f(\\mathbf{x}), \\quad \\text{Input: } \\mathbf{x} \\in\\mathbb{R}^n \\longrightarrow \\text{ Output: }y \\in\\mathbb{R}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions",
    "href": "lecture2-linear-regression/slides.html#linear-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nA linear function is any function \\(f\\) where the following conditions always hold: \\[ f(\\mathbf{x} + \\mathbf{y}) =f(\\mathbf{x}) + f(\\mathbf{y})\\] and \\[ f(a\\mathbf{x}) = a f(\\mathbf{x})\\] For a linear function, the output can be defined as a weighted sum of the inputs.\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_iw_i + b\n\\]\nHere, \\(w_i\\) and \\(b\\) are the parameters of the function."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-1",
    "href": "lecture2-linear-regression/slides.html#linear-functions-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nWe can also write a linear function using a dot-product between our input \\(\\mathbf{x}\\) and parameter vector \\(\\mathbf{w}\\) as:\n\\[\nf(\\mathbf{x}) = \\mathbf{x} \\cdot \\mathbf{w} + b \\quad \\text{or} \\quad f(\\mathbf{x}) = \\mathbf{x}^T  \\mathbf{w} + b\n\\]\nWe typically refer to \\(\\mathbf{w}\\) specifically as the weight vector (or weights) and \\(b\\) as the bias.\n\\[\n\\textbf{Linear function:  }f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w}+b,\\quad \\textbf{Parameters:}\\quad \\big(\\text{Weights:  } \\mathbf{w},\\ \\text{Bias:  } b \\big)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-2",
    "href": "lecture2-linear-regression/slides.html#linear-functions-2",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nIn one dimension, a linear function is always a line, for example:"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-3",
    "href": "lecture2-linear-regression/slides.html#linear-functions-3",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nIn higher dimensions, it is a plane or hyperplane:"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-functions-4",
    "href": "lecture2-linear-regression/slides.html#linear-functions-4",
    "title": "Lecture 2: Linear regression",
    "section": "Linear Functions",
    "text": "Linear Functions\nIn numpy we can easily write a linear function of this form:\n\ndef f(x):\n    w = np.array([-0.6, -0.2])\n    b = -1\n    return np.dot(x, w) + b"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#handling-bias-compactly",
    "href": "lecture2-linear-regression/slides.html#handling-bias-compactly",
    "title": "Lecture 2: Linear regression",
    "section": "Handling bias compactly",
    "text": "Handling bias compactly\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\longrightarrow \\mathbf{x}_{aug}= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\\\ 1 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} \\longrightarrow \\mathbf{w}_{aug}=  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\\\ b \\end{bmatrix}\n\\]\nWe can easily see then that using this notation:\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{w} +b = \\mathbf{x}_{aug}^T \\mathbf{w}_{aug}\n\\]\nWe won’t bother with the \\(aug\\) notation and just assume that any linear function defined as \\(f(\\mathbf{x})=\\mathbf{x}^T\\mathbf{w}\\) can be defined to include a bias implicitly.\nIn numpy this is similarly straightforward:\n\ndef f(x):\n    w = np.array([-0.6, -0.2, -1])\n    x = np.pad(x, ((0,1),), constant_values=1)\n    return np.dot(x, w)"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#datasets-and-observations",
    "href": "lecture2-linear-regression/slides.html#datasets-and-observations",
    "title": "Lecture 2: Linear regression",
    "section": "Datasets and observations",
    "text": "Datasets and observations\nDataset \\(\\mathbf{D}\\) made up of \\(N\\) pairs of inputs ( \\(\\mathbf{x}\\) ) and outputs ( \\(y\\) ):\n\\[\n\\mathbf{D} = \\{ (\\mathbf{x}_1, y_1),\\ (\\mathbf{x}_2, y_2),\\ ...\\ (\\mathbf{x}_N, y_N)\\}\n\\]\nWe call each of these pairs an observation."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#fuel-efficiency",
    "href": "lecture2-linear-regression/slides.html#fuel-efficiency",
    "title": "Lecture 2: Linear regression",
    "section": "Fuel efficiency",
    "text": "Fuel efficiency\nLet’s imagine we’re designing a car and we would like to know what the fuel efficiency of the car we’re designing will be in miles per gallon (MPG). Ideally we would have access to a function that would give us the MPG rating if we provide some features.\n\\[\n\\text{mpg} = f(\\text{weight},\\ \\text{horsepower}...)\n\\]\nWe don’t know the exact relationship between a car’s features and fuel efficiency. However, we can look at other cars on the market and see what the corresponding inputs and outputs would be:\n\\[\n\\text{Honda Accord: } \\begin{bmatrix} \\text{Weight:} & \\text{2500 lbs} \\\\ \\text{Horsepower:} & \\text{ 123 HP} \\\\ \\text{Displacement:} & \\text{ 2.4 L} \\\\ \\text{0-60mph:} & \\text{ 7.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 33mpg}\n\\]\n\\[\n\\text{Dodge Aspen: } \\begin{bmatrix} \\text{Weight:} & \\text{3800 lbs} \\\\ \\text{Horsepower:} & \\text{ 155 HP} \\\\ \\text{Displacement:} & \\text{ 3.2 L} \\\\ \\text{0-60mph:} & \\text{ 6.8 Sec} \\end{bmatrix} \\longrightarrow \\text{   MPG: 21mpg}\n\\]\n\\[\n\\vdots \\quad \\vdots\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#fuel-efficiency-1",
    "href": "lecture2-linear-regression/slides.html#fuel-efficiency-1",
    "title": "Lecture 2: Linear regression",
    "section": "Fuel efficiency",
    "text": "Fuel efficiency\nOur dataset will be this collection of data that we have for all other cars. In general, each observation in this dataset will correspond to a car.\n\\[\n\\text{Dataset: } \\mathbf{D}=\\{(\\mathbf{x}_i,\\ y_i) \\text{  for  } i\\in 1...N\\}\n\\]\n\\[\n\\text{Input: } \\mathbf{x}_i= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph}  \\end{bmatrix}, \\quad \\text{Output: } y_i = MPG\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#fuel-efficiency-2",
    "href": "lecture2-linear-regression/slides.html#fuel-efficiency-2",
    "title": "Lecture 2: Linear regression",
    "section": "Fuel efficiency",
    "text": "Fuel efficiency\nLet’s take a look at a single feature: the weight of a car."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#prediction-functions",
    "href": "lecture2-linear-regression/slides.html#prediction-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Prediction functions",
    "text": "Prediction functions\nHow do we predict the output for an input that we haven’t seen before?"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#prediction-functions-1",
    "href": "lecture2-linear-regression/slides.html#prediction-functions-1",
    "title": "Lecture 2: Linear regression",
    "section": "Prediction functions",
    "text": "Prediction functions\nModel our unknown function with a known function that we can evaluate at any input. Chose a function \\(f\\) such that for any observation our dataset, the output of this function approximates the true target output that we observed.\n\\[\nf(\\mathbf{x}_i) \\approx y_i, \\quad \\forall (\\mathbf{x}_i, y_i) \\in \\mathbf{D}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-interpolation",
    "href": "lecture2-linear-regression/slides.html#linear-interpolation",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation\n\nIn some cases this can be a reasonable approach! In fact it’s how the plt.plot function works."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-interpolation-1",
    "href": "lecture2-linear-regression/slides.html#linear-interpolation-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation\nReal data however is messy. Measurements in our dataset might not be 100% accurate or might even conflict!\n\\[(\\text{Weight: }3100, \\text{MPG: } 34), \\quad (\\text{Weight: }3100, \\text{MPG: } 23) \\longrightarrow f(3100) = ?\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-interpolation-2",
    "href": "lecture2-linear-regression/slides.html#linear-interpolation-2",
    "title": "Lecture 2: Linear regression",
    "section": "Linear interpolation",
    "text": "Linear interpolation"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-1",
    "href": "lecture2-linear-regression/slides.html#linear-regression-1",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression models an unknown function with a linear function.\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{w} = \\sum_{i=1}^n x_i w_i\n\\]\nMeaning that the output will be a weighted sum of the features of the input.\n\\[\n\\text{Predicted MPG} = f(\\mathbf{x})=\n\\]\n\\[\n(\\text{weight})w_1 + (\\text{horsepower})w_2 + (\\text{displacement})w_3 + (\\text{0-60mph})w_4 + b\n\\]\nOr in matrix notation:\n\\[\nf(\\mathbf{x})= \\begin{bmatrix} \\text{Weight} \\\\ \\text{Horsepower} \\\\ \\text{Displacement} \\\\ \\text{0-60mph} \\\\ 1 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2\\\\ w_3 \\\\ w_4\\\\ b\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-2",
    "href": "lecture2-linear-regression/slides.html#linear-regression-2",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nclass Regression:\n    def __init__(self, weights):\n        self.weights = weights\n    \n    def predict(self, x):\n        return np.dot(x, self.weights)\n\nmodel = Regression(np.array([1, 1, 1, 1, 1]))\nmodel.predict(np.array([5, 2, 3, 3, 1]))\n\n14"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-3",
    "href": "lecture2-linear-regression/slides.html#linear-regression-3",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression",
    "text": "Linear regression\nWe could chose many different linear functions to make predictions:"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#residuals-and-error",
    "href": "lecture2-linear-regression/slides.html#residuals-and-error",
    "title": "Lecture 2: Linear regression",
    "section": "Residuals and error",
    "text": "Residuals and error\nThe residual or error of a prediction is the difference between the prediction and the true output:\n\\[\ne_i = y_i - f(\\mathbf{x}_i)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#mean-squared-error",
    "href": "lecture2-linear-regression/slides.html#mean-squared-error",
    "title": "Lecture 2: Linear regression",
    "section": "Mean squared error",
    "text": "Mean squared error\nWe need a measure of error for the entire dataset. The mean squared error is the average of the residual squared for each observation in our dataset:\n\\[\nMSE = \\frac{1}{N}\\sum_{i=1}^N (f(\\mathbf{x}_i) - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]It follows that the best choice of linear function \\(f^*\\) is the one that minimizes the mean squared error for our dataset. \\[\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#loss-functions",
    "href": "lecture2-linear-regression/slides.html#loss-functions",
    "title": "Lecture 2: Linear regression",
    "section": "Loss functions",
    "text": "Loss functions\nMean squared error depends on the data inputs \\((\\mathbf{x}_1,…,\\mathbf{x}_N)\\), the data targets \\((y_1,…,y_N)\\) and the parameters \\((\\mathbf{w})\\). So we can express the MSE as a function of all three:\n\\[\nMSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nHere we have used \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to refer to the entire collection of inputs and outputs from our dataset \\(( \\mathbf{D})\\) respectively, so:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\\\ \\vdots \\\\ \\mathbf{x}_N \\end{bmatrix} = \\begin{bmatrix}x_{11} & x_{12} & \\dots & x_{1n} \\\\ x_{21} & x_{22} & \\dots & x_{2n}\\\\ \\vdots & \\vdots  & \\ddots & \\vdots \\\\ x_{N1} & x_{N2} & \\dots & x_{Nn} \\end{bmatrix}, \\quad \\mathbf{y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#loss-functions-1",
    "href": "lecture2-linear-regression/slides.html#loss-functions-1",
    "title": "Lecture 2: Linear regression",
    "section": "Loss functions",
    "text": "Loss functions\nThis is an example of loss function. We can drop the explicit dependence on \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\), looking at the loss as purely a function of our choice of parameters:\n\\[\n\\textbf{Loss}(\\mathbf{w})= MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]\nAgain, if our goal is to minimize error, we want to choose the parameters \\(\\mathbf{w}^*\\) that minimize this loss:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{Loss}(\\mathbf{w})= \\underset{\\mathbf{w}}{\\text{argmin}} \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#visualizing-loss",
    "href": "lecture2-linear-regression/slides.html#visualizing-loss",
    "title": "Lecture 2: Linear regression",
    "section": "Visualizing loss",
    "text": "Visualizing loss\nIf we consider the case where our inputs are 1-dimensional, as in the weight example above, then our parameter vector \\(\\mathbf{w}\\) only has 2 entries: \\(w_1\\) and \\(b\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that point where the loss is lowest, corresponds to the line that best fits our data!"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent",
    "href": "lecture2-linear-regression/slides.html#gradient-descent",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe have a function \\(f(\\mathbf{\\cdot})\\) and we would like find the input \\(\\mathbf{w}^*\\) that minimizes the output of the function:\n\\[\n\\text{Find: } \\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmin}}\\ f(\\mathbf{w})\n\\]\nWe don’t know how to find \\(\\mathbf{w}^*\\) directly, but if we have an initial guess \\(\\mathbf{w}^{(0)}\\), we can try to update our guess to improve it.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} + \\mathbf{g}\n\\]\n\n\n[ 0.5 -0.6]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-1",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-1",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nThe gradient of a function at point \\(\\mathbf{x}\\) corresponds to the slope of \\(f\\) at \\(\\mathbf{w}\\), or equivalently the direction of maximum change. This gives us a natural choice for the update to our guess.\n\\[\n\\mathbf{w}^{(1)} \\leftarrow \\mathbf{w}^{(0)} - \\nabla f(\\mathbf{w}^{(0)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-2",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-2",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent",
    "text": "Gradient descent\nWe can repeat this process many times, continuously updating our estimate.\n\\[\n\\text{For }i \\text{ in 1,...,T}\\text{ :}\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-convergence",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-convergence",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent convergence",
    "text": "Gradient descent convergence\nAt it’s minimum value \\(\\mathbf{w}^*\\), a function \\(f\\) must have a gradient of \\(\\mathbf{0}\\).\n\\[\n\\nabla f(\\mathbf{w}^*) = \\mathbf{0}\n\\]\nIt follows that:\n\\[\n\\mathbf{w}^{*} = \\mathbf{w}^{*} - \\nabla f(\\mathbf{w}^{*})\n\\]\nWe could write our algorithm to account for this:\n\\[\n\\text{While } \\nabla f(\\mathbf{w}^{(i)}) \\neq \\mathbf{0} \\text{ :}\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#gradient-descent-convergence-1",
    "href": "lecture2-linear-regression/slides.html#gradient-descent-convergence-1",
    "title": "Lecture 2: Linear regression",
    "section": "Gradient descent convergence",
    "text": "Gradient descent convergence\nStops the iteration when the gradient magnitude is sufficiently small:\n\\[\n\\text{While } ||\\nabla f(\\mathbf{w}^{(i)})||_2 &gt; \\epsilon \\text{ :}\n\\quad \\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\nabla f(\\mathbf{w}^{(i)})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes",
    "href": "lecture2-linear-regression/slides.html#step-sizes",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nThis approach says that when the magnitude of the gradient is large, we should take a large step, and vice-versa."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-1",
    "href": "lecture2-linear-regression/slides.html#step-sizes-1",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nHowever, if we take too large a step, we can overshoot the minimum entirely! In the worst case, this can lead to divergence, where gradient descent overshoots the minimum more and more at each step."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-2",
    "href": "lecture2-linear-regression/slides.html#step-sizes-2",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nThe gradient is making a linear approximation to the function. A strait line has no minimum, so the gradient has no information about where along the approximation the true minimum will be."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-3",
    "href": "lecture2-linear-regression/slides.html#step-sizes-3",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nThe gradient gives us the direction of maximum change of the function, but this is only true in the limit of a very small step.\n\\[\n\\frac{df}{d\\mathbf{w}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{w} + \\mathbf{\\epsilon}) - f(\\mathbf{w})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]\nIn higher dimensions, the gradient may not point directly to the minimum."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#step-sizes-4",
    "href": "lecture2-linear-regression/slides.html#step-sizes-4",
    "title": "Lecture 2: Linear regression",
    "section": "Step sizes",
    "text": "Step sizes\nWe can introduce an additional control to our algorithm: a step size or learning rate. This is a small constant \\(\\alpha\\), that we will multiply the gradient by in each of our updates.\n\\[\n\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\alpha \\nabla f(\\mathbf{w}^{(i)})\n\\]\nUsing a small learning rate \\((\\alpha &lt;&lt; 1)\\) will make gradient descent slower, but much more reliable."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression",
    "text": "Optimizing linear regression\nWe can apply gradient descent to linear regression in order to find the parameters that minimize the mean squared error loss.\n\\[\n\\nabla_{\\mathbf{w}} \\textbf{MSE}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)^2 \\bigg)\n\\]\n\\[\n= \\frac{2}{N}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWith this gradient our gradient descent update becomes:\n\\[\n\\mathbf{w}^{(i+1)} \\longleftarrow \\mathbf{w}^{(i)} - \\alpha\\bigg(\\frac{2 }{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w}^{(i)} - y_i)\\mathbf{x}_i\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\nWe know that at the minimum, the gradient must be \\(\\mathbf{0}\\), so the following condition must hold:\n\\[\n\\mathbf{0} = \\bigg( \\frac{2}{N}\\bigg)\\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nWe can solve for a \\(\\mathbf{w}\\) that satisfied this condition by first dropping the constant \\(\\frac{2}{N}\\).\n\\[\n\\mathbf{0} = \\sum_{i=1}^N  (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\n\\[\n\\mathbf{0} = \\sum_{i=1}^N \\big( \\mathbf{x}_i\\mathbf{x}_i^T\\mathbf{w} - y_i \\mathbf{x}_i \\big)\n\\]\n\\[\n\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-1",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-1",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\nNote that \\(\\mathbf{x}_i \\mathbf{x}_i^T\\) is a vector outer product:\n\\[\n\\mathbf{x}_i \\mathbf{x}_i^T = \\begin{bmatrix} x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\  x_{in}\\end{bmatrix} \\begin{bmatrix} x_{i1} & x_{i2} & \\dots &  x_{in}\\end{bmatrix} =\n\\begin{bmatrix} x_{i1} x_{i1} & x_{i1} x_{i2} & \\dots & x_{i1} x_{in} \\\\\nx_{i2} x_{i1} & x_{i2} x_{i2} & \\dots & x_{i2} x_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{in} x_{i1} & x_{in} x_{i2} & \\dots & x_{in} x_{in} \\\\\n\\end{bmatrix}\n\\]\nThus \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)\\) is a matrix."
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-2",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-2",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\n\\[\n\\sum_{i=1}^N  y_i \\mathbf{x}_i  =\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)  \\mathbf{w}\n\\]\nMultiplying both sides by the inverse \\(\\bigg(\\sum_{i=1}^N \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1}\\) we get:\n\\[\n\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^*\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-3",
    "href": "lecture2-linear-regression/slides.html#optimizing-linear-regression-directly-3",
    "title": "Lecture 2: Linear regression",
    "section": "Optimizing linear regression directly",
    "text": "Optimizing linear regression directly\n\\[\n\\bigg(\\sum_{i=1}^N  \\mathbf{x}_i\\mathbf{x}_i^T \\bigg)^{-1} \\bigg(\\sum_{i=1}^N  y_i \\mathbf{x}_i\\bigg)  =  \\mathbf{w}^*\n\\]\nWe can write this more compactly using the stacked input matrix and label vector notation we saw in homework 1.\n\\[\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_{1} \\\\ \\mathbf{x}_{2} \\\\ \\vdots \\\\  \\mathbf{x}_{N} \\end{bmatrix},\\quad \\mathbf{y} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\  y_{N} \\end{bmatrix}\n\\]\nIn this case, the expression becomes:\n\\[\\mathbf{w}^* = \\big( \\mathbf{X}^T \\mathbf{X} \\big)^{-1} \\big(\\mathbf{y}\\mathbf{X}\\big)\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#normal-distributions",
    "href": "lecture2-linear-regression/slides.html#normal-distributions",
    "title": "Lecture 2: Linear regression",
    "section": "Normal distributions",
    "text": "Normal distributions\nThe Normal distribution (also known as the Gaussian distribution) is a continuous probability distribution with the following probability density function:\n\\[\np(y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathbf{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y -\\mu)^2\\bigg)\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#linear-regression-as-a-probabilistic-model",
    "href": "lecture2-linear-regression/slides.html#linear-regression-as-a-probabilistic-model",
    "title": "Lecture 2: Linear regression",
    "section": "Linear regression as a probabilistic model",
    "text": "Linear regression as a probabilistic model\nThe probabilistic model for linear regression will make the assumption that the output is normally distributed conditioned on the input:\n\\[\ny_i \\sim N\\big(\\mathbf{x}_i^T \\mathbf{w},\\ \\sigma^2\\big)\n\\]\nWe can write the conditional probability or likelihood of an output as:\n\\[\np(y_i\\mid\\mathbf{x}_i, \\mathbf{w}) =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\mathbf{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\n\\]\n\nImage credit: Lily Chen Towards Data Science"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-1",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-1",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nHow do we find the optimal value for \\(\\mathbf{w}\\)? Choose the \\(\\mathbf{w}\\) that maximizes the likelihood (conditional probability) of all of the outputs in our dataset:\n\\[\n\\mathbf{w}^* = \\underset{\\mathbf{w}}{\\text{argmax}} \\ p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) =\\underset{\\mathbf{w}}{\\text{argmax}} \\ p(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) \\]\nGenerally our model also assumes conditional independence across observations so:\n\\[\np(y_1,...,y_N \\mid \\mathbf{x}_1, ...,\\mathbf{x}_N, \\mathbf{w}) = \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-2",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-2",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nEquivalently frame the optimal value in terms of the negative log-likelihood rather than the likelihood.\n\\[\n\\underset{\\mathbf{w}}{\\text{argmax}} \\prod_{i=1}^N p(y_i\\mid \\mathbf{x}_i, \\mathbf{w}) = \\underset{\\mathbf{w}}{\\text{argmin}} - \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w}) = \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]\nWe see that the negative log-likelihood is a natural loss function to optimize to find \\(\\mathbf{w}^*\\).\n\\[\n\\textbf{Loss}(\\mathbf{w}) =\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})=- \\sum_{i=1}^N \\log p(y_i \\mid \\mathbf{x}_i, \\mathbf{w})\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-3",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-3",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nWe can write out the negative log-likelihood explicitly using the normal PDF:\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = -\\sum_{i=1}^N\\log\\bigg[\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\text{exp}\\bigg(-\\frac{1}{2\\sigma^2} (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\bigg)\\bigg]\n\\]\n\\[\n= \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n\\]"
  },
  {
    "objectID": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-4",
    "href": "lecture2-linear-regression/slides.html#maximum-likelihood-estimation-4",
    "title": "Lecture 2: Linear regression",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\n\\[\n\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}}\n\\]\nWe see that this loss is very similar to the MSE loss. Taking the gradient this becomes even more clear.\n\\[\n\\nabla_{\\mathbf{w}}\\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) =\n\\frac{d}{d\\mathbf{w}}\\bigg( \\frac{1}{2\\sigma^2} \\sum_{i=1}^N(y_i - \\mathbf{x}_i^T\\mathbf{w})^2 - \\frac{N}{\\sigma \\sqrt{2 \\pi}} \\bigg)\n\\]\n\\[\n= \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (\\mathbf{x}_i^T\\mathbf{w} - y_i)\\mathbf{x}_i\n\\]\nThe optimal value for \\(\\mathbf{w}\\) is the same for both MSE and negative log-likelihood and the optimal value does not depend on \\(\\sigma^2\\)!\n\\[\n\\underset{\\mathbf{w}}{\\text{argmin}}\\  MSE(\\mathbf{w}, \\mathbf{X}, \\mathbf{y}) = \\underset{\\mathbf{w}}{\\text{argmin}}\\ \\textbf{NLL}(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html",
    "href": "lecture1-background/notes.html",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "We will begin with a review of many of the basic concepts from linear algebra that we will need for this course. As we go through these concepts we will see how to implement them in Python.\n\n\nNumpy Is the linear algebra library that we will be using for much of this class (later on we will introduce PyTorch, which is more geared towards neural networks). The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors.\n\n\n\nScalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\(x=3.5\\). In code this would be written as:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)\n\nIn general, we will assume that scalars are real numbers meaning decimal numbers in the range \\((-\\infty, \\infty)\\). We can denote this as \\(x \\in \\mathbb{R}\\), meaning “\\(x\\) is in the set of real numbers”.\n\n\n\nVectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nThis bold notation is common, but not universal, so expect that external resources may denote things differently!\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])\n\n\n\n\n\n\n\nOther useful vector creation functions\n\n\n\n\n\nConstants:\n\na = np.zeros(5) # Create a size 5 vector filled with zeros\nprint(a)\na = np.ones(5) # Create a size 5 vector filled with ones\nprint(a)\n\n[0. 0. 0. 0. 0.]\n[1. 1. 1. 1. 1.]\n\n\nInteger ranges:\n\na = np.array([1,2,3,4,5,6,7])\nprint(a)\n# or equivalently, using Python iterables:\na = np.array( range(1,8) )\nprint(a)\n# or the NumPy function np.arange:\na = np.arange(1, 8)\nprint(a)\n\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n\n\nDecimal ranges:\n\nb = np.linspace(1.0, 7.0, 4) # length-4 vector interpolating between 1.0 and 7.0\nprint(b)\nc = np.logspace(0.0, 2.0, 5) # length-7 vector interpolating between 10^0 and 10^2 logarithmically\nprint(c)\n\n[1. 3. 5. 7.]\n[  1.           3.16227766  10.          31.6227766  100.        ]\n\n\nCombining vectors\n\na = np.concatenate([b, c]) # length 4 + 7 vector with the elements of both b and c\n\n\n\n\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\), where \\(n\\) is the number of entries or size of the vector.\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nWe may say that \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to mean that \\(\\mathbf{x}\\) is in the set of vectors with \\(n\\) real-valued entries. In numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2\n\n\nA vector represents either a location or a change in location in \\(n\\) -dimensional space. For instance, the \\(\\mathbf{x}\\) vector we defined could represent the point with coordinates \\((2, 5, 1)\\) in 3-D space or a movement of 2 along the first axis, 5 along the second axis and 1 along the third axis.\n\n\n/Users/gabe/Documents/Courses/CS152-Neural-Networks-Fall-2023.github.io/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\nManim Community v0.17.3\n\n\n\n\n\n\n\n\n\n\n\n\nVectors do not necessarily need to represent geometric points or directions. As they are simply ordered collections of numbers, we can a vector to represent any type of data in a more formal way.\nFor example, a space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]\n\n\n\nWe say that two vectors are equal if and only if all of the corresponding elements are equal, so \\(\\mathbf{x} = \\mathbf{y}\\) implies that \\(x_1=y_1, x_2=y_2…\\). In numpy when we compare two vectors for equality, we get a new vector that indicates which entries are equal.\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy.\n\n\n\nWhen we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])\n\n\nThis corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\).\n\n\n\n\n\n\n\n\nIn both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))\n\n\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms.\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]\n\n\n\nThe magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)\n\nWe can also compute this explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5\n\n\n\n\n\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])\n\n\n\n\n\nThe dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nThe result is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them. If\\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) ).\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33\n\n\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) .\n\n\n\nA matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])\n\n\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal.\n\n\n\n\n\n\nOther matrix creation routines\n\n\n\n\n\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally\n\n\n\n\n\n\n\nOften we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])\n\n\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])\n\n\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])\n\n\n\n\n\n\n\n\nAdvanced slicing in numpy\n\n\n\n\n\n\nA = np.arange(1, 13).reshape((3, 4))\nprint(A)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]\n\n\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]\n\n\n\n\n\n\n\n\nThe matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)\n\n\n\n\n\nThe transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])\n\n\n\n\n\nAs with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])\n\n\n\n\n\n\n\n\nOther element-wise operations\n\n\n\n\n\nUnary operations\n\nnp.sqrt(A)     # Take the square root of every element\nnp.log(A)      # Take the (natural) log of every element\nnp.exp(A)      # Take e^x for every element x in A\nnp.sin(A)      # Take sin(x) for every element x in A\nnp.cos(A)      # Take cos(x) for every element x in A\n\narray([[ 0.54030231,  0.54030231,  0.54030231],\n       [-0.41614684, -0.41614684, -0.41614684]])\n\n\nScalar operations\n\nA + 2          # Add a scalar to every element of A\nA - 1          # Subtract a scalar from every element of A\nA * 4          # Multiply a scalar with every element of A\nA / 6          # Divide every element by a scalar\nA ** 3         # Take every element to a power\n\narray([[1, 1, 1],\n       [8, 8, 8]])\n\n\n\n\n\n\n\n\nA matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nWe say that a square matrix \\(A\\) defines a linear mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), which simply means if we multiply any vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) by \\(A\\) we get a new vector in \\(\\mathbb{R}^n\\) where the elements are a linear combination of the elements in \\(\\mathbf{x}\\).\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin.\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])\n\n\nIn general, an \\(n\\times m\\) matrix defines a linear mapping \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\), transforming \\(\\mathbf{x} \\in \\mathbb{R}^n\\) into a possibly higher or lower dimensional space \\(\\mathbb{R}^m\\).\n\n\n\nMatrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)\n\nNote that the number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]\nWe can see matrix multiplication as a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))\n\n\n\n\n\nIt is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]\n\n\n\nAs we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))\n\n\n\n\n\n\n\n\nOther matrix reduction examples\n\n\n\n\n\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3\n\n\n\n\n\n\n\n\nThe identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix\n\n\n\n\nConsider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nWe call this solving a system of linear equations. A common algorithm used to find \\(\\mathbf{x}\\) is Gaussian elimination, the details of which are outside the scope of this course. Luckily, numpy gives us a convenient function for solving this problem: np.linalg.solve.\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course.\n\n\n\nThe inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/notes.html#numpy",
    "href": "lecture1-background/notes.html#numpy",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Numpy Is the linear algebra library that we will be using for much of this class (later on we will introduce PyTorch, which is more geared towards neural networks). The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors."
  },
  {
    "objectID": "lecture1-background/notes.html#scalars",
    "href": "lecture1-background/notes.html#scalars",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Scalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\(x=3.5\\). In code this would be written as:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)\n\nIn general, we will assume that scalars are real numbers meaning decimal numbers in the range \\((-\\infty, \\infty)\\). We can denote this as \\(x \\in \\mathbb{R}\\), meaning “\\(x\\) is in the set of real numbers”."
  },
  {
    "objectID": "lecture1-background/notes.html#vectors",
    "href": "lecture1-background/notes.html#vectors",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Vectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nThis bold notation is common, but not universal, so expect that external resources may denote things differently!\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])\n\n\n\n\n\n\n\nOther useful vector creation functions\n\n\n\n\n\nConstants:\n\na = np.zeros(5) # Create a size 5 vector filled with zeros\nprint(a)\na = np.ones(5) # Create a size 5 vector filled with ones\nprint(a)\n\n[0. 0. 0. 0. 0.]\n[1. 1. 1. 1. 1.]\n\n\nInteger ranges:\n\na = np.array([1,2,3,4,5,6,7])\nprint(a)\n# or equivalently, using Python iterables:\na = np.array( range(1,8) )\nprint(a)\n# or the NumPy function np.arange:\na = np.arange(1, 8)\nprint(a)\n\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n[1 2 3 4 5 6 7]\n\n\nDecimal ranges:\n\nb = np.linspace(1.0, 7.0, 4) # length-4 vector interpolating between 1.0 and 7.0\nprint(b)\nc = np.logspace(0.0, 2.0, 5) # length-7 vector interpolating between 10^0 and 10^2 logarithmically\nprint(c)\n\n[1. 3. 5. 7.]\n[  1.           3.16227766  10.          31.6227766  100.        ]\n\n\nCombining vectors\n\na = np.concatenate([b, c]) # length 4 + 7 vector with the elements of both b and c\n\n\n\n\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\), where \\(n\\) is the number of entries or size of the vector.\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nWe may say that \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to mean that \\(\\mathbf{x}\\) is in the set of vectors with \\(n\\) real-valued entries. In numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2\n\n\nA vector represents either a location or a change in location in \\(n\\) -dimensional space. For instance, the \\(\\mathbf{x}\\) vector we defined could represent the point with coordinates \\((2, 5, 1)\\) in 3-D space or a movement of 2 along the first axis, 5 along the second axis and 1 along the third axis.\n\n\n/Users/gabe/Documents/Courses/CS152-Neural-Networks-Fall-2023.github.io/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture1-background/notes.html#vectors-as-data",
    "href": "lecture1-background/notes.html#vectors-as-data",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Vectors do not necessarily need to represent geometric points or directions. As they are simply ordered collections of numbers, we can a vector to represent any type of data in a more formal way.\nFor example, a space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-equality",
    "href": "lecture1-background/notes.html#vector-equality",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "We say that two vectors are equal if and only if all of the corresponding elements are equal, so \\(\\mathbf{x} = \\mathbf{y}\\) implies that \\(x_1=y_1, x_2=y_2…\\). In numpy when we compare two vectors for equality, we get a new vector that indicates which entries are equal.\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy."
  },
  {
    "objectID": "lecture1-background/notes.html#vector-addition",
    "href": "lecture1-background/notes.html#vector-addition",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "When we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])\n\n\nThis corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\)."
  },
  {
    "objectID": "lecture1-background/notes.html#element-wise-operations",
    "href": "lecture1-background/notes.html#element-wise-operations",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "In both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))\n\n\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms.\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#vector-magnitude",
    "href": "lecture1-background/notes.html#vector-magnitude",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)\n\nWe can also compute this explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5\n\n\n\n\n\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])"
  },
  {
    "objectID": "lecture1-background/notes.html#dot-products",
    "href": "lecture1-background/notes.html#dot-products",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nThe result is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them. If\\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) ).\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33\n\n\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) ."
  },
  {
    "objectID": "lecture1-background/notes.html#matrices",
    "href": "lecture1-background/notes.html#matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "A matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])\n\n\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal.\n\n\n\n\n\n\nOther matrix creation routines\n\n\n\n\n\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally"
  },
  {
    "objectID": "lecture1-background/notes.html#slicing-matrices",
    "href": "lecture1-background/notes.html#slicing-matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Often we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])\n\n\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])\n\n\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])\n\n\n\n\n\n\n\n\nAdvanced slicing in numpy\n\n\n\n\n\n\nA = np.arange(1, 13).reshape((3, 4))\nprint(A)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]\n\n\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-shapes",
    "href": "lecture1-background/notes.html#matrix-shapes",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-transpose",
    "href": "lecture1-background/notes.html#matrix-transpose",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])"
  },
  {
    "objectID": "lecture1-background/notes.html#element-wise-matrix-operations",
    "href": "lecture1-background/notes.html#element-wise-matrix-operations",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "As with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])\n\n\n\n\n\n\n\n\nOther element-wise operations\n\n\n\n\n\nUnary operations\n\nnp.sqrt(A)     # Take the square root of every element\nnp.log(A)      # Take the (natural) log of every element\nnp.exp(A)      # Take e^x for every element x in A\nnp.sin(A)      # Take sin(x) for every element x in A\nnp.cos(A)      # Take cos(x) for every element x in A\n\narray([[ 0.54030231,  0.54030231,  0.54030231],\n       [-0.41614684, -0.41614684, -0.41614684]])\n\n\nScalar operations\n\nA + 2          # Add a scalar to every element of A\nA - 1          # Subtract a scalar from every element of A\nA * 4          # Multiply a scalar with every element of A\nA / 6          # Divide every element by a scalar\nA ** 3         # Take every element to a power\n\narray([[1, 1, 1],\n       [8, 8, 8]])"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-vector-products",
    "href": "lecture1-background/notes.html#matrix-vector-products",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "A matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nWe say that a square matrix \\(A\\) defines a linear mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), which simply means if we multiply any vector \\(\\mathbf{x}\\in\\mathbb{R}^n\\) by \\(A\\) we get a new vector in \\(\\mathbb{R}^n\\) where the elements are a linear combination of the elements in \\(\\mathbf{x}\\).\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin.\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])\n\n\nIn general, an \\(n\\times m\\) matrix defines a linear mapping \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^m\\), transforming \\(\\mathbf{x} \\in \\mathbb{R}^n\\) into a possibly higher or lower dimensional space \\(\\mathbb{R}^m\\)."
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-multiplication",
    "href": "lecture1-background/notes.html#matrix-multiplication",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Matrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)\n\nNote that the number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]\nWe can see matrix multiplication as a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))"
  },
  {
    "objectID": "lecture1-background/notes.html#element-wise-multiplication",
    "href": "lecture1-background/notes.html#element-wise-multiplication",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "It is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#matrix-reductions",
    "href": "lecture1-background/notes.html#matrix-reductions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "As we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))\n\n\n\n\n\n\n\n\nOther matrix reduction examples\n\n\n\n\n\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3"
  },
  {
    "objectID": "lecture1-background/notes.html#identity-matrices",
    "href": "lecture1-background/notes.html#identity-matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix"
  },
  {
    "objectID": "lecture1-background/notes.html#solving-systems-of-linear-equations",
    "href": "lecture1-background/notes.html#solving-systems-of-linear-equations",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "Consider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nWe call this solving a system of linear equations. A common algorithm used to find \\(\\mathbf{x}\\) is Gaussian elimination, the details of which are outside the scope of this course. Luckily, numpy gives us a convenient function for solving this problem: np.linalg.solve.\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course."
  },
  {
    "objectID": "lecture1-background/notes.html#inverse-matrices",
    "href": "lecture1-background/notes.html#inverse-matrices",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "",
    "text": "The inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/notes.html#functions",
    "href": "lecture1-background/notes.html#functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Functions",
    "text": "Functions\nA function is a general mapping from one set to another.\n\\[\ny=f(x),\\quad f:\\mathbb{R}\\rightarrow\\mathbb{R}\n\\]\nIn this course will focus on functions of real numbers, that is, mappings from real numbers to other real numbers this is denoted using the notation \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\) above. We call the set of possible inputs the domain of the function (in this case real numbers) and the corresponding set of possible outputs the codomain or range of the function. In the notation above \\(x\\) is the real valued input and \\(y\\) is the real-valued output.\nWe can definite functions as compositions of simple operations. For example we could define a polynomial function as:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\nIn code we can implement functions as, well, functions:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\nf(5)\n\n41"
  },
  {
    "objectID": "lecture1-background/notes.html#derivatives",
    "href": "lecture1-background/notes.html#derivatives",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Derivatives",
    "text": "Derivatives\nThe derivative of a function at input \\(x\\) defines how the function’s output changes as the input changes from \\(x\\). It is equivalent to the slope of the line tangent to the function at the input \\(x\\). We’ll use the notation \\(\\frac{df}{dx}\\) to denote the derivative of the function \\(f\\) at input \\(x\\). Formally:\n\\[\n\\frac{df}{dx} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\n\\]\nIntunitively, this means if we change our input \\(x\\) by some small amount \\(\\epsilon\\), the output of our function will change by approximately \\(\\frac{df}{dx}\\epsilon\\)\n\\[\nf(x+\\epsilon) \\approx f(x)+\\frac{df}{dx}\\epsilon\n\\]\nNote that with respect to \\(\\epsilon\\) this approximation is a line with slope \\(\\frac{df}{dx}\\) and intercept \\(f(x)\\), therefore we say that this is a linear approximation to the function \\(f\\) at input \\(x\\).\nWe can also use the notation \\(\\frac{d}{dx}\\) to denote the derivative operator. This means “find the derivative of the following expression with respect to \\(x\\)”.\n\\[\n\\frac{d}{dx}f(x) = \\frac{df}{dx}\n\\]\n\n\n\n\n\n\nCaveat\n\n\n\n\n\nThis definition assumes that the limit exists at the input \\(x\\), which is not always true. We will see practical examples later in the course where this assumption does not hold."
  },
  {
    "objectID": "lecture1-background/notes.html#derivative-functions",
    "href": "lecture1-background/notes.html#derivative-functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Derivative functions",
    "text": "Derivative functions\nWe can also talk about the function that maps any input \\(x\\) to the derivative \\(\\frac{df}{dx}\\) we call this the derivative function and denote it as \\(f'(x)\\). So:\n\\[\n\\frac{df}{dx}=f'(x)\n\\]\nGiven a function defined as a composition of basic operations, we can use a set of standard rules to find the corresponding derivative function. For example using the rules \\(\\frac{d}{dx}x^a=ax\\) , \\(\\frac{d}{dx}ax=a\\) and \\(\\frac{d}{dx}a=0\\), we can derive the derivative function for the polynomial above:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\n\\[\nf'(x) = 2x + 3\n\\]\n\n\n\n\n\n\nBasic derivative rules\n\n\n\n\n\n\n\n\nOperation\nDerivative \\(\\frac{d}{dx}\\)\n\n\n\n\n\\(a\\)\n\\(0\\)\n\n\n\\(ax\\)\n\\(a\\)\n\n\n\\(x^a\\)\n\\(ax\\)\n\n\n\\(\\log(x)\\)\n\\(\\frac{1}{x}\\)\n\n\n\\(e^x\\)\n\\(e^x\\)\n\n\n\\(f(x) + g(x)\\)\n\\(f'(x)+g'(x)\\)\n\n\n\\(f(x)g(x)\\)\n\\(f'(x)g(x) + f(x)g'(x)\\)\n\n\n\\(\\frac{f(x)}{g(x)}\\)\n\\(\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\\)"
  },
  {
    "objectID": "lecture1-background/notes.html#chain-rule",
    "href": "lecture1-background/notes.html#chain-rule",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Chain rule",
    "text": "Chain rule\nComposing two functions means to apply one function to the output of another, for example we could apply \\(f\\) to the output of \\(g\\):\n\\[\ny = f\\big(g\\left(x\\right)\\big)\n\\]\nThis is easily replicated in code:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\ndef g(x):\n    return 5 * x - 2\n\nf(g(3))\n\n209\n\n\nThe chain rule tells us how to find the derivative of a composition of functions like this. We can write the rule either in terms of derivatives or derivative functions\n\\[\n\\frac{d}{dx}f\\big(g\\left(x\\right)\\big) = \\frac{df}{dg}\\frac{dg}{dx} \\quad \\text{or} \\quad \\frac{d}{dx}f\\big(g\\left(x\\right)\\big) =  f'\\big(g\\left(x\\right)\\big)g'\\left(x\\right)\n\\]\nNote that in our derivative notation we’re using \\(f\\) and \\(g\\) to denote the outputs of the respective functions.\n\n\n\n\n\n\nDerivatives in code\n\n\n\n\n\nA natural question to ask at this point is: does the derivative operator exist in Python or numpy?. The answer is amazingly: yes! However implementing such an operator is nontrivial. In fact, a significant portion of this course will be devoted to exploring how to implement exactly such an operation for numpy. As a preview, we will end up with the ability to take derivatives as follows:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\nfx = f(5)\ndf_dx = derivative(f)(5)"
  },
  {
    "objectID": "lecture1-background/notes.html#partial-derivatives",
    "href": "lecture1-background/notes.html#partial-derivatives",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nA function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\nIn code this would look like;\n\ndef f(x, y, z):\n    return x ** 2 + 3 * y + np.log(z)\n\nA partial derivative is the derivative of a multiple-input function with respect to a single input, assuming all other inputs are constant. We will explore the implications of that condition later on in this course. For now, we will simply view partial derivatives as a straightforward extension of derivatives, using the modified notation \\(\\frac{\\partial}{\\partial x}\\).\nMore formally, we can define the partial derivative with respect to each input of a function as:\n\\[\n\\frac{\\partial f}{\\partial x} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon, y, z) - f(x,y,z)}{\\epsilon}, \\quad \\frac{\\partial f}{\\partial y} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x, y+\\epsilon, z) - f(x,y,z)}{\\epsilon}\n\\]\nThese partial derivatives tell us how the output of the function changes as we change each of the inputs individually."
  },
  {
    "objectID": "lecture1-background/notes.html#partial-derivative-functions",
    "href": "lecture1-background/notes.html#partial-derivative-functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Partial derivative functions",
    "text": "Partial derivative functions\nWe can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.\n\\[\n\\frac{\\partial f}{\\partial x} = f_x'(x, y, z)\n\\]\nWe can derive partial derivative functions using the same set of derivative rules:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\n\\[\nf_x'(x, y, z) = 2x + 3y\n\\]\n\\[\nf_y'(x, y, z) = 3x\n\\]\n\\[\nf_z'(x, y, z) = -\\frac{1}{z}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#functions-of-vectors",
    "href": "lecture1-background/notes.html#functions-of-vectors",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Functions of vectors",
    "text": "Functions of vectors\nWe can also define functions that take vectors (or matrices) as inputs.\n\\[\ny = f(\\mathbf{x}) \\quad f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\]\nHere \\(f\\) is a mapping from length \\(n\\) vectors to real numbers. As a concrete example we could define the function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_i^3 + 1\n\\]\nHere’s the same function in numpy:\n\ndef f(x):\n    return np.sum(x ** 3) + 1\n\nf(np.array([1, 2, 3]))\n\n37\n\n\nNote that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!"
  },
  {
    "objectID": "lecture1-background/notes.html#gradients",
    "href": "lecture1-background/notes.html#gradients",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Gradients",
    "text": "Gradients\nThe gradient of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.\n\\[\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\\\ \\vdots \\end{bmatrix}\n\\]\nThe gradient is a vector that tangent to the function \\(f\\) at the input \\(\\mathbf{x}\\). Just as with derivatives, this means that the gradient defines a linear approximation to the function at the point \\(\\mathbf{x}\\).\n\\[\nf(\\mathbf{x}+\\mathbf{\\epsilon}) \\approx f(\\mathbf{x}) + \\frac{df}{d\\mathbf{x}} \\cdot \\mathbf{\\epsilon}\n\\]\nWhere \\(\\mathbf{\\epsilon}\\) is now a small vector. Intuitively, this means that if we take a small step in any direction as defined by \\(\\mathbf{\\epsilon}\\), the gradient will approximate the change in the output of the function. Becuase we are now in more than 1 dimension, this approximation defines a plane in \\(\\mathbb{R}^n\\).\nAnother extremely important property of the gradient is that it points in the direction of maximum change in the function. Meaning that if we were to take an infinitesimal step \\(\\mathbf{\\epsilon}\\) from \\(\\mathbf{x}\\) in any direction, stepping in the gradient direction would give use the maximum value of \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\). We can see this from the approximation above: \\(f(\\mathbf{x} +\\mathbf{\\epsilon})\\) is maximized when \\(\\frac{df}{d\\mathbf{x}}\\) and \\(\\mathbf{\\epsilon}\\) are colinear.\nWe can define the gradient in this sense this more formally as:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\underset{\\gamma \\rightarrow 0}{\\lim}\\ \\underset{\\|\\mathbf{\\epsilon}\\|_2 &lt; \\gamma}{\\max} \\frac{f(\\mathbf{x} + \\mathbf{\\epsilon}) - f(\\mathbf{x})}{\\|\\mathbf{\\epsilon}\\|_2}\n\\]"
  },
  {
    "objectID": "lecture1-background/notes.html#gradient-functions",
    "href": "lecture1-background/notes.html#gradient-functions",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Gradient functions",
    "text": "Gradient functions\nJust as with derivatives and partial derivatives, we can define a gradient function that maps an input vector \\(\\mathbf{x}\\) to the gradient of the function \\(f\\) at \\(\\mathbf{x}\\) as:\n\\[\n\\frac{df}{d\\mathbf{x}}=\\nabla f(\\mathbf{x})\n\\]\nHere \\(\\nabla f\\) is the gradient function for \\(f\\). If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}), \\quad \\frac{df}{d\\mathbf{y}}= \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})\n\\]\nNote that the gradient function is a mapping from \\(\\mathbb{R}^n\\rightarrow\\mathbb{R}^n\\), meaning that it returns a vector with the same size as the input."
  },
  {
    "objectID": "lecture1-background/notes.html#getting-started",
    "href": "lecture1-background/notes.html#getting-started",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Getting started",
    "text": "Getting started\nThe standard way to import MatPlotLib is:\n\nimport matplotlib.pyplot as plt\n\nThe standard approach to interacting with MatPlotLib can take some getting used to. In MatPlotLib the current plot that we’re working on is part of the global state, meaning that we don’t create an explicit plot object, we simply call functions of plt to update what the current plot will look like.\nWhen working in Jupyter notebooks, the current plot is displayed at the end of the current cell when it is run."
  },
  {
    "objectID": "lecture1-background/notes.html#scatterplots-and-line-plots",
    "href": "lecture1-background/notes.html#scatterplots-and-line-plots",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Scatterplots and line plots",
    "text": "Scatterplots and line plots\nThe most typical action is to plot one sequence (x-values) against another (y-values); this can be done using disconnected points (a scatterplot), or by connecting adjacent points in the sequence (in the order they were provided). The latter is usually used to give a nice (piecewise linear) visualization of a continuous curve, by specifying x-values in order, and the y-values given by the function at those x-values.\nPlotting a scatter of data points:\n\nx_values = np.random.rand(1,10)   # unformly in [0,1)\ny_values = np.random.randn(1,10)  # Gaussian distribution\nplt.plot(x_values, y_values, 'ko');\n\n\n\n\nThe string determines the plot appearance -- in this case, black circles. You can use color strings (‘r’, ‘g’, ‘b’, ‘m’, ‘c’, ‘y’, ...) or use the “Color” keyword to specify an RGB color. Marker appearance (‘o’,‘s’,‘v’,‘.’, ...) controls how the points look.\nIf we connect those points using a line appearance specification (‘-’,‘--’,‘:’,...), it will not look very good, because the points are not ordered in any meaningful way. Let’s try a line plot using an ordered sequence of x values:\n\nx_values = np.linspace(0,8,100)\ny_values = np.sin(x_values)\nplt.plot(x_values,y_values,'b');\n\n\n\n\nThis is actually a plot of a large number of points (100), with no marker shape and connected by a solid line.\nFor plotting multiple point sets or curves, you can pass more vectors into the plot function, or call the function multiple times:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny2 = (x_values - 3)**2 / 12   # a simple quadratic curve\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-', x_values, y2, 'g--');  # plot two curves\nplt.plot(x_values, y3, 'r:'); # add a curve to the plot\n\n\n\n\nYou may want to explicitly set the plot ranges -- perhaps the most common pattern is to plot something, get the plot’s ranges, and then restore them later after plotting another function:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-') \nax = plt.axis()               # get the x and y axis ranges\nprint(ax)\n# now plot something else (which will change the axis ranges):\nplt.plot(x_values, y3, 'r:'); # add the linear curve\nplt.axis(ax);                 # restore the original plot's axis ranges\n\n(-0.4, 8.4, -1.099652011574681, 1.0998559934443881)"
  },
  {
    "objectID": "lecture1-background/notes.html#histograms",
    "href": "lecture1-background/notes.html#histograms",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Histograms",
    "text": "Histograms\nHistograms are also useful visualizations:\n\nplt.hist(y2, bins=20);\n\n\n\n\nThe outputs of hist include the bin locations, the number of data in each bin, and the “handles” to the plot elements to manipulate their appearance, if desired."
  },
  {
    "objectID": "lecture1-background/notes.html#subplots-and-plot-sizes",
    "href": "lecture1-background/notes.html#subplots-and-plot-sizes",
    "title": "Lecture 1: Introduction to Neural Networks",
    "section": "Subplots and plot sizes",
    "text": "Subplots and plot sizes\nIt is often useful to put more than one plot together in a group; you can do this using the subplot function. There are various options; for example, “sharex” and “sharey” allow multiple plots to share a single axis range (or, you can set it manually, of course).\nI often find it necessary to also change the geometry of the figure for multiple subplots -- although this is more generally useful as well, if you have a plot that looks better wider and shorter, for example.\n\nfig,ax = plt.subplots(1,3, figsize=(8.0, 2.0))      # make a 1 x 3 grid of plots:\nax[0].plot(x_values, y1, 'b-');   # plot y1 in the first subplot\nax[1].plot(x_values, y2, 'g--');  #   y2 in the 2nd\nax[2].plot(x_values, y3, 'r:');   #   and y3 in the last"
  },
  {
    "objectID": "lecture1-background/slides.html",
    "href": "lecture1-background/slides.html",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "We will begin with a review of many of the basic concepts from linear algebra that we will need for this course. As we go through these concepts we will see how to implement them in Python.\n\n\nNumpy Is the linear algebra library that we will be using for much of this class. The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors.\n\n\n\nScalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\[x=3.5\\] In code:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)\n\n\n\n\nVectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])\n\n\n\n\nThe individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nIn numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2\n\n\n\n\n\nA vector represents either a location or a change in location in \\(n\\) -dimensional space.\n\n\n/Users/gabe/Documents/Courses/CS152-Neural-Networks-Fall-2023.github.io/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\nManim Community v0.17.3\n\n\n\n\n\n\n\n\n\n\n\n\nA space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]\n\n\n\nWe say that two vectors are equal if and only if all of the corresponding elements are equal, so \\[\\mathbf{x} = \\mathbf{y}\\] implies that \\[x_1=y_1, x_2=y_2…\\]\n\n\n\n\nx = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy.\n\n\n\nWhen we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])\n\n\n\n\n\nAddition corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\).\n\n\n\n\n\n\n\n\nIn both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))\n\n\n\n\n\nNote that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms.\n\n\n\nOperations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]\n\n\n\nThe magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)\n\n\n\n\nWe can also compute magnitude explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5\n\n\n\n\n\n\n\n\nA unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])\n\n\n\n\n\nThe dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nThe result is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them.\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33\n\n\n\n\n\nIf \\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) ).\n\n\n\nGeometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) .\n\n\n\nA matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])\n\n\n\n\n\nAs with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal.\n\n\n\nBasic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally\n\n\n\n\nOften we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])\n\n\n\n\n\nWe can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])\n\n\n\n\n\nWe can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])\n\n\n\n\n\nOpen-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]\n\n\n\n\n\nMore general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]\n\n\n\n\n\nThe matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)\n\n\n\n\n\nThe transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])\n\n\n\n\n\nAs with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])\n\n\n\n\n\nA matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin.\n\n\n\nThe number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])\n\n\n\n\n\nMatrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)\n\n\n\n\nThe number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]\n\n\n\nMatrix multiplication is a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))\n\n\n\n\n\nIt is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]\n\n\n\nAs we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]\n\n\n\nIn many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))\n\n\n\n\n\n\nprint(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3\n\n\n\n\n\nThe identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix\n\n\n\n\nConsider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nIn Numpy:\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course.\n\n\n\nThe inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/slides.html#numpy",
    "href": "lecture1-background/slides.html#numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Numpy Is the linear algebra library that we will be using for much of this class. The standard way to import Numpy is:\n\nimport numpy as np\n\nThe most basic object from Numpy that we will just is the array (np.array), which will represent vectors matrices and even higher-dimensional objects known as tensors."
  },
  {
    "objectID": "lecture1-background/slides.html#scalars",
    "href": "lecture1-background/slides.html#scalars",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Scalars are just single numbers. In our math we will typically denote scalars with a lower-case letter. For example, we might call a scalar \\(x\\) and give it a value as \\[x=3.5\\] In code:\n\nx = 3.5\n\n# or as a 0-dimensional numpy array\nx = np.array(3.5)"
  },
  {
    "objectID": "lecture1-background/slides.html#vectors",
    "href": "lecture1-background/slides.html#vectors",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Vectors are ordered lists of numbers, as shown below. We will typically denote vectors with bold lower case letters, such as \\(\\mathbf{x}\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} 2\\\\ 5\\\\ 1\\end{bmatrix}\n\\]\nIn Numpy, we can create an array object representing a vector by passing np.array a list of numbers.\n\nx = np.array([2, 5, 1])"
  },
  {
    "objectID": "lecture1-background/slides.html#entries",
    "href": "lecture1-background/slides.html#entries",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The individual numbers in the vector are called the entries and we will refer to them individually as subscripted scalars: \\(x_1, x_2,…,x_n\\).\n\\[\n\\mathbf{x} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix}\n\\]\nIn numpy we can access individual elements of a vector using [] operators (note that numpy is 0-indexed!).\n\nx[0]\n\n2"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-interpretation",
    "href": "lecture1-background/slides.html#vector-interpretation",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "A vector represents either a location or a change in location in \\(n\\) -dimensional space.\n\n\n/Users/gabe/Documents/Courses/CS152-Neural-Networks-Fall-2023.github.io/.venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n  warnings.warn(\n\n\nManim Community v0.17.3"
  },
  {
    "objectID": "lecture1-background/slides.html#vectors-as-data",
    "href": "lecture1-background/slides.html#vectors-as-data",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "A space of vectors could represent students, with each entry corresponding to a student’s grade in a different subject:\n\\[\n\\text{Student } \\mathbf{x} = \\begin{bmatrix} \\text{Math} \\\\ \\text{CS} \\\\ \\text{Literature} \\\\\\text{History} \\end{bmatrix}\n\\]\nAny two students might have different grades across the subjects. We can represent these two students (say \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\)) as two vectors.\n\\[\n\\mathbf{x}_1 = \\begin{bmatrix} 3.0 \\\\ 4.0 \\\\ 3.7 \\\\ 2.3 \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 3.7 \\\\ 3.3 \\\\ 3.3 \\\\ 4.0 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-equality",
    "href": "lecture1-background/slides.html#vector-equality",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "We say that two vectors are equal if and only if all of the corresponding elements are equal, so \\[\\mathbf{x} = \\mathbf{y}\\] implies that \\[x_1=y_1, x_2=y_2…\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-equality-in-numpy",
    "href": "lecture1-background/slides.html#vector-equality-in-numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "x = np.array([2, 5, 1])\ny = np.array([3, 5, 2])\nx == y\n\narray([False,  True, False])\n\n\nWe can check if all entries are equal (and therefore the vectors are equal) using the np.all function.\n\nnp.all(x == y), np.all(x == x)\n\n(False, True)\n\n\nOther comparison operators (&gt;,&lt;, &gt;=, &lt;=, !=) also perform element-wise comparison in numpy."
  },
  {
    "objectID": "lecture1-background/slides.html#vector-addition",
    "href": "lecture1-background/slides.html#vector-addition",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "When we add or subtract vectors, we add or subtract the corresponding elements.\n\\[\\mathbf{x} + \\mathbf{y} = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} + \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3\\end{bmatrix} = \\begin{bmatrix} x_1 + y_1\\\\ x_2 + y_2\\\\ x_3 + y_3\\end{bmatrix}\\] This works the same in numpy\n\nx + y\n\narray([ 5, 10,  3])"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-addition-1",
    "href": "lecture1-background/slides.html#vector-addition-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Addition corresponds to shifting the point \\(\\mathbf{x}\\) by the vector \\(\\mathbf{y}\\)."
  },
  {
    "objectID": "lecture1-background/slides.html#element-wise-operations",
    "href": "lecture1-background/slides.html#element-wise-operations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "In both mathematical notation and numpy, most operations on vectors will be assumed to be taken element-wise. That is,\n\\[\n\\mathbf{x}^2 = \\begin{bmatrix} x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} , \\quad \\log(\\mathbf{x}) = \\begin{bmatrix} \\log x_1 \\\\ \\log x_2 \\\\ \\log x_3 \\end{bmatrix},\\ \\text{etc.}\n\\]\nIn numpy:\n\nx ** 2, np.log(x)\n\n(array([ 4, 25,  1]), array([0.69314718, 1.60943791, 0.        ]))"
  },
  {
    "objectID": "lecture1-background/slides.html#note-on-logarithms",
    "href": "lecture1-background/slides.html#note-on-logarithms",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Note that in this class (and in numpy!) logarithms are assumed to be base \\(e\\), otherwise known as natural logarithms."
  },
  {
    "objectID": "lecture1-background/slides.html#scalar-vector-operations",
    "href": "lecture1-background/slides.html#scalar-vector-operations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Operations between scalars and vectors are also assumed to be element-wise as in this scalar-vector multiplication\n\\[\na\\mathbf{x} = \\begin{bmatrix} a x_1 \\\\ a x_2 \\\\ a x_3 \\end{bmatrix}, \\quad a + \\mathbf{x} = \\begin{bmatrix} a + x_1 \\\\ a + x_2 \\\\ a + x_3 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-magnitude",
    "href": "lecture1-background/slides.html#vector-magnitude",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The magnitude of a vector its length in \\(\\mathbb{R}^n\\), or equivalently the Euclidean distance from the origin to the point the vector represents. It is denoted as\\(\\|\\mathbf{x}\\|_2\\) and defined as:\n\\[\n\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n\\]\nThe subscript \\(2\\) specifies that we are talking about Euclidean distance. Because of this we will also refer to the magnitude as the two-norm.\nIn numpy we can compute this using the np.linalg.norm function.\n\nxnorm = np.linalg.norm(x)"
  },
  {
    "objectID": "lecture1-background/slides.html#vector-magnitude-1",
    "href": "lecture1-background/slides.html#vector-magnitude-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "We can also compute magnitude explicitly with the np.sum function, which computes the sum of the elements of a vector.\n\nxnorm_explicit = np.sqrt(np.sum(x ** 2))\nxnorm, xnorm_explicit\n\n(5.477225575051661, 5.477225575051661)\n\n\n\n\n\n\n\n\nOther useful aggregation functions\n\n\n\n\n\n\nprint(np.mean(x))    # Take the mean of the elements in x\nprint(np.std(x))     # Take the standard deviation of the elements in x\nprint(np.min(x))     # Find the minimum element in x\nprint(np.max(x))     # Find the maximum element in x\n\n2.6666666666666665\n1.699673171197595\n1\n5"
  },
  {
    "objectID": "lecture1-background/slides.html#unit-vectors",
    "href": "lecture1-background/slides.html#unit-vectors",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "A unit vector is a vector with length \\(1\\). We can find a unit vector that has the same direction as \\(\\mathbf{x}\\) by dividing \\(\\mathbf{x}\\) by it’s magnitude:\n\\[\n\\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n\\]\n\nx / np.linalg.norm(x)\n\narray([0.36514837, 0.91287093, 0.18257419])"
  },
  {
    "objectID": "lecture1-background/slides.html#dot-products",
    "href": "lecture1-background/slides.html#dot-products",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The dot-product operation between two vectors is defined as\n\\[\n\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n\\]\nThe result is a scalar whose value is equal to \\(\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2 \\cos\\theta\\), where \\(\\theta\\) is the angle between them.\nWe can compute dot products in numpy using the np.dot function\n\nnp.dot(x, y)\n\n33"
  },
  {
    "objectID": "lecture1-background/slides.html#dot-products-1",
    "href": "lecture1-background/slides.html#dot-products-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "If \\(\\theta=\\frac{\\pi}{2}\\), the vectors are orthogonal and the dot product with be \\(0\\). If \\(\\theta&gt;\\frac{\\pi}{2}\\) or \\(\\theta &lt; -\\frac{\\pi}{2}\\) , the dot product will be negative.\nIf \\(\\theta=0\\) then \\(\\cos(\\theta)=1\\). In this case we say the vectors are colinear. This formulation implies that given two vectors of fixed length, the dot product is maximized with they are colinear ( \\(\\theta=0\\) )."
  },
  {
    "objectID": "lecture1-background/slides.html#dot-products-2",
    "href": "lecture1-background/slides.html#dot-products-2",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Geometrically, \\(\\|\\mathbf{x}\\|_2\\cos\\theta\\) is the length of the projection of \\(\\mathbf{x}\\) onto \\(\\mathbf{y}\\). Thus, we can compute the length of this projection using the dot-product as \\(\\frac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{y}\\|}\\) ."
  },
  {
    "objectID": "lecture1-background/slides.html#matrices",
    "href": "lecture1-background/slides.html#matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "A matrix is a 2-dimensional collection of numbers. We will denote matrices using bold capital letters, e.g. \\(\\mathbf{A}\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2  \\end{bmatrix}\n\\]\nIn numpy we can create a matrix by passing np.array as list-of-lists, where each inner list specifies a row of the matrix.\n\nA = np.array([[3, 5, 4], [1, 1, 2]])\nA\n\narray([[3, 5, 4],\n       [1, 1, 2]])"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-elements",
    "href": "lecture1-background/slides.html#matrix-elements",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "As with vectors we will denote individual elements of a matrix using subscripts. In this case, each element has 2 coordinates. Conventions is to always list the row first.\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can access elements in a matrix similarly.\n\nA[1, 2]\n\n2\n\n\nAs with vectors, we say that two matrices are equal if and only if all of the corresponding elements are equal."
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-creation-in-numpy",
    "href": "lecture1-background/slides.html#matrix-creation-in-numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Basic creation functions\n\nA0 = np.zeros((3, 4))          # create a 3x4 matrix of all zeros\nA1 = np.ones((4, 4))           # create a 4x4 matrix of all ones\nAinf = np.full((3, 3), np.inf) # create a 3x3 matrix of all infinities\nI = np.eye(3)                  # create a 3x3 itentity matrix\n\nCreating a matrix by “reshaping” a vector with the same number of elements\n\nV = np.arange(1, 13).reshape((3, 4)) # Create a 3x4 matrix with elements 1-12\n\nCreating matrices by combining matrices\n\nB = np.tile(A0, (3, 2))               # create a matrix by \"tiling\" copies of A0 (3 copies by 2 copies)\nB = np.concatenate([A0, A1], axis=0)  # create a (2n x m) matrix by stacking two matrices vertically\nB = np.concatenate([A0, I], axis=1)  # create a (n x 2m) matrix by stacking two matrices horizontally"
  },
  {
    "objectID": "lecture1-background/slides.html#slicing-matrices",
    "href": "lecture1-background/slides.html#slicing-matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Often we will refer to an entire row of a matrix (which is itself a vector) using matrix notation with a single index\n\\[\n\\mathbf{A}_i = \\begin{bmatrix} A_{i1} \\\\ A_{i2} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access a single row similarly.\n\nA[1]\n\narray([1, 1, 2])"
  },
  {
    "objectID": "lecture1-background/slides.html#slicing-matrices-1",
    "href": "lecture1-background/slides.html#slicing-matrices-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "We can refer to an entire column by replacing the row index with a \\(*\\), indicating we are referring to all rows in that column.\n\\[\n\\mathbf{A}_{*i} = \\begin{bmatrix} A_{1i} \\\\ A_{2i} \\\\ \\vdots \\end{bmatrix}\n\\]\nIn numpy we can access an entire column (or row) using the slice operator :, which takes all elements along an axis.\n\nA[:, 1] # Take all elements in column 1\n\narray([5, 1])"
  },
  {
    "objectID": "lecture1-background/slides.html#slicing-matrices-2",
    "href": "lecture1-background/slides.html#slicing-matrices-2",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "We can also use the slice operator to take a subset of elements along an axis.\n\nA[:, 1:3] #Take the second and third columns of A\n\narray([[5, 4],\n       [1, 2]])"
  },
  {
    "objectID": "lecture1-background/slides.html#advanced-slicing-in-numpy",
    "href": "lecture1-background/slides.html#advanced-slicing-in-numpy",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Open-ended slices\n\nprint(\"A[:2]=\", A[:2]) # Take rows up to 2 (not inucluding 2)\nprint(\"A[1:]=\", A[1:]) # Take rows starting at (and including) 1\n\nA[:2]= [[1 2 3 4]\n [5 6 7 8]]\nA[1:]= [[ 5  6  7  8]\n [ 9 10 11 12]]\n\n\nTaking rows and colmns\n\nprint(\"A[0, :]=\", A[0, :])   # first row, all columns\nprint(\"A[:, 1]=\", A[:, 1])   # all rows, second column\nprint(\"A[-1, :]\", A[-1, :])  # all columns of the last row\nprint(\"A[:, -2]\", A[:, -2])  # second to last column of every row\n\nA[0, :]= [1 2 3 4]\nA[:, 1]= [ 2  6 10]\nA[-1, :] [ 9 10 11 12]\nA[:, -2] [ 3  7 11]"
  },
  {
    "objectID": "lecture1-background/slides.html#advanced-slicing-in-numpy-1",
    "href": "lecture1-background/slides.html#advanced-slicing-in-numpy-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "More general slicing with steps\n\nprint(\"A[1,0:2]=\", A[1,0:2])\nprint(\"A[0,0:4:2]=\", A[0,0:4:2])\nprint(\"A[:,0:4:2]=\\n\", A[:,0:4:2])\n\nA[1,0:2]= [5 6]\nA[0,0:4:2]= [1 3]\nA[:,0:4:2]=\n [[ 1  3]\n [ 5  7]\n [ 9 11]]\n\n\nTaking one row and selected columns\n\nprint(\"A[2, [1,4]]=\",A[2, [0,3]])\n\nA[2, [1,4]]= [ 9 12]\n\n\nTaking all rows and selected columns\n\nprint(\"A[:, [1,4]]=\\n\",A[:, [0,3]])\n\nA[:, [1,4]]=\n [[ 1  4]\n [ 5  8]\n [ 9 12]]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-shapes",
    "href": "lecture1-background/slides.html#matrix-shapes",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The matrix \\(\\mathbf{A}\\) above has 2 rows and 3 columns, thus we would specify it’s shape as \\(2\\times3\\). A square matrix has the same number of rows and columns.\n\\[\n\\mathbf{A} = \\begin{bmatrix} 3 & 5 & 4 \\\\ 1 & 1 & 2 \\\\ 4 & 1 & 3 \\end{bmatrix}\n\\]\nWe can access the shape of a matrix in numpy using its shape property.\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nprint(A.shape)\n\n(3, 3)"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-transpose",
    "href": "lecture1-background/slides.html#matrix-transpose",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The transpose of a matrix is an operation that swaps the rows and columns of the matrix. We denote the transpose of a matrix \\(\\mathbf{A}\\) as \\(\\mathbf{A}^T\\).\n\\[\n\\mathbf{A} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\end{bmatrix}, \\quad \\mathbf{A}^T = \\begin{bmatrix} A_{11} & A_{21} \\\\  A_{12} & A_{22} \\\\  A_{13} & A_{23} \\end{bmatrix}\n\\]\nIn numpy we can transpose a matrix by using the T property.\n\nA = np.array([[1, 1, 1], [2, 2, 2]])\nA.T\n\narray([[1, 2],\n       [1, 2],\n       [1, 2]])"
  },
  {
    "objectID": "lecture1-background/slides.html#element-wise-matrix-operations",
    "href": "lecture1-background/slides.html#element-wise-matrix-operations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "As with vectors, many operations on matrices are performed element-wise:\n\\[\n\\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} A_{11} + B_{11}  & A_{12} + B_{12} & A_{13} + B_{13} \\\\ A_{21} + B_{21} & A_{22} + B_{22} & A_{23} + B_{23} \\end{bmatrix}, \\quad \\log\\mathbf{A} = \\begin{bmatrix} \\log A_{11}  & \\log A_{12} & \\log A_{13} \\\\ \\log A_{21} & \\log A_{22} & \\log A_{23} \\end{bmatrix}\n\\]\nScalar-matrix operation are also element-wise:\n\\[\nc\\mathbf{A} = \\begin{bmatrix} cA_{11}  & cA_{12} & cA_{13} \\\\ cA_{21} & cA_{22} & cA_{23} \\end{bmatrix}\n\\]\nIn numpy:\n\nB = 5 * A \nA + B\n\narray([[ 6,  6,  6],\n       [12, 12, 12]])"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-vector-products",
    "href": "lecture1-background/slides.html#matrix-vector-products",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "A matrix-vector product is an operation between a matrix and a vector that produces a new vector. Given a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we write the matrix-vector product as:\n\\[\n\\mathbf{A}\\mathbf{x} = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\end{bmatrix} = \\begin{bmatrix} \\sum_{i=1}^n x_i A_{1i} \\\\  \\sum_{i=1}^n x_i A_{2i} \\\\  \\sum_{i=1}^n x_i A_{3i} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{x}\\cdot  \\mathbf{A}_{1} \\\\ \\mathbf{x}\\cdot  \\mathbf{A}_{2} \\\\ \\mathbf{x} \\cdot \\mathbf{A}_{2} \\end{bmatrix}\n\\]\nIn other words, each entry of the resulting vector is the dot product between \\(\\mathbf{x}\\) and a row of \\(A\\). In numpy we also use np.dot for matrix-vector products:\n\nA = np.array([[3, 5, 4], [1, 1, 2], [4, 1, 3]])\nx = np.array([2, 5, 1])\n\nnp.dot(A, x)\n\narray([35,  9, 16])\n\n\nGeometrically the matrix \\(A\\) defines a transformation that scales and rotates any vector \\(\\mathbf{x}\\) about the origin."
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-vector-products-1",
    "href": "lecture1-background/slides.html#matrix-vector-products-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The number of columns of \\(A\\) must match the size of the vector \\(\\mathbf{x}\\), but if \\(A\\) has a different number of rows, the output will simply have a different size.\n\nnp.dot(A[:2], x)\n\narray([35,  9])"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-multiplication",
    "href": "lecture1-background/slides.html#matrix-multiplication",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Matrix multiplication is a fundamental operation between two matrices. It is defined as:\n\\[\n\\mathbf{A}\\mathbf{B}  = \\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ B_{31} & B_{32} \\end{bmatrix}  =\\begin{bmatrix} \\sum_{i=1}^n A_{1i} B_{i1} & \\sum_{i=1}^n A_{i1}B_{i2} \\\\  \\sum_{i=1}^n A_{2i}B_{i1} & \\sum_{i=1}^n A_{2i}B_{i2}  \\\\  \\sum_{i=1}^n A_{3i}B_{i1} & \\sum_{i=1}^n A_{3i}B_{i2} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2} \\\\ \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{2} \\cdot \\mathbf{B}_{*2}  \\\\ \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*1} & \\mathbf{A}_{1} \\cdot \\mathbf{B}_{*2}  \\end{bmatrix}\n\\]\nThis means that if we multiply an \\(n\\times m\\) matrix \\(\\mathbf{A}\\) with an \\(m\\times k\\) matrix \\(\\mathbf{B}\\) we get an \\(n\\times k\\) matrix \\(\\mathbf{C}\\), such that \\(\\mathbf{C}_{ij}\\) is the dot product of the \\(i\\)-th row of \\(\\mathbf{A}\\) with the \\(j\\)-th row of \\(\\mathbf{B}\\).\nIn numpy we once again use the np.dot function to perform matrix-multiplications.\n\nB = np.array([[2, -1], [3, 1], [-2, 5]])\nC = np.dot(A, B)"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-multiplication-1",
    "href": "lecture1-background/slides.html#matrix-multiplication-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The number of rows of \\(\\mathbf{A}\\) must match the number of columns of \\(\\mathbf{B}\\) for the matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) to be defined. This implies that matrix multiplication is non-communitive:\n\\[\n\\mathbf{A}\\mathbf{B}\\neq \\mathbf{B}\\mathbf{A}\n\\]\nHowever matrix multiplication is associative and distributive:\n\\[\n\\mathbf{A}(\\mathbf{B}\\mathbf{C})=(\\mathbf{A}\\mathbf{B})\\mathbf{C}, \\quad \\mathbf{A}(\\mathbf{B} +\\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-multiplication-2",
    "href": "lecture1-background/slides.html#matrix-multiplication-2",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Matrix multiplication is a composition of linear maps, meaning that if we take the product \\(\\mathbf{A}\\mathbf{B}\\) and apply the resulting matrix to a vector \\(\\mathbf{x}\\), it is equivalent to first transforming \\(\\mathbf{x}\\) with \\(\\mathbf{B}\\) and then with \\(\\mathbf{A}\\). We can state this succinctly as:\n\\[\n(\\mathbf{A}\\mathbf{B})\\mathbf{x} = \\mathbf{A}(\\mathbf{B}\\mathbf{x})\n\\]\nWe can see this in numpy:\n\nx = np.array([1, 3])\nnp.dot(np.dot(A, B), x), np.dot(A, np.dot(B, x))\n\n(array([79, 31, 41]), array([79, 31, 41]))"
  },
  {
    "objectID": "lecture1-background/slides.html#element-wise-multiplication",
    "href": "lecture1-background/slides.html#element-wise-multiplication",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "It is important to note that in numpy the * operator does not perform matrix multiplication, instead it performs element-wise multiplication for both matrices and vectors.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nB = np.array([[1, 2], [1, 2], [1, 2]])\nA * B\n\narray([[1, 2],\n       [2, 4],\n       [3, 6]])\n\n\nIn mathematical notation we will denote element-wise multiplication as:\n\\[\n\\mathbf{A} \\odot \\mathbf{B} = \\begin{bmatrix} A_{11} B_{11}  & A_{12}  B_{12} & A_{13}  B_{13} \\\\ A_{21}  B_{21} & A_{22}  B_{22} & A_{23}  B_{23} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-reductions",
    "href": "lecture1-background/slides.html#matrix-reductions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "As we saw with vectors, we can take the sum of the elements of a matrix using the np.sum function:\n\nnp.sum(A)\n\n12\n\n\nThe result is a scalar of the form:\n\\[\n\\text{sum}(\\mathbf{A}) = \\sum_{i=1}^n\\sum_{j=1}^m A_{ij}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#matrix-reductions-1",
    "href": "lecture1-background/slides.html#matrix-reductions-1",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "In many cases we may wish to take the sum along an axis of a matrix. This operation results in a vector where each entry is the sum of elements from the corresponding row or column of the matrix.\n\\[\n\\text{rowsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{i=1}^n A_{i1} \\\\ \\sum_{i=1}^n A_{i2} \\\\ \\sum_{i=1}^n A_{i3} \\\\ \\vdots\\end{bmatrix}, \\quad \\text{colsum}(\\mathbf{A}) = \\begin{bmatrix} \\sum_{j=1}^m A_{1j} \\\\ \\sum_{j=1}^m A_{2j} \\\\ \\sum_{j=1}^m A_{3j} \\\\ \\vdots\\end{bmatrix}\n\\]\nIn numpy we can specify a sum along an axis by providing an axis argument to np.sum. Setting axis=0 specifies a row-sum, while axis=1 specifies a column sum.\n\nA = np.array([[1, 1], [2, 2], [3, 3]])\nprint(A)\nnp.sum(A, axis=0), np.sum(A, axis=1)\n\n[[1 1]\n [2 2]\n [3 3]]\n\n\n(array([6, 6]), array([2, 4, 6]))"
  },
  {
    "objectID": "lecture1-background/slides.html#other-matrix-reduction-examples",
    "href": "lecture1-background/slides.html#other-matrix-reduction-examples",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "print(np.mean(A))            # Take the mean of all elements in x\nprint(np.std(A, axis=0))     # Take the standard deviation of each column of x\nprint(np.min(A, axis=1))     # Find the minimum element in each row of x\nprint(np.max(A))             # Find the maximum element in x\n\n2.0\n[0.81649658 0.81649658]\n[1 2 3]\n3"
  },
  {
    "objectID": "lecture1-background/slides.html#identity-matrices",
    "href": "lecture1-background/slides.html#identity-matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The identity matrix, denoted at \\(\\mathbf{I}\\) is a special type of square matrix. It is defined as the matrix with \\(1\\) for every diagonal element (\\(\\mathbf{I}_{i=j}=1\\)) and \\(0\\) for every other element (\\(\\mathbf{I}_{i\\neq j}=0\\)). A \\(3\\times 3\\) identity matrix looks like:\n\\[\\mathbf{I} = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}\\]\nThe identify matrix has the unique property that any appropriately sized matrix (or vector) multiplied with \\(\\mathbf{I}\\) will equal itself:\n\\[\n\\mathbf{I}\\mathbf{A} = \\mathbf{A}\n\\]\nIf we think of matrices as linear mappings the identity matrix simply maps any vector to itself.\n\\[\n\\mathbf{I} \\mathbf{x} = \\mathbf{x}\n\\]\nIn numpy we can create an identity matrix using the np.eye function:\n\nI = np.eye(3) # Create a 3x3 identity matrix"
  },
  {
    "objectID": "lecture1-background/slides.html#solving-systems-of-linear-equations",
    "href": "lecture1-background/slides.html#solving-systems-of-linear-equations",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "Consider the matrix-vector product between a matrix \\(\\mathbf{A}\\) and a vector \\(\\mathbf{x}\\), we can denote the result of this multiplication as \\(\\mathbf{b}\\):\n\\[\n\\mathbf{A}\\mathbf{x} =\\mathbf{b}\n\\]\nIn many common cases we will know the matrix \\(\\mathbf{A}\\) and the vector \\(\\mathbf{b}\\), but not the vector \\(\\mathbf{x}\\). In such cases we need to solve this equation for \\(\\mathbf{x}\\):\n\\[\n\\begin{bmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{23} & A_{32} & A_{33} \\end{bmatrix} \\begin{bmatrix} \\textbf{?}\\\\ \\textbf{?}\\\\ \\textbf{?}\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\  b_2 \\\\  b_3 \\end{bmatrix}\n\\]\nIn Numpy:\n\nA = np.array([[3, 1], [-1, 4]])\nb = np.array([-2, 1])\nx = np.linalg.solve(A, b)\n\nNote that in some cases there may not be any \\(\\mathbf{x}\\) that satisfies the equation (or there may be infinitely many). The conditions for this are beyond the scope of this course."
  },
  {
    "objectID": "lecture1-background/slides.html#inverse-matrices",
    "href": "lecture1-background/slides.html#inverse-matrices",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "",
    "text": "The inverse of a square matrix is denoted by \\(\\mathbf{A}^{-1}\\) and is defined as the matrix such that:\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\\]\nThis corresponds to the inverse of the linear map defined by \\(\\mathbf{A}\\). Any vector transformed by \\(\\mathbf{A}\\) can be transformed back by applying the inverse matrix:\n\\[\n\\mathbf{A}^{-1}\\left(\\mathbf{A}\\mathbf{x}\\right) =\\mathbf{x}\n\\]\nWe can also write the solution to a system of linear equations in terms of the inverse, by multiplying both sides by \\(\\mathbf{A}^{-1}\\):\n\\[\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\quad \\longrightarrow \\quad \\mathbf{A}^{-1}(\\mathbf{A}\\mathbf{x})=\\mathbf{A}^{-1}\\mathbf{b} \\quad \\longrightarrow \\quad \\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\n\\]\nIn numpy we can find the inverse of a matrix using the np.linalg.inv function:\n\nA_inv = np.linalg.inv(A)\n\nNote that not every matrix has an inverse! Again, we won’t worry about these cases for now."
  },
  {
    "objectID": "lecture1-background/slides.html#functions",
    "href": "lecture1-background/slides.html#functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Functions",
    "text": "Functions\nA function is a general mapping from one set to another.\n\\[\ny=f(x),\\quad f:\\mathbb{R}\\rightarrow\\mathbb{R}\n\\]\nWe can definite functions as compositions of simple operations. For example we could define a polynomial function as:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\nIn code we can implement functions as, well, functions:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\nf(5)\n\n41"
  },
  {
    "objectID": "lecture1-background/slides.html#derivatives",
    "href": "lecture1-background/slides.html#derivatives",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Derivatives",
    "text": "Derivatives\nThe derivative of a function at input \\(x\\) defines how the function’s output changes as the input changes from \\(x\\). It is equivalent to the slope of the line tangent to the function at the input \\(x\\). We’ll use the notation \\(\\frac{df}{dx}\\) to denote the derivative of the function \\(f\\) at input \\(x\\). Formally:\n\\[\n\\frac{df}{dx} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\n\\]\nIntunitively, this means if we change our input \\(x\\) by some small amount \\(\\epsilon\\), the output of our function will change by approximately \\(\\frac{df}{dx}\\epsilon\\)\n\\[\nf(x+\\epsilon) \\approx f(x)+\\frac{df}{dx}\\epsilon\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#derivative-operator",
    "href": "lecture1-background/slides.html#derivative-operator",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Derivative operator",
    "text": "Derivative operator\nWe can also use the notation \\(\\frac{d}{dx}\\) to denote the derivative operator. This means “find the derivative of the following expression with respect to \\(x\\)”.\n\\[\n\\frac{d}{dx}f(x) = \\frac{df}{dx}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#derivative-functions",
    "href": "lecture1-background/slides.html#derivative-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Derivative functions",
    "text": "Derivative functions\nWe can also talk about the function that maps any input \\(x\\) to the derivative \\(\\frac{df}{dx}\\) we call this the derivative function and denote it as \\(f'(x)\\). So:\n\\[\n\\frac{df}{dx}=f'(x)\n\\]\nGiven a function defined as a composition of basic operations, we can use a set of standard rules to find the corresponding derivative function. For example using the rules \\(\\frac{d}{dx}x^a=ax\\) , \\(\\frac{d}{dx}ax=a\\) and \\(\\frac{d}{dx}a=0\\), we can derive the derivative function for the polynomial above:\n\\[\nf(x) = x^2 + 3x + 1\n\\]\n\\[\nf'(x) = 2x + 3\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#basic-derivative-rules",
    "href": "lecture1-background/slides.html#basic-derivative-rules",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Basic derivative rules",
    "text": "Basic derivative rules\n\n\n\nOperation\nDerivative \\(\\frac{d}{dx}\\)\n\n\n\n\n\\(a\\)\n\\(0\\)\n\n\n\\(ax\\)\n\\(a\\)\n\n\n\\(x^a\\)\n\\(ax\\)\n\n\n\\(\\log(x)\\)\n\\(\\frac{1}{x}\\)\n\n\n\\(e^x\\)\n\\(e^x\\)\n\n\n\\(f(x) + g(x)\\)\n\\(f'(x)+g'(x)\\)\n\n\n\\(f(x)g(x)\\)\n\\(f'(x)g(x) + f(x)g'(x)\\)\n\n\n\\(\\frac{f(x)}{g(x)}\\)\n\\(\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\\)"
  },
  {
    "objectID": "lecture1-background/slides.html#compositions-of-functions",
    "href": "lecture1-background/slides.html#compositions-of-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Compositions of functions",
    "text": "Compositions of functions\nComposing two functions means to apply one function to the output of another, for example we could apply \\(f\\) to the output of \\(g\\):\n\\[\ny = f\\big(g\\left(x\\right)\\big)\n\\]\nThis is easily replicated in code:\n\ndef f(x):\n    return x ** 2 + 3 * x + 1\n\ndef g(x):\n    return 5 * x - 2\n\nf(g(3))\n\n209"
  },
  {
    "objectID": "lecture1-background/slides.html#chain-rule",
    "href": "lecture1-background/slides.html#chain-rule",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Chain rule",
    "text": "Chain rule\nThe chain rule tells us how to find the derivative of a composition of functions like this. We can write the rule either in terms of derivatives or derivative functions\n\\[\n\\frac{d}{dx}f\\big(g\\left(x\\right)\\big) = \\frac{df}{dg}\\frac{dg}{dx} \\quad \\text{or} \\quad \\frac{d}{dx}f\\big(g\\left(x\\right)\\big) =  f'\\big(g\\left(x\\right)\\big)g'\\left(x\\right)\n\\]\nNote that in our derivative notation we’re using \\(f\\) and \\(g\\) to denote the outputs of the respective functions."
  },
  {
    "objectID": "lecture1-background/slides.html#multivariate-functions",
    "href": "lecture1-background/slides.html#multivariate-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Multivariate functions",
    "text": "Multivariate functions\nA function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\nIn code this would look like;\n\ndef f(x, y, z):\n    return x ** 2 + 3 * y + np.log(z)"
  },
  {
    "objectID": "lecture1-background/slides.html#partial-derivatives",
    "href": "lecture1-background/slides.html#partial-derivatives",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Partial derivatives",
    "text": "Partial derivatives\nA partial derivative is the derivative of a multiple-input function with respect to a single input, assuming all other inputs are constant.\n\\[\n\\frac{\\partial f}{\\partial x} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x+\\epsilon, y, z) - f(x,y,z)}{\\epsilon}, \\quad \\frac{\\partial f}{\\partial y} = \\underset{\\epsilon\\rightarrow0}{\\lim} \\frac{f(x, y+\\epsilon, z) - f(x,y,z)}{\\epsilon}\n\\]\nThese partial derivatives tell us how the output of the function changes as we change each of the inputs individually."
  },
  {
    "objectID": "lecture1-background/slides.html#partial-derivative-functions",
    "href": "lecture1-background/slides.html#partial-derivative-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Partial derivative functions",
    "text": "Partial derivative functions\nWe can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.\n\\[\n\\frac{\\partial f}{\\partial x} = f_x'(x, y, z)\n\\]\nWe can derive partial derivative functions using the same set of derivative rules:\n\\[\nf(x, y, z) = x^2 + 3xy - \\log(z)\n\\]\n\\[\nf_x'(x, y, z) = 2x + 3y\n\\]\n\\[\nf_y'(x, y, z) = 3x\n\\]\n\\[\nf_z'(x, y, z) = -\\frac{1}{z}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#functions-of-vectors",
    "href": "lecture1-background/slides.html#functions-of-vectors",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Functions of vectors",
    "text": "Functions of vectors\nWe can also define functions that take vectors (or matrices) as inputs.\n\\[\ny = f(\\mathbf{x}) \\quad f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\]\nHere \\(f\\) is a mapping from length \\(n\\) vectors to real numbers. As a concrete example we could define the function:\n\\[\nf(\\mathbf{x}) = \\sum_{i=1}^n x_i^3 + 1\n\\]\nHere’s the same function in numpy:\n\ndef f(x):\n    return np.sum(x ** 3) + 1\n\nf(np.array([1, 2, 3]))\n\n37\n\n\nNote that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!"
  },
  {
    "objectID": "lecture1-background/slides.html#gradients",
    "href": "lecture1-background/slides.html#gradients",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Gradients",
    "text": "Gradients\nThe gradient of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.\n\\[\n\\frac{df}{d\\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\frac{\\partial f}{\\partial x_3} \\\\ \\vdots \\end{bmatrix}\n\\]\nThe gradient is a vector that tangent to the function \\(f\\) at the input \\(\\mathbf{x}\\). Just as with derivatives, this means that the gradient defines a linear approximation to the function at the point \\(\\mathbf{x}\\).\n\\[\nf(\\mathbf{x}+\\mathbf{\\epsilon}) \\approx f(\\mathbf{x}) + \\frac{df}{d\\mathbf{x}} \\cdot \\mathbf{\\epsilon}\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#gradient-functions",
    "href": "lecture1-background/slides.html#gradient-functions",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Gradient functions",
    "text": "Gradient functions\nJust as with derivatives and partial derivatives, we can define a gradient function that maps an input vector \\(\\mathbf{x}\\) to the gradient of the function \\(f\\) at \\(\\mathbf{x}\\) as:\n\\[\n\\frac{df}{d\\mathbf{x}}=\\nabla f(\\mathbf{x})\n\\]\nHere \\(\\nabla f\\) is the gradient function for \\(f\\). If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:\n\\[\n\\frac{df}{d\\mathbf{x}}= \\nabla_{\\mathbf{x}} f(\\mathbf{x}, \\mathbf{y}), \\quad \\frac{df}{d\\mathbf{y}}= \\nabla_{\\mathbf{y}} f(\\mathbf{x}, \\mathbf{y})\n\\]"
  },
  {
    "objectID": "lecture1-background/slides.html#getting-started",
    "href": "lecture1-background/slides.html#getting-started",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Getting started",
    "text": "Getting started\nThe standard way to import MatPlotLib is:\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "lecture1-background/slides.html#scatterplots",
    "href": "lecture1-background/slides.html#scatterplots",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Scatterplots",
    "text": "Scatterplots\nPlotting a scatter of data points:\n\nx_values = np.random.rand(1,10)   # unformly in [0,1)\ny_values = np.random.randn(1,10)  # Gaussian distribution\nplt.plot(x_values, y_values, 'ko');\n\n\n\n\nThe string determines the plot appearance -- in this case, black circles. You can use color strings (‘r’, ‘g’, ‘b’, ‘m’, ‘c’, ‘y’, ...) or use the “Color” keyword to specify an RGB color. Marker appearance (‘o’,‘s’,‘v’,‘.’, ...) controls how the points look."
  },
  {
    "objectID": "lecture1-background/slides.html#line-plots",
    "href": "lecture1-background/slides.html#line-plots",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Line plots",
    "text": "Line plots\nIf we connect points using a line appearance specification (‘-’,‘--’,‘:’,...), it will not look very good, because the points are not ordered in any meaningful way. Let’s try a line plot using an ordered sequence of x values:\n\nx_values = np.linspace(0,8,100)\ny_values = np.sin(x_values)\nplt.plot(x_values,y_values,'b');\n\n\n\n\nThis is actually a plot of a large number of points (100), with no marker shape and connected by a solid line."
  },
  {
    "objectID": "lecture1-background/slides.html#multiple-plots",
    "href": "lecture1-background/slides.html#multiple-plots",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Multiple plots",
    "text": "Multiple plots\nFor plotting multiple point sets or curves, you can pass more vectors into the plot function, or call the function multiple times:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny2 = (x_values - 3)**2 / 12   # a simple quadratic curve\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-', x_values, y2, 'g--');  # plot two curves\nplt.plot(x_values, y3, 'r:'); # add a curve to the plot"
  },
  {
    "objectID": "lecture1-background/slides.html#plot-ranges",
    "href": "lecture1-background/slides.html#plot-ranges",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Plot ranges",
    "text": "Plot ranges\nYou may want to explicitly set the plot ranges -- perhaps the most common pattern is to plot something, get the plot’s ranges, and then restore them later after plotting another function:\n\nx_values = np.linspace(0,8,100)\ny1 = np.sin(x_values)         # sinusoidal function\ny3 = 0.5*x_values - 1.0       # a simple linear function\n\nplt.plot(x_values, y1, 'b-') \nax = plt.axis()               # get the x and y axis ranges\nprint(ax)\n# now plot something else (which will change the axis ranges):\nplt.plot(x_values, y3, 'r:'); # add the linear curve\nplt.axis(ax);                 # restore the original plot's axis ranges\n\n(-0.4, 8.4, -1.099652011574681, 1.0998559934443881)"
  },
  {
    "objectID": "lecture1-background/slides.html#histograms",
    "href": "lecture1-background/slides.html#histograms",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Histograms",
    "text": "Histograms\nHistograms are also useful visualizations:\n\nplt.hist(y2, bins=20);\n\n\n\n\nThe outputs of hist include the bin locations, the number of data in each bin, and the “handles” to the plot elements to manipulate their appearance, if desired."
  },
  {
    "objectID": "lecture1-background/slides.html#subplots-and-plot-sizes",
    "href": "lecture1-background/slides.html#subplots-and-plot-sizes",
    "title": "Lecture 1: Background and Prerequisites",
    "section": "Subplots and plot sizes",
    "text": "Subplots and plot sizes\nIt is often useful to put more than one plot together in a group; you can do this using the subplot function. There are various options; for example, “sharex” and “sharey” allow multiple plots to share a single axis range (or, you can set it manually, of course).\n\nfig,ax = plt.subplots(1,3, figsize=(8.0, 2.0))      # make a 1 x 3 grid of plots:\nax[0].plot(x_values, y1, 'b-');   # plot y1 in the first subplot\nax[1].plot(x_values, y2, 'g--');  #   y2 in the 2nd\nax[2].plot(x_values, y3, 'r:');   #   and y3 in the last"
  }
]