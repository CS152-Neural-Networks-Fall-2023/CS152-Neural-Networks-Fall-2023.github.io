<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.319">

  <title>CS 152: Neural Networks - Lecture 3: Logistic regression</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Lecture 3: Logistic regression</h1>

<div class="quarto-title-authors">
</div>

</section>
<section>
<section id="vector-calculus-review" class="title-slide slide level1 center">
<h1>Vector Calculus Review</h1>

</section>
<section id="partial-derivatives" class="slide level2">
<h2>Partial derivatives</h2>
<p>A function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:</p>
<p><span class="math display">\[
f(x, y, z) = x^2 + 3xy - \log(z)
\]</span></p>
<p>In code this would look like;</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> f(x, y, z):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="cf">return</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> y <span class="op">+</span> np.log(z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A <strong>partial derivative</strong> is the derivative of a multiple-input function with respect to a single input, <em>assuming all other inputs are constant</em>. We will explore the implications of that condition later on in this course. For now, we will simply view partial derivatives as a straightforward extension of derivatives, using the modified notation <span class="math inline">\(\frac{\partial}{\partial x}\)</span>.</p>
<p>More formally, we can define the partial derivative with respect to each input of a function as:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = \underset{\epsilon\rightarrow0}{\lim} \frac{f(x+\epsilon, y, z) - f(x,y,z)}{\epsilon}, \quad \frac{\partial f}{\partial y} = \underset{\epsilon\rightarrow0}{\lim} \frac{f(x, y+\epsilon, z) - f(x,y,z)}{\epsilon}
\]</span></p>
<p>These partial derivatives tell us how the output of the function changes as we change each of the inputs individually.</p>
</section>
<section id="partial-derivative-functions" class="slide level2">
<h2>Partial derivative functions</h2>
<p>We can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = f_x'(x, y, z)
\]</span></p>
<p>We can derive partial derivative functions using the same set of derivative rules:</p>
<p><span class="math display">\[
f(x, y, z) = x^2 + 3xy - \log(z)
\]</span></p>
<p><span class="math display">\[
f_x'(x, y, z) = 2x + 3y
\]</span></p>
<p><span class="math display">\[
f_y'(x, y, z) = 3x
\]</span></p>
<p><span class="math display">\[
f_z'(x, y, z) = -\frac{1}{z}
\]</span></p>
</section>
<section id="functions-of-vectors" class="slide level2">
<h2>Functions of vectors</h2>
<p>We can also define functions that take vectors (or matrices) as inputs.</p>
<p><span class="math display">\[
y = f(\mathbf{x}) \quad f: \mathbb{R}^n \rightarrow \mathbb{R}
\]</span></p>
<p>Here <span class="math inline">\(f\)</span> is a mapping from length <span class="math inline">\(n\)</span> vectors to real numbers. As a concrete example we could define the function:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_i^3 + 1
\]</span></p>
<p>Here’s the same function in numpy:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">3</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>f(np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>37</code></pre>
</div>
</div>
<p>Note that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!</p>
</section>
<section id="gradients" class="slide level2">
<h2>Gradients</h2>
<p>The <strong>gradient</strong> of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \frac{\partial f}{\partial x_3} \\ \vdots \end{bmatrix}
\]</span></p>
<p>The gradient is a vector that <em>tangent</em> to the function <span class="math inline">\(f\)</span> at the input <span class="math inline">\(\mathbf{x}\)</span>. Just as with derivatives, this means that the gradient defines a <em>linear</em> <em>approximation</em> to the function at the point <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p><span class="math display">\[
f(\mathbf{x}+\mathbf{\epsilon}) \approx f(\mathbf{x}) + \frac{df}{d\mathbf{x}} \cdot \mathbf{\epsilon}
\]</span></p>
<p>Where <span class="math inline">\(\mathbf{\epsilon}\)</span> is now a small vector. Intuitively, this means that if we take a small step <em>in any direction</em> as defined by <span class="math inline">\(\mathbf{\epsilon}\)</span>, the gradient will approximate the change in the output of the function. Becuase we are now in more than 1 dimension, this approximation defines a <em>plane</em> in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>Another extremely important property of the gradient is that it points in the direction of maximum change in the function. Meaning that if we were to take an infinitesimal step <span class="math inline">\(\mathbf{\epsilon}\)</span> from <span class="math inline">\(\mathbf{x}\)</span> in any direction, stepping in the gradient direction would give use the maximum value of <span class="math inline">\(f(\mathbf{x} +\mathbf{\epsilon})\)</span>. We can see this from the approximation above: <span class="math inline">\(f(\mathbf{x} +\mathbf{\epsilon})\)</span> is maximized when <span class="math inline">\(\frac{df}{d\mathbf{x}}\)</span> and <span class="math inline">\(\mathbf{\epsilon}\)</span> are colinear.</p>
<p>We can define the gradient in this sense this more formally as:</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}}= \underset{\gamma \rightarrow 0}{\lim}\ \underset{\|\mathbf{\epsilon}\|_2 &lt; \gamma}{\max} \frac{f(\mathbf{x} + \mathbf{\epsilon}) - f(\mathbf{x})}{\|\mathbf{\epsilon}\|_2}
\]</span></p>
</section>
<section id="gradient-functions" class="slide level2">
<h2>Gradient functions</h2>
<p>Just as with derivatives and partial derivatives, we can define a <strong>gradient function</strong> that maps an input vector <span class="math inline">\(\mathbf{x}\)</span> to the gradient of the function <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{x}\)</span> as:</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}}=\nabla f(\mathbf{x})
\]</span></p>
<p>Here <span class="math inline">\(\nabla f\)</span> is the gradient function for <span class="math inline">\(f\)</span>. If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}}= \nabla_{\mathbf{x}} f(\mathbf{x}, \mathbf{y}), \quad \frac{df}{d\mathbf{y}}= \nabla_{\mathbf{y}} f(\mathbf{x}, \mathbf{y})
\]</span></p>
<p>Note that the gradient function is a mapping from <span class="math inline">\(\mathbb{R}^n\rightarrow\mathbb{R}^n\)</span>, meaning that it returns a vector with the same size as the input.</p>
</section>
<section id="vector-notation" class="slide level2">
<h2>Vector notation</h2>
<p>As we’ve seen a vector is a 1-dimensional set of numbers. For example, we can write the vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^3\)</span> as:</p>
<p><span class="math display">\[\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\]</span></p>
<p>Recall that a vector can also be seen as either an <span class="math inline">\(n \times 1\)</span> matrix (column vector) or a <span class="math inline">\(1 \times n\)</span> matrix (row vector).</p>
<p><span class="math display">\[\text{Column vector: } \mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}, \quad \text{Row vector: } \mathbf{x} = \begin{bmatrix}
x_1 &amp; x_2 &amp; x_3
\end{bmatrix}\]</span></p>
<p>Note that we may use the same notation for both as they refer to the same concept (a vector).</p>
</section>
<section id="vector-notation-1" class="slide level2">
<h2>Vector notation</h2>
<p>The difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: <span class="math display">\[\text{Matrix-vector: }\mathbf{A}\mathbf{x} = \mathbf{b}, \quad \text{Vector-matrix: }\mathbf{x}\mathbf{A}^T= \mathbf{b}\]</span> In <em>matrix-vector multiplication</em> we treat <span class="math inline">\(\textbf{x}\)</span> as a column vector (<span class="math inline">\(n \times 1\)</span> matrix), while in <em>vector-matrix multiplication</em> we treat it as a row vector (<span class="math inline">\(n \times 1\)</span> matrix). Transposing <span class="math inline">\(A\)</span> for left multiplication ensures that the two forms give the same answer.</p>
<p>In Numpy the <code>np.dot</code> function works in this way. Given a matrix <code>A</code> and a 1-dimensional vector <code>x</code>, performing both operations will give the same result (another 1-dimensional vector):</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>A <span class="op">=</span> np.array([[ <span class="dv">1</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb4-2"><a href="#cb4-2"></a>              [ <span class="dv">5</span>, <span class="op">-</span><span class="dv">3</span>,  <span class="dv">2</span>],</span>
<span id="cb4-3"><a href="#cb4-3"></a>              [<span class="op">-</span><span class="dv">2</span>,  <span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>],</span>
<span id="cb4-4"><a href="#cb4-4"></a>             ])</span>
<span id="cb4-5"><a href="#cb4-5"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>])</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a>Ax   <span class="op">=</span> np.dot(A, x)</span>
<span id="cb4-8"><a href="#cb4-8"></a>xA_T <span class="op">=</span> np.dot(x, A.T)</span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="bu">print</span>(<span class="st">'Ax   = '</span>, Ax)</span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="bu">print</span>(<span class="st">'xA^T = '</span>, xA_T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ax   =  [-4 13 -8]
xA^T =  [-4 13 -8]</code></pre>
</div>
</div>
</section>
<section id="vector-notation-revisited" class="slide level2">
<h2>Vector notation revisited</h2>
<p>It often is much simpler to explicitly define vectors as being either row or column vectors. The common convention in machine learning is to assume that all vectors are column vectors (<span class="math inline">\(n \times 1\)</span> matricies) and thus a row vector ( <span class="math inline">\(1\times n\)</span> matrix) is obtained by explicit transposition:</p>
<p><span class="math display">\[\text{Column vector: } \mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}, \quad \text{Row vector: } \mathbf{x}^T = \begin{bmatrix}
x_1 &amp; x_2 &amp; x_3
\end{bmatrix}\]</span></p>
<p>In this case, we would rewrite the matrix-vector and vector-matrix products we saw above as:</p>
<p><span class="math display">\[\text{Matrix-vector: }\mathbf{A}\mathbf{x} = \mathbf{b}, \quad \text{Vector-matrix: }\mathbf{x}^T\mathbf{A}^T= \mathbf{b}^T\]</span></p>
<p>In Numpy, we can make a vector into an explicit column or row vector by inserting a new dimension, either with the <code>np.expand_dims</code> function or with the indexing operator:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>row_x <span class="op">=</span> np.expand_dims(x, axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># Add a new leading dimension to x</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>row_x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>array([[ 1, -2,  1]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>column_x <span class="op">=</span> np.expand_dims(x, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># Add a new second dimension to x</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="cf">assert</span> np.<span class="bu">all</span>(column_x.T <span class="op">==</span> row_x)</span>
<span id="cb8-3"><a href="#cb8-3"></a>column_x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>array([[ 1],
       [-2],
       [ 1]])</code></pre>
</div>
</div>
<p>Alternatively:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>row_x <span class="op">=</span> x[<span class="va">None</span>, :]</span>
<span id="cb10-2"><a href="#cb10-2"></a>column_x <span class="op">=</span> x[:, <span class="va">None</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section>
<section>
<section id="classification" class="title-slide slide level1 center">
<h1>Classification</h1>

</section>
<section id="categorical-outputs" class="slide level2">
<h2>Categorical outputs</h2>
<p>In the last lecture we considered approximating functions of the form:</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in\mathbb{R}
\]</span></p>
<p>In many real-world problems, the output we want to model is not a continuous value, but a <em>categorical</em> value.</p>

<img data-src="pictures/catdogmouse.png" width="776" class="r-stretch quarto-figure-center"></section>
<section id="binary-outputs" class="slide level2">
<h2>Binary outputs</h2>
<p>In the simplest case there are two possible outputs.</p>

<img data-src="pictures/catdog.png" width="822" class="r-stretch quarto-figure-center"><p>We use the set <span class="math inline">\(\{0, 1\}\)</span> to denote the possible outputs for a binary categorical function.</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in \{0, 1\}
\]</span></p>
<p>We might say that <span class="math inline">\(0 = \textbf{"cat"}\)</span> and <span class="math inline">\(1=\textbf{"dog"}\)</span>.</p>
<p>We call prediction of a categorical output <strong>classification</strong>.</p>
</section>
<section id="visualizing-categorical-functions" class="slide level2">
<h2>Visualizing categorical functions</h2>
<p>Consider the fuel efficiency example from the previous lecture.</p>
<p><span class="math display">\[
\text{Input: } \mathbf{x}_i= \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph}  \end{bmatrix}, \quad \text{Output: } y_i = \begin{cases} 1: \text{Meets target } (MPG \geq 30) \\ 0:
\text{Fails to meet target } (MPG &lt; 30) \\  \end{cases}
\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-9-output-1.png" class="r-stretch"></section>
<section id="visualizing-categorical-functions-1" class="slide level2">
<h2>Visualizing categorical functions</h2>
<p>With this new output definition our dataset will look like:</p>
<p><span class="math display">\[
\text{Honda Accord: } \begin{bmatrix} \text{Weight:} &amp; \text{2500 lbs} \\ \text{Horsepower:} &amp; \text{ 123 HP} \\ \text{Displacement:} &amp; \text{ 2.4 L} \\ \text{0-60mph:} &amp; \text{ 7.8 Sec} \end{bmatrix} \longrightarrow \text{1   (Meets target)}
\]</span></p>
<p><span class="math display">\[
\text{Dodge Aspen: } \begin{bmatrix} \text{Weight:} &amp; \text{3800 lbs} \\ \text{Horsepower:} &amp; \text{ 155 HP} \\ \text{Displacement:} &amp; \text{ 3.2 L} \\ \text{0-60mph:} &amp; \text{ 6.8 Sec} \end{bmatrix} \longrightarrow  \text{0   (Does not meet target)}
\]</span></p>
<p><span class="math display">\[
\vdots \quad \vdots
\]</span></p>
</section>
<section id="visualizing-categorical-functions-2" class="slide level2">
<h2>Visualizing categorical functions</h2>

<img data-src="slides_files/figure-revealjs/cell-10-output-1.png" class="r-stretch"></section>
<section id="making-binary-predictions" class="slide level2">
<h2>Making binary predictions</h2>
<p>We could fit a linear regression model to our binary data, by simply treating the labels <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> as real-valued outputs.</p>

<img data-src="slides_files/figure-revealjs/cell-11-output-1.png" class="r-stretch"></section>
<section id="making-binary-predictions-1" class="slide level2">
<h2>Making binary predictions</h2>
<p>A more suitable prediction function would <em>only</em> output one of our two possible labels <span class="math inline">\(\{0, 1\}\)</span>. Using a <em>cutoff</em> (typically 0), as follows:</p>
<p><span class="math display">\[
f(\mathbf{x})=\mathbf{x}^T\mathbf{w} \quad \longrightarrow \quad f(\mathbf{x})=\begin{cases} 1\ \text{   if   }\ \mathbf{x}^T\mathbf{w} \geq 0 \\
0\ \text{   if   }\ \mathbf{x}^T\mathbf{w} &lt; 0\end{cases}
\]</span></p>
<p>We might also write this as:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbb{I}(\mathbf{x}^T\mathbf{w} \geq 0)
\]</span></p>
<p>Where <span class="math inline">\(\mathbb{I}\)</span> is an <em>indicator function</em> that is simply <span class="math inline">\(1\)</span> if the boolean expression is true and <span class="math inline">\(0\)</span> otherwise.</p>
</section>
<section id="making-binary-predictions-2" class="slide level2">
<h2>Making binary predictions</h2>
<p>This gives us a prediction function that looks like step function in 1 dimension:</p>

<img data-src="slides_files/figure-revealjs/cell-12-output-1.png" class="r-stretch"></section>
<section id="making-binary-predictions-3" class="slide level2">
<h2>Making binary predictions</h2>
<p>For our efficiency example, the binary prediction function can be written as:</p>
<p><span class="math display">\[
\text{Meets target} = f(\mathbf{x})=
\]</span></p>
<p><span class="math display">\[
\big((\text{weight})w_1 + (\text{horsepower})w_2 + (\text{displacement})w_3 + (\text{0-60mph})w_4 + b\big) \geq 0
\]</span></p>
<p>Or in matrix notation:</p>
<p><span class="math display">\[
f(\mathbf{x})= \left( \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph} \\ 1 \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ w_2\\ w_3 \\ w_4\\ b\end{bmatrix} \geq  0\right)
\]</span></p>
<p>In this form we can see that the <em>sign</em> of each weight parameter determines whether the corresponding feature is more predictive of label <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> and to what extent.</p>
</section>
<section id="making-binary-predictions-4" class="slide level2">
<h2>Making binary predictions</h2>
<p>This has a geometric interpretation if we think of <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> as vectors. If the angle between <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> is in the range <span class="math inline">\([-\frac{\pi}{2}, \frac{\pi}{2}]\)</span> (or <span class="math inline">\([-90^o, 90^o]\)</span> in degrees), then the prediction will be <span class="math inline">\(1\)</span>.</p>

<img data-src="slides_files/figure-revealjs/cell-13-output-1.png" class="r-stretch"></section>
<section id="descision-boundaries" class="slide level2">
<h2>Descision boundaries</h2>
<p>We can visualize a classification dataset as a function of two variables using color to distinguish between observations with each label. In this example we’ll look at weight and engine displacement.</p>

<img data-src="slides_files/figure-revealjs/cell-14-output-1.png" class="r-stretch"></section>
<section id="descision-boundaries-1" class="slide level2">
<h2>Descision boundaries</h2>
<p>The <strong>decision boundary</strong> is the border between regions of the input space corresponding to each prediction. For a linear classification model the decision boundary is line or plane:</p>
<p><span class="math display">\[\mathbf{x}^T\mathbf{w}=0\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-15-output-1.png" class="r-stretch"></section>
<section id="measuring-error" class="slide level2">
<h2>Measuring error</h2>
<p>A natural measure for error for binary classifiers is <strong>accuracy</strong>. The <em>accuracy</em> of a prediction function is the fraction of observations where the prediction matches the true output:</p>
<p><span class="math display">\[
\textbf{Accuracy} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}\big(f(\mathbf{x}_i) = y_i\big)
\]</span></p>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-stdout">
<pre><code>Model accuracy: 0.8291</code></pre>
</div>

</div>
<img data-src="slides_files/figure-revealjs/cell-16-output-2.png" class="r-stretch"></section>
<section id="defining-a-loss-function" class="slide level2">
<h2>Defining a loss function</h2>
<p>In the last lecture we saw that we can find an optimal choice of parameters <span class="math inline">\(\mathbf{w}\)</span> for a linear regression model by defining a measure of <em>error</em> or <em>loss</em> for our approximation on our dataset and minimizing that error as a function of <span class="math inline">\(\mathbf{w}\)</span>, either directly or with gradient descent.</p>
<p>We might consider using (negative) accuracy as a loss function or the same mean squared error that we used for linear regression. However, if we tried to minimize one of these losses with gradient descent, we would run into a fundamental problem: the derivative of the prediction function is <em>always</em> <span class="math inline">\(0\)</span>. We can see this if we</p>
</section></section>
<section>
<section id="logistic-regression" class="title-slide slide level1 center">
<h1>Logistic Regression</h1>

</section>
<section id="the-bernoulli-distribution" class="slide level2">
<h2>The Bernoulli distribution</h2>
<p>The <strong>Beroulli distribution</strong> is a distribution over binary outcomes (0 or 1). It is parameterized simply by <span class="math inline">\(q=p(y=1)\)</span></p>
<p><span class="math display">\[
p(y)=\begin{cases} q\quad\ \ \ \ \ \ \  \text{if }\ y=1\\
1-q\quad \text{if }\ y=0\\
\end{cases}\quad q\in[0,1],\ y\in\{0, 1\}
\]</span></p>
<p>We can also write this as:</p>
<p><span class="math display">\[
p(y) = q^y(1-q)^{1-y}
\]</span></p>
<p>The <em>log-probability</em> or <em>log-likelihood</em> is then:</p>
<p><span class="math display">\[
\log p(y) = y\log q + (1-y)\log(1-q)
\]</span></p>
</section>
<section id="a-probabilistic-model-for-binary-classification" class="slide level2">
<h2>A probabilistic model for binary classification</h2>
<p>Last lecture saw a probabilistic model for linear regression could be defined as:</p>
<p><span class="math display">\[
y_i \sim \mathcal{N}(\mathbf{x}^T\mathbf{w},\ \sigma^2)
\]</span></p>
<p>We’d ideally like to define a similar model for the case of binary outputs using the Bernoulli distribution. However we need to enforce that the Bernoulli parameter is in <span class="math inline">\([0,1]\)</span></p>
<p><span class="math display">\[
\mathbf{x}^T\mathbf{w}\notin [0, 1] \quad \longrightarrow \quad y_i \sim \mathbf{Bernoulli}(\mathbf{ q=? })\quad
\]</span></p>
<p>So we need a function that can map <span class="math inline">\(\mathbf{x}^T\mathbf{w}\)</span> to <span class="math inline">\([0,1]\)</span></p>
<p><span class="math display">\[
\textbf{Need }\ f(x):\ \mathbb{R} \longrightarrow [0,1]
\]</span></p>
<p><span class="math display">\[
\textbf{Input: } x \in \mathbb{R} \longrightarrow \textbf{Output: } y \in [0,1]
\]</span></p>
</section>
<section id="sigmoid-function" class="slide level2">
<h2>Sigmoid function</h2>
<p>The <strong>sigmoid</strong> (or logistic) function is a convenient choice</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}
\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-17-output-1.png" class="r-stretch"></section>
<section id="sigmoid-function-1" class="slide level2">
<h2>Sigmoid function</h2>
<p>The sigmoid function has some nice properties</p>
<p><span class="math display">\[
\sigma(0) = 0.5
\]</span></p>
<p><span class="math display">\[
1-\sigma(x) = \sigma(-x)
\]</span></p>
<p><span class="math display">\[
\frac{d}{dx}\sigma(x) = \sigma(x)\big(1-\sigma(x)\big)
\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-18-output-1.png" class="r-stretch"></section>
<section id="a-probabilistic-model-for-binary-classification-1" class="slide level2">
<h2>A probabilistic model for binary classification</h2>
<p>With the sigmoid function we can define our probabilistic model</p>
<p><span class="math display">\[
y_i \sim \mathbf{Bernoulli}\big(\mathbf{ \sigma(\mathbf{x}_i^T\mathbf{w} })\big)
\]</span></p>
<p><span class="math display">\[
p(y_i = 1) = \sigma(\mathbf{x}_i^T\mathbf{w}), \quad p(y_i=0)=1-\sigma(\mathbf{x}_i^T\mathbf{w})=\sigma(-\mathbf{x}_i^T\mathbf{w})
\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-19-output-1.png" class="r-stretch"></section>
<section id="a-probabilistic-model-for-binary-classification-2" class="slide level2">
<h2>A probabilistic model for binary classification</h2>
<p>We see that if we choose a probability cutoff of <span class="math inline">\(0.5\)</span>, our decision boundary doesn’t change!</p>
<p><span class="math display">\[
p(y_i=1)\geq 0.5 \quad \longrightarrow \quad \mathbf{x}^T\mathbf{w}\geq 0
\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-20-output-1.png" class="r-stretch"></section>
<section id="maximum-likelihood-estimation" class="slide level2">
<h2>Maximum likelihood estimation</h2>
<p>Let’s review how to find the parameters of a model using <strong>maximum likelihood estimation</strong></p>
<p><span class="math display">\[
\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmax}} \ p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) =\underset{\mathbf{w}}{\text{argmax}} \ p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) \]</span></p>
<p>Generally our model also assumes <em>conditional independence</em> across observations so:</p>
<p><span class="math display">\[
p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) = \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
</section>
<section id="maximum-likelihood-estimation-1" class="slide level2">
<h2>Maximum likelihood estimation</h2>
<p>For convenience, it is typical to frame the optimal value in terms of the <em>negative log-likelihood</em> rather than the likelihood, but the two are equivalent.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{argmax}} \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w}) = \underset{\mathbf{w}}{\text{argmin}} - \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w}) = \textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})
\]</span></p>
<p>We see that the negative log-likelihood is a natural <em>loss function</em> to optimize to find <span class="math inline">\(\mathbf{w}^*\)</span>.</p>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{w}) =\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})=- \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
</section>
<section id="maximum-likelihood-estimation-2" class="slide level2">
<h2>Maximum likelihood estimation</h2>
<p>We can now write the maximum likelihood loss for logistic regression.</p>
<p><span class="math display">\[
\mathbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N y_i\log \sigma(\mathbf{x}^T\mathbf{w}) + (1-y_i)\log(1-\sigma(\mathbf{x}^T\mathbf{w}))
\]</span></p>
<p><span class="math display">\[
=-\sum_{i=1}^N y_i\log \sigma(\mathbf{x}^T\mathbf{w}) + (1-y_i)\log \sigma(-\mathbf{x}^T\mathbf{w})
\]</span></p>
<p><span class="math display">\[
\textbf{Ideally: }\ p(y_i\mid \mathbf{x}_i, \mathbf{w})=1,\ \forall (\mathbf{x}_i, y_i)\in \mathcal{D}
\]</span></p>
<p><span class="math display">\[
\mathbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) = -\mathbf{y}^T\log \sigma(\mathbf{X}\mathbf{w})
\]</span></p>
</section>
<section id="comparing-loss-functions" class="slide level2">
<h2>Comparing loss functions</h2>
<p>Loss for <span class="math inline">\(y=0\)</span> as a function of <span class="math inline">\(z=\mathbf{x}^T\mathbf{w}\)</span></p>
<p><span class="math display">\[
\textbf{Let: }\ z=\mathbf{x}^T\mathbf{w}
\]</span></p>

<img data-src="slides_files/figure-revealjs/cell-21-output-1.png" class="r-stretch"></section></section>
<section>
<section id="multi-class-classification" class="title-slide slide level1 center">
<h1>Multi-class classification</h1>

</section>
<section id="multi-class-prediction-functions" class="slide level2">
<h2>Multi-class prediction functions</h2>
<p>We will typically use a set of integers <span class="math inline">\(\{0,1, 2,...,C\}\)</span> to denote the possible outputs for a general categorical function.</p>

<img data-src="pictures/catdogmouse.png" class="r-stretch quarto-figure-center"><p>Therefore in general we are considering functions of the form:</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in \{0, 1, 2, ...,C\}
\]</span></p>
</section>
<section id="multi-class-prediction-functions-1" class="slide level2">
<h2>Multi-class prediction functions</h2>
<p>It’s important to note that we are <em>not</em> assuming that the <em>ordering</em> of labels is meaningful For instance if we’re classifying images of animals we might set the labels such that:</p>
<p><span class="math display">\[
\textbf{0:  Cat},\quad
\textbf{1:  Dog},\quad
\textbf{2:  Mouse}
\]</span></p>
<p>But this is equally valid:</p>
<p><span class="math display">\[
\textbf{0:  Dog},\quad
\textbf{1:  Mouse},\quad
\textbf{2:  Cat}
\]</span></p>
</section>
<section id="multi-class-prediction-functions-2" class="slide level2">
<h2>Multi-class prediction functions</h2>
<p>A simple prediction function for multiclass classification is:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0...C\}}{\text{argmax}}\ \mathbf{x}^T\mathbf{w}_c
\]</span></p>
<p>Alternatively:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0...C\}}{\text{argmax}}\ (\mathbf{x}^T\mathbf{W})_c, \quad \mathbf{W} \in \mathbb{R}^{d\times C}
\]</span></p>

<img data-src="images/paste-1.png" class="r-stretch"></section>
<section id="multi-class-prediction-functions-3" class="slide level2">
<h2>Multi-class prediction functions</h2>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0...C\}}{\text{argmax}}\ (\mathbf{x}^T\mathbf{W})_c, \quad \mathbf{W} \in \mathbb{R}^{d\times C}
\]</span></p>
<p>This function reduces to the same one we saw before for the case of <span class="math inline">\(C=2\)</span>:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0,1\}}{\text{argmax}}\ (\mathbf{x}^T\mathbf{W})_c = \mathbb{I}(\mathbf{x}^T\mathbf{w}_1 - \mathbf{x}^T\mathbf{w}_0 \geq 0)
\]</span></p>
<p><span class="math display">\[
=\mathbb{I}(\mathbf{x}^T(\mathbf{w}_1 - \mathbf{w}_0) \geq 0) \quad \longrightarrow \quad \mathbb{I}(\mathbf{x}^T\mathbf{w} \geq 0), \quad \mathbf{w}=\mathbf{w}_1-\mathbf{w}_0
\]</span></p>
<p>This means the boundary between any two predictions is linear.</p>

<img data-src="images/paste-2.png" class="r-stretch"></section>
<section id="categorical-distribution" class="slide level2">
<h2>Categorical distribution</h2>
<p>The <strong>Categorical distribution</strong> is a distribution over several distinct (discrete) outcomes. It’s parameterized by a <em>vector</em> of probabilities for each outcome:</p>
<p><span class="math display">\[
p(y=c) = q_c, \quad y\in \{1...C\}
\]</span></p>
<p><span class="math display">\[
\mathbf{q} \in \mathbb{R}^C\quad q_c \geq 0\ \forall c\in \{1...C\}\quad \sum_{c=1}^C q_c=1
\]</span></p>
<p>It can also be written as:</p>
<p><span class="math display">\[
p(y)=\prod q_c^{\mathbb{I}(y=c)}
\]</span></p>
<p>The log-likelihood is then:</p>
<p><span class="math display">\[
\log p(y) = \sum_{c=1}^C \mathbb{I}(y=c)\log q_c = \log q_y
\]</span></p>
</section>
<section id="a-probabilistic-model-for-multi-class-classification" class="slide level2">
<h2>A probabilistic model for multi-class classification</h2>
<p>Once again we need to translate our linear function output into a valid parameter for this distribution:</p>
<p><span class="math display">\[
y_i\sim \mathbf{Categorical}(\mathbf{q}=?)
\]</span></p>
<p><span class="math display">\[
\mathbf{q}=\mathbf{x}^T\mathbf{W}?
\]</span></p>
<p><span class="math display">\[
\mathbf{x}^T\mathbf{W}\in \mathbb{R}^C,\quad  q_c \ngeq 0\ \forall c\in \{1...C\}, \quad \sum_{c=1}^C q_c\neq1
\]</span></p>
<p><span class="math display">\[
\textbf{Need }\ f(\mathbf{x}):\ \mathbb{R}^C \longrightarrow [0,\infty)^C,\ \sum_{i=1}^Cf(\mathbf{x})_c = 1
\]</span></p>
</section>
<section id="softmax-function" class="slide level2">
<h2>Softmax function</h2>
<p>Here we can use the softmax function!</p>
<p><span class="math display">\[
\text{softmax}(\mathbf{x})_c = \frac{e^{x_c}}{\sum_{j=1}^Ce^{x_j}}
\]</span></p>
</section>
<section id="a-probabilistic-model-for-multi-class-classification-1" class="slide level2">
<h2>A probabilistic model for multi-class classification</h2>
<p><span class="math display">\[
y_i\sim \mathbf{Categorical}\big(\text{softmax}(\mathbf{x}^T\mathbf{W})\big)
\]</span></p>
<p><span class="math display">\[
p(y_i=c) = \text{softmax}(\mathbf{x}^T\mathbf{W})_c=\frac{e^{\mathbf{x}^T\mathbf{w}_c}}{\sum_{j=1}^Ce^{\mathbf{x}^T\mathbf{w}_j}}
\]</span></p>
</section>
<section id="maximum-likelihood-estimation-3" class="slide level2">
<h2>Maximum likelihood estimation</h2>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{W}) =\textbf{NLL}(\mathbf{W}, \mathbf{X}, \mathbf{y})=- \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{W})
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^N \log\ \text{softmax}(\mathbf{x}_i^T\mathbf{W})_{y_i} = \sum_{i=1}^N  \log \frac{e^{\mathbf{x}_i^T\mathbf{w}_{y_i}}}{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}}
\]</span></p>
<p><span class="math display">\[
=\sum_{i=1}^N \bigg(\mathbf{x}_i^T\mathbf{w}_{y_i}- \log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)
\]</span></p>
</section>
<section id="maximum-likelihood-estimation-4" class="slide level2">
<h2>Maximum likelihood estimation</h2>
<p>In this case our parameters are a matrix</p>
<p><span class="math display">\[
\nabla_{\mathbf{W}} \mathbf{NLL}(\mathbf{W}, \mathbf{X}, \mathbf{y})= \begin{bmatrix} \frac{\partial \mathbf{NLL}}{\partial W_{11}} &amp; \frac{\partial \mathbf{NLL}}{\partial W_{12}} &amp; \dots &amp; \frac{\partial \mathbf{NLL}}{\partial W_{1C}} \\
\frac{\partial \mathbf{NLL}}{\partial W_{21}} &amp; \frac{\partial \mathbf{NLL}}{\partial W_{22}} &amp; \dots &amp; \frac{\partial \mathbf{NLL}}{\partial W_{2C}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\frac{\partial \mathbf{NLL}}{\partial W_{d1}} &amp; \frac{\partial \mathbf{NLL}}{\partial W_{d2}} &amp; \dots &amp; \frac{\partial \mathbf{NLL}}{\partial W_{dC}}
\end{bmatrix}
\]</span></p>
<p>We can still perform gradient descent as before.</p>
<p><span class="math display">\[
\mathbf{W}^{(i+1)} \leftarrow \mathbf{W}^{(i)} - \alpha \nabla_{\mathbf{W}} \mathbf{NLL}(\mathbf{W}^{(i)}, \mathbf{X}, \mathbf{y})
\]</span></p>
</section></section>
<section>
<section id="evaluating-models" class="title-slide slide level1 center">
<h1>Evaluating models</h1>

</section>
<section id="training-and-test-datasets" class="slide level2">
<h2>Training and test datasets</h2>
<p>In machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for <em>new</em> data. One way to estimate how well our model our model will generalize to new data is to <em>hold out</em> data while fitting our model. To do this we will split our dataset into two smaller datasets: a <em>training dataset</em> that we will use to fit our model, and a <em>test</em> or <em>held-out</em> dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.</p>
<p><span class="math display">\[\mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_N, y_N) \}\quad \longrightarrow \quad
\]</span></p>
<p><span class="math display">\[
\mathcal{D}_{train} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_{Ntrain}, y_{Ntrain}) \},\  
\mathcal{D}_{test} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_{Ntest}, y_{Ntest}) \}
\]</span></p>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<p><img data-src="slides_files/figure-revealjs/cell-22-output-1.png"></p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img data-src="slides_files/figure-revealjs/cell-23-output-1.png"></p>
</div>
</div>
<div class="cell" data-execution_count="23">
<div class="cell-output cell-output-display">
<p><img data-src="slides_files/figure-revealjs/cell-24-output-1.png"></p>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<p><img data-src="slides_files/figure-revealjs/cell-25-output-1.png"></p>
</div>
</div>

<div class="footer footer-default">

</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>