<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 3: Logistic regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#vector-calculus-review" id="toc-vector-calculus-review" class="nav-link active" data-scroll-target="#vector-calculus-review">Vector Calculus Review</a>
  <ul class="collapse">
  <li><a href="#partial-derivatives" id="toc-partial-derivatives" class="nav-link" data-scroll-target="#partial-derivatives">Partial derivatives</a></li>
  <li><a href="#partial-derivative-functions" id="toc-partial-derivative-functions" class="nav-link" data-scroll-target="#partial-derivative-functions">Partial derivative functions</a></li>
  <li><a href="#functions-of-vectors" id="toc-functions-of-vectors" class="nav-link" data-scroll-target="#functions-of-vectors">Functions of vectors</a></li>
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients">Gradients</a></li>
  <li><a href="#gradient-functions" id="toc-gradient-functions" class="nav-link" data-scroll-target="#gradient-functions">Gradient functions</a></li>
  <li><a href="#vector-notation" id="toc-vector-notation" class="nav-link" data-scroll-target="#vector-notation">Vector notation</a></li>
  <li><a href="#vector-notation-1" id="toc-vector-notation-1" class="nav-link" data-scroll-target="#vector-notation-1">Vector notation</a></li>
  <li><a href="#vector-notation-revisited" id="toc-vector-notation-revisited" class="nav-link" data-scroll-target="#vector-notation-revisited">Vector notation revisited</a></li>
  <li><a href="#dot-products-as-summations" id="toc-dot-products-as-summations" class="nav-link" data-scroll-target="#dot-products-as-summations">Dot products as summations</a></li>
  <li><a href="#derivatives-of-summations" id="toc-derivatives-of-summations" class="nav-link" data-scroll-target="#derivatives-of-summations">Derivatives of summations</a></li>
  <li><a href="#nested-summations" id="toc-nested-summations" class="nav-link" data-scroll-target="#nested-summations">Nested summations</a></li>
  <li><a href="#derivatives-with-matrix-vector-products" id="toc-derivatives-with-matrix-vector-products" class="nav-link" data-scroll-target="#derivatives-with-matrix-vector-products">Derivatives with matrix-vector products</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#binary-outputs" id="toc-binary-outputs" class="nav-link" data-scroll-target="#binary-outputs">Binary outputs</a></li>
  <li><a href="#visualizing-categorical-functions" id="toc-visualizing-categorical-functions" class="nav-link" data-scroll-target="#visualizing-categorical-functions">Visualizing categorical functions</a></li>
  <li><a href="#making-binary-predictions" id="toc-making-binary-predictions" class="nav-link" data-scroll-target="#making-binary-predictions">Making binary predictions</a></li>
  <li><a href="#descision-boundaries" id="toc-descision-boundaries" class="nav-link" data-scroll-target="#descision-boundaries">Descision boundaries</a></li>
  <li><a href="#measuring-error" id="toc-measuring-error" class="nav-link" data-scroll-target="#measuring-error">Measuring error</a></li>
  <li><a href="#defining-a-loss-function" id="toc-defining-a-loss-function" class="nav-link" data-scroll-target="#defining-a-loss-function">Defining a loss function</a></li>
  </ul></li>
  <li><a href="#logistic-regression-1" id="toc-logistic-regression-1" class="nav-link" data-scroll-target="#logistic-regression-1">Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#the-bernoulli-distribution" id="toc-the-bernoulli-distribution" class="nav-link" data-scroll-target="#the-bernoulli-distribution">The Bernoulli distribution</a></li>
  <li><a href="#a-probabilistic-model-for-binary-classification" id="toc-a-probabilistic-model-for-binary-classification" class="nav-link" data-scroll-target="#a-probabilistic-model-for-binary-classification">A probabilistic model for binary classification</a></li>
  <li><a href="#sigmoid-function" id="toc-sigmoid-function" class="nav-link" data-scroll-target="#sigmoid-function">Sigmoid function</a></li>
  <li><a href="#a-probabilistic-model-for-binary-classification-1" id="toc-a-probabilistic-model-for-binary-classification-1" class="nav-link" data-scroll-target="#a-probabilistic-model-for-binary-classification-1">A probabilistic model for binary classification</a></li>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
  <li><a href="#comparing-loss-functions" id="toc-comparing-loss-functions" class="nav-link" data-scroll-target="#comparing-loss-functions">Comparing loss functions</a></li>
  </ul></li>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link" data-scroll-target="#multi-class-classification">Multi-class classification</a>
  <ul class="collapse">
  <li><a href="#multi-class-prediction-functions" id="toc-multi-class-prediction-functions" class="nav-link" data-scroll-target="#multi-class-prediction-functions">Multi-class prediction functions</a></li>
  <li><a href="#categorical-distribution" id="toc-categorical-distribution" class="nav-link" data-scroll-target="#categorical-distribution">Categorical distribution</a></li>
  <li><a href="#a-probabilistic-model-for-multi-class-classification" id="toc-a-probabilistic-model-for-multi-class-classification" class="nav-link" data-scroll-target="#a-probabilistic-model-for-multi-class-classification">A probabilistic model for multi-class classification</a></li>
  <li><a href="#softmax-function" id="toc-softmax-function" class="nav-link" data-scroll-target="#softmax-function">Softmax function</a></li>
  <li><a href="#a-probabilistic-model-for-multi-class-classification-1" id="toc-a-probabilistic-model-for-multi-class-classification-1" class="nav-link" data-scroll-target="#a-probabilistic-model-for-multi-class-classification-1">A probabilistic model for multi-class classification</a></li>
  <li><a href="#maximum-likelihood-estimation-1" id="toc-maximum-likelihood-estimation-1" class="nav-link" data-scroll-target="#maximum-likelihood-estimation-1">Maximum likelihood estimation</a></li>
  </ul></li>
  <li><a href="#evaluating-models" id="toc-evaluating-models" class="nav-link" data-scroll-target="#evaluating-models">Evaluating models</a>
  <ul class="collapse">
  <li><a href="#training-and-test-datasets" id="toc-training-and-test-datasets" class="nav-link" data-scroll-target="#training-and-test-datasets">Training and test datasets</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 3: Logistic regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="vector-calculus-review" class="level1">
<h1>Vector Calculus Review</h1>
<section id="partial-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="partial-derivatives">Partial derivatives</h2>
<p>A function does not need to be restricted to having a single input. We can specify a function with multiple inputs as follows:</p>
<p><span class="math display">\[
f(x, y, z) = x^2 + 3xy - \log(z)
\]</span></p>
<p>In code this would look like;</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y, z):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> y <span class="op">+</span> np.log(z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A <strong>partial derivative</strong> is the derivative of a multiple-input function with respect to a single input, <em>assuming all other inputs are constant</em>. We will explore the implications of that condition later on in this course. For now, we will simply view partial derivatives as a straightforward extension of derivatives, using the modified notation <span class="math inline">\(\frac{\partial}{\partial x}\)</span>.</p>
<p>More formally, we can define the partial derivative with respect to each input of a function as:</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = \underset{\epsilon\rightarrow0}{\lim} \frac{f(x+\epsilon, y, z) - f(x,y,z)}{\epsilon}, \quad \frac{\partial f}{\partial y} = \underset{\epsilon\rightarrow0}{\lim} \frac{f(x, y+\epsilon, z) - f(x,y,z)}{\epsilon}
\]</span></p>
<p>These partial derivatives tell us how the output of the function changes as we change each of the inputs individually.</p>
</section>
<section id="partial-derivative-functions" class="level2">
<h2 class="anchored" data-anchor-id="partial-derivative-functions">Partial derivative functions</h2>
<p>We can also specify partial derivative functions in the same way as derivative functions. We’ll use subscript notation to specify which input we are differentiating with respect to.</p>
<p><span class="math display">\[
\frac{\partial f}{\partial x} = f_x'(x, y, z)
\]</span></p>
<p>We can derive partial derivative functions using the same set of derivative rules:</p>
<p><span class="math display">\[
f(x, y, z) = x^2 + 3xy - \log(z)
\]</span></p>
<p><span class="math display">\[
f_x'(x, y, z) = 2x + 3y
\]</span></p>
<p><span class="math display">\[
f_y'(x, y, z) = 3x
\]</span></p>
<p><span class="math display">\[
f_z'(x, y, z) = -\frac{1}{z}
\]</span></p>
</section>
<section id="functions-of-vectors" class="level2">
<h2 class="anchored" data-anchor-id="functions-of-vectors">Functions of vectors</h2>
<p>We can also define functions that take vectors (or matrices) as inputs.</p>
<p><span class="math display">\[
y = f(\mathbf{x}) \quad f: \mathbb{R}^n \rightarrow \mathbb{R}
\]</span></p>
<p>Here <span class="math inline">\(f\)</span> is a mapping from length <span class="math inline">\(n\)</span> vectors to real numbers. As a concrete example we could define the function:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^n x_i^3 + 1
\]</span></p>
<p>Here’s the same function in numpy:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">3</span>) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>f(np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="271">
<pre><code>37</code></pre>
</div>
</div>
<p>Note that functions of vectors are equivalent to multiple-input functions, but with a more compact notation!</p>
</section>
<section id="gradients" class="level2">
<h2 class="anchored" data-anchor-id="gradients">Gradients</h2>
<p>The <strong>gradient</strong> of a vector-input function is a vector such that each element is the partial derivative of the function with respect to the corresponding element of the input vector. We’ll use the same notation as derivatives for gradients.</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \frac{\partial f}{\partial x_3} \\ \vdots \end{bmatrix}
\]</span></p>
<p>The gradient is a vector that <em>tangent</em> to the function <span class="math inline">\(f\)</span> at the input <span class="math inline">\(\mathbf{x}\)</span>. Just as with derivatives, this means that the gradient defines a <em>linear</em> <em>approximation</em> to the function at the point <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p><span class="math display">\[
f(\mathbf{x}+\mathbf{\epsilon}) \approx f(\mathbf{x}) + \frac{df}{d\mathbf{x}} \cdot \mathbf{\epsilon}
\]</span></p>
<p>Where <span class="math inline">\(\mathbf{\epsilon}\)</span> is now a small vector. Intuitively, this means that if we take a small step <em>in any direction</em> as defined by <span class="math inline">\(\mathbf{\epsilon}\)</span>, the gradient will approximate the change in the output of the function. Becuase we are now in more than 1 dimension, this approximation defines a <em>plane</em> in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>Another extremely important property of the gradient is that it points in the direction of maximum change in the function. Meaning that if we were to take an infinitesimal step <span class="math inline">\(\mathbf{\epsilon}\)</span> from <span class="math inline">\(\mathbf{x}\)</span> in any direction, stepping in the gradient direction would give use the maximum value of <span class="math inline">\(f(\mathbf{x} +\mathbf{\epsilon})\)</span>. We can see this from the approximation above: <span class="math inline">\(f(\mathbf{x} +\mathbf{\epsilon})\)</span> is maximized when <span class="math inline">\(\frac{df}{d\mathbf{x}}\)</span> and <span class="math inline">\(\mathbf{\epsilon}\)</span> are colinear.</p>
<p>We can define the gradient in this sense this more formally as:</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}}= \underset{\gamma \rightarrow 0}{\lim}\ \underset{\|\mathbf{\epsilon}\|_2 &lt; \gamma}{\max} \frac{f(\mathbf{x} + \mathbf{\epsilon}) - f(\mathbf{x})}{\|\mathbf{\epsilon}\|_2}
\]</span></p>
</section>
<section id="gradient-functions" class="level2">
<h2 class="anchored" data-anchor-id="gradient-functions">Gradient functions</h2>
<p>Just as with derivatives and partial derivatives, we can define a <strong>gradient function</strong> that maps an input vector <span class="math inline">\(\mathbf{x}\)</span> to the gradient of the function <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{x}\)</span> as:</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}}=\nabla f(\mathbf{x})
\]</span></p>
<p>Here <span class="math inline">\(\nabla f\)</span> is the gradient function for <span class="math inline">\(f\)</span>. If the function takes multiple vectors as input, we can specify the gradient function with respect to a particular input using subscript notation:</p>
<p><span class="math display">\[
\frac{df}{d\mathbf{x}}= \nabla_{\mathbf{x}} f(\mathbf{x}, \mathbf{y}), \quad \frac{df}{d\mathbf{y}}= \nabla_{\mathbf{y}} f(\mathbf{x}, \mathbf{y})
\]</span></p>
<p>Note that the gradient function is a mapping from <span class="math inline">\(\mathbb{R}^n\rightarrow\mathbb{R}^n\)</span>, meaning that it returns a vector with the same size as the input.</p>
</section>
<section id="vector-notation" class="level2">
<h2 class="anchored" data-anchor-id="vector-notation">Vector notation</h2>
<p>As we’ve seen a vector is a 1-dimensional set of numbers. For example, we can write the vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^3\)</span> as:</p>
<p><span class="math display">\[\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\]</span></p>
<p>Recall that a vector can also be seen as either an <span class="math inline">\(n \times 1\)</span> matrix (column vector) or a <span class="math inline">\(1 \times n\)</span> matrix (row vector).</p>
<p><span class="math display">\[\text{Column vector: } \mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}, \quad \text{Row vector: } \mathbf{x} = \begin{bmatrix}
x_1 &amp; x_2 &amp; x_3
\end{bmatrix}\]</span></p>
<p>Note that we may use the same notation for both as they refer to the same concept (a vector).</p>
</section>
<section id="vector-notation-1" class="level2">
<h2 class="anchored" data-anchor-id="vector-notation-1">Vector notation</h2>
<p>The difference between row and column vectors becomes relevant when we consider matrix-vector multiplication. We can write matrix-vector multiplication in two ways: <span class="math display">\[\text{Matrix-vector: }\mathbf{A}\mathbf{x} = \mathbf{b}, \quad \text{Vector-matrix: }\mathbf{x}\mathbf{A}^T= \mathbf{b}\]</span> In <em>matrix-vector multiplication</em> we treat <span class="math inline">\(\textbf{x}\)</span> as a column vector (<span class="math inline">\(n \times 1\)</span> matrix), while in <em>vector-matrix multiplication</em> we treat it as a row vector (<span class="math inline">\(n \times 1\)</span> matrix). Transposing <span class="math inline">\(A\)</span> for left multiplication ensures that the two forms give the same answer.</p>
<p>In Numpy the <code>np.dot</code> function works in this way. Given a matrix <code>A</code> and a 1-dimensional vector <code>x</code>, performing both operations will give the same result (another 1-dimensional vector):</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[ <span class="dv">1</span>,  <span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>              [ <span class="dv">5</span>, <span class="op">-</span><span class="dv">3</span>,  <span class="dv">2</span>],</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>              [<span class="op">-</span><span class="dv">2</span>,  <span class="dv">1</span>, <span class="op">-</span><span class="dv">4</span>],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>             ])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>Ax   <span class="op">=</span> np.dot(A, x)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>xA_T <span class="op">=</span> np.dot(x, A.T)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Ax   = '</span>, Ax)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'xA^T = '</span>, xA_T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ax   =  [-4 13 -8]
xA^T =  [-4 13 -8]</code></pre>
</div>
</div>
</section>
<section id="vector-notation-revisited" class="level2">
<h2 class="anchored" data-anchor-id="vector-notation-revisited">Vector notation revisited</h2>
<p>It often is much simpler to explicitly define vectors as being either row or column vectors. The common convention in machine learning is to assume that all vectors are column vectors (<span class="math inline">\(n \times 1\)</span> matricies) and thus a row vector ( <span class="math inline">\(1\times n\)</span> matrix) is obtained by explicit transposition:</p>
<p><span class="math display">\[\text{Column vector: } \mathbf{x} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}, \quad \text{Row vector: } \mathbf{x}^T = \begin{bmatrix}
x_1 &amp; x_2 &amp; x_3
\end{bmatrix}\]</span></p>
<p>In this case, we would rewrite the matrix-vector and vector-matrix products we saw above as:</p>
<p><span class="math display">\[\text{Matrix-vector: }\mathbf{A}\mathbf{x} = \mathbf{b}, \quad \text{Vector-matrix: }\mathbf{x}^T\mathbf{A}^T= \mathbf{b}^T\]</span></p>
<p>In Numpy, we can make a vector into an explicit column or row vector by inserting a new dimension, either with the <code>np.expand_dims</code> function or with the indexing operator:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>row_x <span class="op">=</span> np.expand_dims(x, axis<span class="op">=</span><span class="dv">0</span>) <span class="co"># Add a new leading dimension to x</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>row_x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="273">
<pre><code>array([[ 1, -2,  1]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>column_x <span class="op">=</span> np.expand_dims(x, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># Add a new second dimension to x</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">all</span>(column_x.T <span class="op">==</span> row_x)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>column_x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="274">
<pre><code>array([[ 1],
       [-2],
       [ 1]])</code></pre>
</div>
</div>
<p>Alternatively:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>row_x <span class="op">=</span> x[<span class="va">None</span>, :]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>column_x <span class="op">=</span> x[:, <span class="va">None</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dot-products-as-summations" class="level2">
<h2 class="anchored" data-anchor-id="dot-products-as-summations">Dot products as summations</h2>
</section>
<section id="derivatives-of-summations" class="level2">
<h2 class="anchored" data-anchor-id="derivatives-of-summations">Derivatives of summations</h2>
</section>
<section id="nested-summations" class="level2">
<h2 class="anchored" data-anchor-id="nested-summations">Nested summations</h2>
</section>
<section id="derivatives-with-matrix-vector-products" class="level2">
<h2 class="anchored" data-anchor-id="derivatives-with-matrix-vector-products">Derivatives with matrix-vector products</h2>
</section>
</section>
<section id="logistic-regression" class="level1">
<h1>Logistic Regression</h1>
<section id="binary-outputs" class="level2">
<h2 class="anchored" data-anchor-id="binary-outputs">Binary outputs</h2>
<p>In the last lecture we considered approximating functions of the form:</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in\mathbb{R}
\]</span></p>
<p>In that setup our function takes in a vector and produces a real number as an output (for example a miles per gallon rating).</p>
<p>In many real-world problems, the output we want to model is not a continuous value, but a <em>categorical</em> value, meaning the function produces one a fixed set of known possible outputs. In the simplest <em>binary</em> case our function produces one of two possible outputs.</p>
<p>For example: consider the problem of labeling images as containing either cats or dogs. Conceptually we would like a function that maps images to either a cat label or a dog label:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pictures/catdog.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>For convenience and generality, we will typically use the set <span class="math inline">\(\{0, 1\}\)</span> to denote the possible outputs for a binary categorical function. Therefore in general we are considering functions of the form:</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in \{0, 1\}
\]</span></p>
<p>We can assign these outputs to correspond to our actual target labels. For instance we might say that <span class="math inline">\(0 = \textbf{"cat"}\)</span> and <span class="math inline">\(1=\textbf{"dog"}\)</span>.</p>
<p>We call prediction of a categorical output <strong>classification</strong>.</p>
</section>
<section id="visualizing-categorical-functions" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-categorical-functions">Visualizing categorical functions</h2>
<p>As a simpler example, let’s again consider the fuel efficiency example from the previous lecture. Perhaps our company has set a target fuel efficiency of 30 miles per gallon for our new model and we want to predict whether our design will meet that target. In this case our inputs will be the same as before, but our output will become a binary label:</p>
<p><span class="math display">\[
\text{Input: } \mathbf{x}_i= \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph}  \end{bmatrix}, \quad \text{Output: } y_i = \begin{cases} 1: \text{Meets target } (MPG \geq 30) \\ 0:
\text{Fails to meet target } (MPG &lt; 30) \\  \end{cases}
\]</span></p>
<p>We can visualize which observations meet our target efficiency by again plotting weight against MPG and using colors to distinguish observations would have label <span class="math inline">\(1\)</span> vs.&nbsp;label <span class="math inline">\(0\)</span>.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>With this new output definition our dataset will look like:</p>
<p><span class="math display">\[
\text{Honda Accord: } \begin{bmatrix} \text{Weight:} &amp; \text{2500 lbs} \\ \text{Horsepower:} &amp; \text{ 123 HP} \\ \text{Displacement:} &amp; \text{ 2.4 L} \\ \text{0-60mph:} &amp; \text{ 7.8 Sec} \end{bmatrix} \longrightarrow \text{1   (Meets target)}
\]</span></p>
<p><span class="math display">\[
\text{Dodge Aspen: } \begin{bmatrix} \text{Weight:} &amp; \text{3800 lbs} \\ \text{Horsepower:} &amp; \text{ 155 HP} \\ \text{Displacement:} &amp; \text{ 3.2 L} \\ \text{0-60mph:} &amp; \text{ 6.8 Sec} \end{bmatrix} \longrightarrow  \text{0   (Does not meet target)}
\]</span></p>
<p><span class="math display">\[
\vdots \quad \vdots
\]</span></p>
<p>In this case, we’ve gotten rid of the <span class="math inline">\(MPG\)</span> output variable and replaced it with a binary output <span class="math inline">\(y_i \in \{0, 1\}\)</span>. If we plot this version of the data, we can see more directly how this <em>classification</em> task differs from the <em>regression</em> task we saw in the last lecture.</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="making-binary-predictions" class="level2">
<h2 class="anchored" data-anchor-id="making-binary-predictions">Making binary predictions</h2>
<p>We could fit a linear regression model to our binary data, by simply treating the labels <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> as real-valued outputs. For our fuel economy example, such a model would look like this:</p>
<div class="cell" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>However, this doesn’t really address our problem. How do we interpret a prediction of <span class="math inline">\(-1\)</span> or <span class="math inline">\(10\)</span> or <span class="math inline">\(0.5\)</span>? A more suitable prediction function would <em>only</em> output one of our two possible labels <span class="math inline">\(\{0, 1\}\)</span>. Fortunately, we can simply adapt our linear regression function in this way by defining a <em>cutoff</em> (typically 0), as follows:</p>
<p><span class="math display">\[
f(\mathbf{x})=\mathbf{x}^T\mathbf{w} \quad \longrightarrow \quad f(\mathbf{x})=\begin{cases} 1\ \text{   if   }\ \mathbf{x}^T\mathbf{w} \geq 0 \\
0\ \text{   if   }\ \mathbf{x}^T\mathbf{w} &lt; 0\end{cases}
\]</span></p>
<p>We might also write this as:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \mathbb{I}(\mathbf{x}^T\mathbf{w} \geq 0)
\]</span></p>
<p>Where <span class="math inline">\(\mathbb{I}\)</span> is an <em>indicator function</em> that is simply <span class="math inline">\(1\)</span> if the boolean expression is true and <span class="math inline">\(0\)</span> otherwise.</p>
<p>This gives us a prediction function that looks like step function in 1 dimension:</p>
<div class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>For our efficiency example, the binary prediction function can be written as:</p>
<p><span class="math display">\[
\text{Meets target} = f(\mathbf{x})=
\]</span></p>
<p><span class="math display">\[
\big((\text{weight})w_1 + (\text{horsepower})w_2 + (\text{displacement})w_3 + (\text{0-60mph})w_4 + b\big) \geq 0
\]</span></p>
<p>Or in matrix notation:</p>
<p><span class="math display">\[
f(\mathbf{x})= \left( \begin{bmatrix} \text{Weight} \\ \text{Horsepower} \\ \text{Displacement} \\ \text{0-60mph} \\ 1 \end{bmatrix} \cdot \begin{bmatrix} w_1 \\ w_2\\ w_3 \\ w_4\\ b\end{bmatrix} \geq  0\right)
\]</span></p>
<p>In this form we can see that the <em>sign</em> of each weight parameter determines whether the corresponding feature is more predictive of label <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span> and to what extent. For instance, large positive weights indicate features that are very predictive of <span class="math inline">\(1\)</span>.</p>
<p>This has a geometric interpretation if we think of <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> as vectors. If the angle between <span class="math inline">\(\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> is in the range <span class="math inline">\([-\frac{\pi}{2}, \frac{\pi}{2}]\)</span> (or <span class="math inline">\([-90^o, 90^o]\)</span> in degrees), then the prediction will be <span class="math inline">\(1\)</span>.</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="descision-boundaries" class="level2">
<h2 class="anchored" data-anchor-id="descision-boundaries">Descision boundaries</h2>
<p>We can visualize a classification dataset as a function of two variables using color to distinguish between observations with each label. In this example we’ll look at weight and engine displacement.</p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>For a binary classification model the <strong>decision boundary</strong> is the border between regions of the input space corresponding to each prediction. For a linear classification model the decision boundary is line or plane:</p>
<p><span class="math display">\[\mathbf{x}^T\mathbf{w}=0\]</span></p>
<p>We can similarly plot the predictions made by a linear model using colors.</p>
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-stdout">
<pre><code>Model accuracy: 0.8291</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-15-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="measuring-error" class="level2">
<h2 class="anchored" data-anchor-id="measuring-error">Measuring error</h2>
<p>A natural measure for error for binary classifiers is <strong>accuracy</strong>. The <em>accuracy</em> of a prediction function is the fraction of observations where the prediction matches the true output:</p>
<p><span class="math display">\[
\textbf{Accuracy} = \frac{1}{N} \sum_{i=1}^N \mathbb{I}\big(f(\mathbf{x}_i) = y_i\big)
\]</span></p>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="defining-a-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="defining-a-loss-function">Defining a loss function</h2>
<p>In the last lecture we saw that we can find an optimal choice of parameters <span class="math inline">\(\mathbf{w}\)</span> for a linear regression model by defining a measure of <em>error</em> or <em>loss</em> for our approximation on our dataset and minimizing that error as a function of <span class="math inline">\(\mathbf{w}\)</span>, either directly or with gradient descent.</p>
<p>We might consider using (negative) accuracy as a loss function or the same mean squared error that we used for linear regression. However, if we tried to minimize one of these losses with gradient descent, we would run into a fundamental problem: the derivative of the prediction function is <em>always</em> <span class="math inline">\(0\)</span>. We can see this if we</p>
</section>
</section>
<section id="logistic-regression-1" class="level1">
<h1>Logistic Regression</h1>
<section id="the-bernoulli-distribution" class="level2">
<h2 class="anchored" data-anchor-id="the-bernoulli-distribution">The Bernoulli distribution</h2>
<p><span class="math display">\[
p(y)=\begin{cases} q\quad\ \ \ \ \ \ \  \text{if }\ y=1\\
1-q\quad \text{if }\ y=0\\
\end{cases}\quad q\in[0,1],\ y\in\{0, 1\}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
p(y) = q^y(1-q)^{1-y}
\]</span></p>
<p><span class="math display">\[
\log p(y) = y\log q + (1-y)\log(1-q)
\]</span></p>
</section>
<section id="a-probabilistic-model-for-binary-classification" class="level2">
<h2 class="anchored" data-anchor-id="a-probabilistic-model-for-binary-classification">A probabilistic model for binary classification</h2>
<p><span class="math display">\[
y_i \sim \mathcal{N}(\mathbf{x}^T\mathbf{w},\ \sigma^2)
\]</span></p>
<p><span class="math display">\[
\mathbf{x}^T\mathbf{w}\notin [0, 1] \quad \longrightarrow \quad y_i \sim \mathbf{Bernoulli}(\mathbf{ q=? })\quad
\]</span></p>
<p><span class="math display">\[
\textbf{Need }\ f(x):\ \mathbb{R} \longrightarrow [0,1]
\]</span></p>
<p><span class="math display">\[
\textbf{Input: } x \in \mathbb{R} \longrightarrow \textbf{Output: } y \in [0,1]
\]</span></p>
</section>
<section id="sigmoid-function" class="level2">
<h2 class="anchored" data-anchor-id="sigmoid-function">Sigmoid function</h2>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><span class="math display">\[
\frac{d}{dx}\sigma(x) = \sigma(x)\big(1-\sigma(x)\big)
\]</span></p>
<p><span class="math display">\[
\sigma(0) = 0.5
\]</span></p>
<p><span class="math display">\[
1-\sigma(x) = \sigma(-x)
\]</span></p>
</section>
<section id="a-probabilistic-model-for-binary-classification-1" class="level2">
<h2 class="anchored" data-anchor-id="a-probabilistic-model-for-binary-classification-1">A probabilistic model for binary classification</h2>
<p><span class="math display">\[
y_i \sim \mathbf{Bernoulli}\big(\mathbf{ \sigma(\mathbf{x}_i^T\mathbf{w} })\big)
\]</span></p>
<p><span class="math display">\[
p(y_i = 1) = \sigma(\mathbf{x}_i^T\mathbf{w}), \quad p(y_i=0)=1-\sigma(\mathbf{x}_i^T\mathbf{w})=\sigma(-\mathbf{x}_i^T\mathbf{w})
\]</span></p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><span class="math display">\[
p(y_i=1)\geq 0.5 \quad \longrightarrow \quad \mathbf{x}^T\mathbf{w}\geq 0
\]</span></p>
</section>
<section id="maximum-likelihood-estimation" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum likelihood estimation</h2>
<p>(conditional probability) of all of the outputs in our dataset:</p>
<p><span class="math display">\[
\mathbf{w}^* = \underset{\mathbf{w}}{\text{argmax}} \ p(\mathbf{y} \mid \mathbf{X}, \mathbf{w}) =\underset{\mathbf{w}}{\text{argmax}} \ p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) \]</span></p>
<p>Generally our model also assumes <em>conditional independence</em> across observations so:</p>
<p><span class="math display">\[
p(y_1,...,y_N \mid \mathbf{x}_1, ...,\mathbf{x}_N, \mathbf{w}) = \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
<p>For convenience, it is typical to frame the optimal value in terms of the <em>negative log-likelihood</em> rather than the likelihood, but the two are equivalent.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{argmax}} \prod_{i=1}^N p(y_i\mid \mathbf{x}_i, \mathbf{w}) = \underset{\mathbf{w}}{\text{argmin}} - \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w}) = \textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})
\]</span></p>
<p>We see that the negative log-likelihood is a natural <em>loss function</em> to optimize to find <span class="math inline">\(\mathbf{w}^*\)</span>.</p>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{w}) =\textbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y})=- \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{w})
\]</span></p>
<p>We can w</p>
<p><span class="math display">\[
\mathbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) = -\sum_{i=1}^N y_i\log \sigma(\mathbf{x}^T\mathbf{w}) + (1-y_i)\log(1-\sigma(\mathbf{x}^T\mathbf{w}))
\]</span></p>
<p><span class="math display">\[
=-\sum_{i=1}^N y_i\log \sigma(\mathbf{x}^T\mathbf{w}) + (1-y_i)\log \sigma(-\mathbf{x}^T\mathbf{w})
\]</span></p>
<p><span class="math display">\[
\textbf{Ideally: }\ p(y_i\mid \mathbf{x}_i, \mathbf{w})=1,\ \forall (\mathbf{x}_i, y_i)\in \mathcal{D}
\]</span></p>
<p><span class="math display">\[
\mathbf{NLL}(\mathbf{w}, \mathbf{X}, \mathbf{y}) = -\mathbf{y}^T\log \sigma(\mathbf{X}\mathbf{w})
\]</span></p>
</section>
<section id="comparing-loss-functions" class="level2">
<h2 class="anchored" data-anchor-id="comparing-loss-functions">Comparing loss functions</h2>
<p>Loss for <span class="math inline">\(y=0\)</span> as a function of <span class="math inline">\(z=\mathbf{x}^T\mathbf{w}\)</span></p>
<p><span class="math display">\[
\textbf{Let: }\ z=\mathbf{x}^T\mathbf{w}
\]</span></p>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="multi-class-classification" class="level1">
<h1>Multi-class classification</h1>
<section id="multi-class-prediction-functions" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-prediction-functions">Multi-class prediction functions</h2>
<p>We will typically use a set of integers <span class="math inline">\(\{0,1, 2,...,C\}\)</span> to denote the possible outputs for a general categorical function. Therefore in general we are considering functions of the form:</p>
<p><span class="math display">\[
y=f(\mathbf{x}), \quad \text{Input: } \mathbf{x} \in\mathbb{R}^n \longrightarrow \text{ Output: }y \in \{0, 1, 2, ...,C\}
\]</span></p>
<p>It’s important to note that we are <em>not</em> assuming that the <em>ordering</em> of labels is meaningful For instance if we’re classifying images of animals we might set the labels such that:</p>
<p><span class="math display">\[
\textbf{0:  Cat},\quad
\textbf{1:  Dog},\quad
\textbf{2:  Mouse}
\]</span></p>
<p>But</p>
<p><span class="math display">\[
\textbf{0:  Dog},\quad
\textbf{1:  Mouse},\quad
\textbf{2:  Cat}
\]</span></p>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0...C\}}{\text{argmax}}\ \mathbf{x}^T\mathbf{w}_c
\]</span></p>
<p>Alternatively:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0...C\}}{\text{argmax}}\ (\mathbf{x}^T\mathbf{W})_c, \quad \mathbf{W} \in \mathbb{R}^{d\times C}
\]</span></p>
<p>if <span class="math inline">\(C=2\)</span>:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \underset{c\in\{0,1\}}{\text{argmax}}\ (\mathbf{x}^T\mathbf{W})_c = \mathbb{I}(\mathbf{x}^T\mathbf{w}_1 - \mathbf{x}^T\mathbf{w}_0 \geq 0)
\]</span></p>
<p><span class="math display">\[
=\mathbb{I}(\mathbf{x}^T(\mathbf{w}_1 - \mathbf{w}_0) \geq 0) \quad \longrightarrow \quad \mathbb{I}(\mathbf{x}^T\mathbf{w} \geq 0), \quad \mathbf{w}=\mathbf{w}_1-\mathbf{w}_0
\]</span></p>
<p>We call prediction of a categorical output <strong>classification</strong>.</p>
</section>
<section id="categorical-distribution" class="level2">
<h2 class="anchored" data-anchor-id="categorical-distribution">Categorical distribution</h2>
<p><span class="math display">\[
p(y=c) = q_c, \quad y\in \{1...C\}
\]</span></p>
<p><span class="math display">\[
\mathbf{q} \in \mathbb{R}^C\quad q_c \geq 0\ \forall c\in \{1...C\}\quad \sum_{c=1}^C q_c=1
\]</span></p>
<p><span class="math display">\[
p(y)=\prod q_c^{\mathbb{I}(y=c)}
\]</span></p>
<p><span class="math display">\[
\log p(y) = \sum_{c=1}^C \mathbb{I}(y=c)\log q_c = \log q_y
\]</span></p>
</section>
<section id="a-probabilistic-model-for-multi-class-classification" class="level2">
<h2 class="anchored" data-anchor-id="a-probabilistic-model-for-multi-class-classification">A probabilistic model for multi-class classification</h2>
<p><span class="math display">\[
y_i\sim \mathbf{Categorical}(\mathbf{q}=?)
\]</span></p>
<p><span class="math display">\[
\mathbf{q}=\mathbf{x}^T\mathbf{W}?
\]</span></p>
<p><span class="math display">\[
\mathbf{x}^T\mathbf{W}\in \mathbb{R}^C,\quad  q_c \ngeq 0\ \forall c\in \{1...C\}, \quad \sum_{c=1}^C q_c\neq1
\]</span></p>
<p><span class="math display">\[
\textbf{Need }\ f(\mathbf{x}):\ \mathbb{R}^C \longrightarrow [0,\infty)^C,\ \sum_{i=1}^Cf(\mathbf{x})_c = 1
\]</span></p>
</section>
<section id="softmax-function" class="level2">
<h2 class="anchored" data-anchor-id="softmax-function">Softmax function</h2>
<p><span class="math display">\[
\text{softmax}(\mathbf{x})_c = \frac{e^{x_c}}{\sum_{j=1}^Ce^{x_j}}
\]</span></p>
<p>if <span class="math inline">\(C=2\)</span></p>
</section>
<section id="a-probabilistic-model-for-multi-class-classification-1" class="level2">
<h2 class="anchored" data-anchor-id="a-probabilistic-model-for-multi-class-classification-1">A probabilistic model for multi-class classification</h2>
<p><span class="math display">\[
y_i\sim \mathbf{Categorical}\big(\text{softmax}(\mathbf{x}^T\mathbf{W})\big)
\]</span></p>
<p><span class="math display">\[
p(y_i=c) = \text{softmax}(\mathbf{x}^T\mathbf{W})_c=\frac{e^{\mathbf{x}^T\mathbf{w}_c}}{\sum_{j=1}^Ce^{\mathbf{x}^T\mathbf{w}_j}}
\]</span></p>
</section>
<section id="maximum-likelihood-estimation-1" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood-estimation-1">Maximum likelihood estimation</h2>
<p><span class="math display">\[
\textbf{Loss}(\mathbf{W}) =\textbf{NLL}(\mathbf{W}, \mathbf{X}, \mathbf{y})=- \sum_{i=1}^N \log p(y_i \mid \mathbf{x}_i, \mathbf{W})
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^N \log\ \text{softmax}(\mathbf{x}_i^T\mathbf{W})_{y_i} = \sum_{i=1}^N  \log \frac{e^{\mathbf{x}_i^T\mathbf{w}_{y_i}}}{\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}}
\]</span></p>
<p><span class="math display">\[
=\sum_{i=1}^N \bigg(\mathbf{x}_i^T\mathbf{w}_{y_i}- \log\sum_{j=1}^Ce^{\mathbf{x}_i^T\mathbf{w}_{j}}\bigg)
\]</span></p>
</section>
</section>
<section id="evaluating-models" class="level1">
<h1>Evaluating models</h1>
<section id="training-and-test-datasets" class="level2">
<h2 class="anchored" data-anchor-id="training-and-test-datasets">Training and test datasets</h2>
<p>In machine learning we are typically less interested in how our model predicts the data we’ve already seen than we are in how well it makes predictions for <em>new</em> data. One way to estimate how well our model our model will generalize to new data is to <em>hold out</em> data while fitting our model. To do this we will split our dataset into two smaller datasets: a <em>training dataset</em> that we will use to fit our model, and a <em>test</em> or <em>held-out</em> dataset that we will only use to evaluate our model. By computing the loss on this test dataset, we can get a sense of how well our model will make prediction for new data.</p>
<p><span class="math display">\[\mathcal{D} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_N, y_N) \}\quad \longrightarrow \quad
\]</span></p>
<p><span class="math display">\[
\mathcal{D}_{train} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_{Ntrain}, y_{Ntrain}) \},\  
\mathcal{D}_{test} = \{ (\mathbf{x}_1, y_1),\ (\mathbf{x}_2, y_2),\ ... \,(\mathbf{x}_{Ntest}, y_{Ntest}) \}
\]</span></p>
<div class="cell" data-execution_count="19">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img src="notes_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>