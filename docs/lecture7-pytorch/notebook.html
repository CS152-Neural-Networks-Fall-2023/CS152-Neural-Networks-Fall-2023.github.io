<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 7: PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-pytorch" id="toc-introduction-to-pytorch" class="nav-link active" data-scroll-target="#introduction-to-pytorch">Introduction to PyTorch</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn">torch.nn</a></li>
  <li><a href="#evaluating-models" id="toc-evaluating-models" class="nav-link" data-scroll-target="#evaluating-models">Evaluating models</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 7: PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_boundary(model, X, y):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">xrange</span> <span class="op">=</span> (<span class="op">-</span>X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">+</span> X[:, <span class="dv">0</span>].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    yrange <span class="op">=</span> (<span class="op">-</span>X[:, y].<span class="bu">min</span>() <span class="op">+</span> X[:, y].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    feature_1, feature_2 <span class="op">=</span> np.meshgrid(</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="bu">xrange</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="bu">xrange</span>, <span class="dv">250</span>),</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> yrange, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> yrange, <span class="dv">250</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> np.vstack([feature_1.ravel(), feature_2.ravel()]).T</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.reshape(model.predict(torch.tensor(grid).<span class="bu">float</span>()).detach().numpy(), feature_1.shape)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    display <span class="op">=</span> DecisionBoundaryDisplay(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        xx0<span class="op">=</span>feature_1, xx1<span class="op">=</span>feature_2, response<span class="op">=</span>y_pred</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    display.plot()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    display.ax_.scatter(</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, edgecolor<span class="op">=</span><span class="st">"black"</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction-to-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-pytorch">Introduction to PyTorch</h2>
<p>The most basic object in PyTorch is a <code>tensor</code>. Tensor objects behave much like the <code>AutogradValue</code> objects we are creating in the homework! We can create a <code>tensor</code> object with a given value as follows</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor(4.)</code></pre>
</div>
</div>
<p>Performing basic operations on <code>tensor</code> objects gives tensor objects.</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>tensor(21.)</code></pre>
</div>
</div>
<p><code>tensor</code> objects also support reverse-mode automatic differentiation! To use this, we must specify that we will want to compute the derivative with respect to a given <code>tensor</code>. We can do this with the <code>requires_grad</code> argument.</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have a <code>tensor</code> that <code>requires_grad</code>, we can perform operations on it to compute a loss.</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.log(a) <span class="co"># Functions like log must be called through torch</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor(3.0445, grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Once we have a loss running the backward pass is done exactly as in the homework. First we call <code>backward()</code> on the loss <code>tensor</code> object, then we can access the derivative through the <code>grad</code> property of <code>x</code>.</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>tensor(0.3810)</code></pre>
</div>
</div>
<p>We can also create <code>tensor</code> objects that wrap arrays.</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(np.array([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>We can also just directly create tensors as we would numpy arrays</p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>Including convienience constructors.</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.ones((<span class="dv">5</span>,)))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1., 1., 1., 1., 1.])
tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>Automatic differentiation still works for arrays. In this case it gives use the gradient of the loss (hence the <code>grad</code> property).</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">3.</span>, <span class="fl">4.</span>, <span class="fl">5.</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>tensor(50., grad_fn=&lt;SumBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>L.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/t9/fjsr8h1x7c7cg4vqw_xhjq2w0000gn/T/ipykernel_79454/323164392.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)
  L.grad</code></pre>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>tensor([ 6.,  8., 10.])</code></pre>
</div>
</div>
<p>We can convert <code>tensor</code> objects back to numpy by calling <code>x.detach().numpy()</code>. (<code>detach</code> removes the variable from any automatic differentiation computations)</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>x.detach().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>array([3., 4., 5.], dtype=float32)</code></pre>
</div>
</div>
<p>At this point it’s probably worth remarking on where the name <code>tensor</code> comes from.</p>
<p>So far we’ve discussed 3 kinds of array objects - <strong>Scalars:</strong> which are just single values (0-dimensional) - <strong>Vectors:</strong> 1-dimensional arrays of numbers - <strong>Matrices:</strong> 2-dimensional arrays of numbers</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <strong>tensor</strong> is the generalization of a vector or matrix to <em>any</em> number of dimensions. For example, a 3-dimensional tensor can be seen in multiple ways.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <code>tensor</code> object can be created with any number of dimensions. For example, we could create a 2x2x2 tensor as:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor([[[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]]])</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])</code></pre>
</div>
</div>
<p>Or we could create the tensor in the image using <code>arange</code> and <code>reshape</code>.</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.arange(<span class="dv">30</span>).reshape((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>tensor([[[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9]],

        [[10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]],

        [[20, 21, 22, 23, 24],
         [25, 26, 27, 28, 29]]])</code></pre>
</div>
</div>
<p>4-dimensional tensors can also be visualized</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>t.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>torch.Size([3, 2, 4, 5])</code></pre>
</div>
</div>
<p>There are some notable differences between torch and numpy when it comes to operations. The important one to watch out for at this point is matrix multiplation. In numpy we accomplished with with <code>np.dot</code>:</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>np.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>array([[5., 5.],
       [5., 5.],
       [5., 5.],
       [5., 5.]])</code></pre>
</div>
</div>
<p>In PyTorch <code>torch.dot</code> only does vector dot products and thus only applies to 1-dimensional <code>tensor</code> objects:</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>torch.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>RuntimeError: 1D tensors expected, but got 2D and 2D tensors</code></pre>
</div>
</div>
<p>Instead we use the <code>torch.matmul</code> function for this purpose</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>torch.matmul(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>tensor([[5., 5.],
        [5., 5.],
        [5., 5.],
        [5., 5.]])</code></pre>
</div>
</div>
<p>PyTorch also has many handy built-in functions that numpy doesn’t have, such as <code>sigmoid</code>.</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">50</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> torch.sigmoid(x)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This makes it very easy to implement something like logistic regression.</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.ones((dims,), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros((), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        f_X <span class="op">=</span> torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(f_X)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.predict_probability(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s try loading a dataset, converting it to <code>tensor</code> and making predictions</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>When working with PyTorch, it is convention to separate the loss function from the model, where the loss function will just take predictions and labels.</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NLL(pred, y):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    LL <span class="op">=</span> y <span class="op">*</span> torch.log(pred) <span class="op">+</span> (<span class="fl">1.</span> <span class="op">-</span> y) <span class="op">*</span> torch.log(<span class="fl">1.</span> <span class="op">-</span> pred)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>LL.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Gradient descent is also implemented in PyTorch in the <code>optim</code> module.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Gradient descent works a bit differently in PyTorch than what we’ve seen. We first need to construct a gradient descent <em>object</em> which specifies which values we’re optimizing and what the learning rate will be. We specify the values to optimize by simply passing a list of weights/parameters to the constructor.</p>
<p>In PyTorch, basic gradient descent is encapsulated in the <code>optim.SGD</code> class (<code>SGD</code> stands for <em>stochastic gradient descent</em>, we’ll talk about what <em>stochastic</em> means in this context next week.)</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([model.weights, model.bias], lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that this object doesn’t even take in the function we’re trying to optimize, only the inputs. We need to call the function ourselves <em>and</em> run <code>backward()</code> to compute the gradients.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s look at our model <code>weights</code></p>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>model.weights</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>model.weights.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>tensor([-5.6526, 24.1355])</code></pre>
</div>
</div>
<p>We can take a single step of gradient descent using the <code>step</code> method of the optimizer.</p>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>model.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>tensor([ 1.5653, -1.4135], requires_grad=True)</code></pre>
</div>
</div>
<p>We see that this actually updates the weights themselves!</p>
<p>It’s important to note that in PyTorch, calling <code>backward</code> does <strong>not</strong> clear the value stored in grad. So computing the gradient multiple times will result in updates to the gradient.</p>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-5.6526, 24.1355])
tensor([-14.5242,  28.7704])
tensor([-23.3958,  33.4053])</code></pre>
</div>
</div>
<p>We can clear the stored gradients using the optimizer.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>None</code></pre>
</div>
</div>
<p>So far we’ve only taking a single step of gradient descent. In order to run many steps, we need to write a loop to do everything we just saw.</p>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>45.934814453125
40.60100555419922
32.82542037963867
30.3773136138916
28.918834686279297
28.118915557861328
27.602989196777344
27.248106002807617
26.989551544189453
26.794662475585938</code></pre>
</div>
</div>
<p>We should now see that our model has been optimized!</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn">torch.nn</h2>
<p>While PyTorch as a tool for automatic differentiation and optimization would be useful by itself. It actually gives us a lot more than that!</p>
<p>On of the most important features of PyTorch is its model-building tools in the <code>torch.nn</code> module. This gives us a lot of powerful features that we can use to build complex neural networks!</p>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s start by building out logistic regression model in the <code>torch.nn</code> framwork. In order for a model to benefit from <code>torch.nn</code> our model class needs to inheret from <code>nn.Module</code></p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.ones((dims,)))</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(()))</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias)</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are 2 changes to note here. The first is that we wrapped our <code>weights</code> and <code>bias</code> terms in <code>nn.Parameter</code>. This tells PyTorch that these are the parameters we will want to optimize. We don’t need to specify <code>requires_grad</code> for parameters, PyTorch will take care of that for us.</p>
<p>The second is that we moved the implmentation of <code>predict_probability</code> to <code>forward</code>. In PyTorch models the <code>forward</code> method is special, it defines the model as a function. If we call the model as a function <code>forward</code> will be called internally.</p>
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="78">
<pre><code>tensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,
        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,
        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,
        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,
        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,
        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,
        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,
        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,
        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,
        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,
        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,
        0.5392], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>This means that we can use instances of <code>nn.Module</code> as parameterized functions. For example, we might create a general linear (technically affine) function in the same way.</p>
<p><span class="math display">\[f(\mathbf{x}) = \mathbf{x}^T\mathbf{W}^T + \mathbf{b},  \quad f: \mathbb{R}^i \rightarrow \mathbb{R}^o\]</span></p>
<p>Note that here we are <strong>not</strong> assuming an augmented representation of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear(nn.Module):</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weightsT <span class="op">=</span> nn.Parameter(torch.ones((inputs, outputs)))</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros((outputs,)))</span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(X, <span class="va">self</span>.weightsT) <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can use this module to implement out logistic regression model above.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(dims, <span class="dv">1</span>)                       <span class="co"># Dims input 1 output</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X)).reshape((<span class="op">-</span><span class="dv">1</span>,)) <span class="co"># Turn output into a vector</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>tensor([0.5652, 0.7431, 0.8238, 0.5468, 0.8991, 0.5719, 0.8651, 0.7784, 0.6153,
        0.7053, 0.7419, 0.3129, 0.4502, 0.5031, 0.4508, 0.7075, 0.7626, 0.4756,
        0.4158, 0.4693, 0.7904, 0.7241, 0.2430, 0.5297, 0.7944, 0.8496, 0.6825,
        0.6149, 0.6730, 0.5231, 0.6151, 0.6108, 0.5509, 0.7476, 0.6634, 0.8512,
        0.8117, 0.7527, 0.5092, 0.7742, 0.8012, 0.7604, 0.6411, 0.3242, 0.2805,
        0.4016, 0.6296, 0.3000, 0.7045, 0.7307, 0.7442, 0.8091, 0.5104, 0.8006,
        0.6166, 0.5173, 0.6404, 0.5779, 0.8199, 0.9030, 0.7872, 0.6059, 0.8091,
        0.9367, 0.5806, 0.7576, 0.8079, 0.3847, 0.4926, 0.4909, 0.8605, 0.6233,
        0.5605, 0.5980, 0.3629, 0.6874, 0.8761, 0.7593, 0.8267, 0.7912, 0.7147,
        0.4980, 0.4396, 0.9295, 0.8956, 0.5096, 0.7148, 0.7788, 0.7961, 0.2600,
        0.4964, 0.6008, 0.5710, 0.7882, 0.9379, 0.8573, 0.7131, 0.8105, 0.6835,
        0.5392], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>The power here is that because <code>Linear</code> is also an instance of <code>nn.Module</code>, PyTorch knows that it’s weights should also be considered part of our models weights. We can access the weights of a model using the <code>parameters()</code> method.</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>[Parameter containing:
 tensor([[1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This lets us easily apply gradient descent:</p>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>76.53839111328125
45.934814453125
40.60100555419922
32.82542037963867
30.3773136138916
28.918834686279297
28.118915557861328
27.602989196777344
27.248106002807617
26.989551544189453</code></pre>
</div>
</div>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-42-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch unsurprisingly also provides a built-in <code>Linear</code> module. As <code>nn.Linear</code>.</p>
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="85">
<pre><code>Linear(in_features=2, out_features=1, bias=True)</code></pre>
</div>
</div>
<p>Knowing how to make a parameterized function in PyTorch, let’s consider making a neural network layer with a sigmoid activation function.</p>
<p><span class="math display">\[f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}^T + \mathbf{b})\]</span></p>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(inputs, outputs)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X))</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s create a layer with 10 neurons. (So <span class="math inline">\(\mathbf{W}:\ (10 \times 2)\)</span>)</p>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> SigmoidLayer(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>layer(X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([100, 2])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>torch.Size([100, 10])</code></pre>
</div>
</div>
<p>Let’s use this to create a neural network class for binary classification!</p>
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims, hidden_size):</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer <span class="op">=</span> SigmoidLayer(dims, hidden_size)</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(hidden_size, <span class="dv">1</span>)                       </span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        hidden_neurons <span class="op">=</span> <span class="va">self</span>.layer(X)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear(hidden_neurons)</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(output).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that PyTorch recognizes both the parameters of the logistic regression and the parameters of our neural network feature transform:</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>[Parameter containing:
 tensor([[-0.0632, -0.0324],
         [-0.4227, -0.0076],
         [ 0.5793, -0.4920],
         [ 0.1640, -0.5442],
         [ 0.2326,  0.3651],
         [-0.0184,  0.6859],
         [ 0.0893,  0.1236],
         [-0.4774, -0.3702],
         [-0.0048,  0.4830],
         [-0.4720,  0.5458]], requires_grad=True),
 Parameter containing:
 tensor([-0.2695, -0.1038, -0.7001,  0.3398, -0.0591,  0.6680,  0.3601, -0.3093,
          0.0831, -0.4315], requires_grad=True),
 Parameter containing:
 tensor([[1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This means that we can easily run our optimization as before.</p>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>251.74571228027344
461.0118103027344
62.689453125
58.230892181396484
51.833839416503906
45.31672668457031
42.269630432128906
41.090309143066406
42.59092712402344
44.23348617553711
51.76810073852539
45.51247024536133
48.5499267578125
37.86709976196289
36.245426177978516
32.07354736328125
30.741405487060547
29.285472869873047
28.51435089111328
27.839073181152344
27.37657928466797
26.992008209228516
26.69622802734375
26.455289840698242
26.262691497802734
26.106895446777344
25.978981018066406
25.872283935546875
25.78037452697754
25.699132919311523
25.625377655029297
25.55675506591797
25.492244720458984
25.430410385131836
25.371490478515625
25.31437873840332
25.260303497314453
25.208097457885742
25.160297393798828
25.115331649780273
25.07819175720215
25.046260833740234
25.029979705810547
25.02396011352539
25.0517578125
25.099485397338867
25.22385025024414
25.381305694580078
25.71344757080078
26.064964294433594
26.785503387451172
27.31593894958496
28.478464126586914
28.659645080566406
29.6827335357666
28.770679473876953
28.902992248535156
27.449777603149414
27.00601577758789
25.92099380493164
25.47977066040039
24.875839233398438
24.58094596862793
24.25676918029785
24.065155029296875
23.871578216552734
23.734432220458984
23.599082946777344
23.48827362060547
23.378538131713867
23.279327392578125
23.17983627319336
23.08414077758789
22.986806869506836
22.889585494995117
22.789363861083984
22.686796188354492
22.579761505126953
22.468294143676758
22.35063362121582
22.226394653320312
22.093841552734375
21.952234268188477
21.799684524536133
21.6351318359375
21.456491470336914
21.26246452331543
21.05084991455078
20.82036781311035
20.568992614746094
20.29586410522461
19.999610900878906
19.680295944213867
19.337650299072266
18.972978591918945
18.587278366088867
18.183042526245117
17.762325286865234
17.328487396240234
16.884227752685547</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-49-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch also gives us an easier (but less flexible) way to define a composition of modules like this. In PyTorch we can define this simple network using <code>nn.Sequential</code></p>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>model(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="92">
<pre><code>tensor([0.3653, 0.3638, 0.3685, 0.3575, 0.3788, 0.3567, 0.3749, 0.3824, 0.3716,
        0.3777, 0.3612, 0.3494, 0.3608, 0.3496, 0.3610, 0.3802, 0.3737, 0.3504,
        0.3581, 0.3621, 0.3859, 0.3814, 0.3410, 0.3605, 0.3768, 0.3890, 0.3599,
        0.3586, 0.3567, 0.3529, 0.3723, 0.3711, 0.3682, 0.3644, 0.3774, 0.3742,
        0.3804, 0.3808, 0.3526, 0.3757, 0.3669, 0.3735, 0.3731, 0.3516, 0.3461,
        0.3566, 0.3558, 0.3496, 0.3594, 0.3775, 0.3755, 0.3808, 0.3586, 0.3792,
        0.3543, 0.3657, 0.3559, 0.3642, 0.3848, 0.3816, 0.3824, 0.3546, 0.3808,
        0.3906, 0.3661, 0.3723, 0.3823, 0.3552, 0.3561, 0.3543, 0.3730, 0.3723,
        0.3523, 0.3669, 0.3531, 0.3590, 0.3764, 0.3702, 0.3680, 0.3841, 0.3798,
        0.3555, 0.3480, 0.3861, 0.3814, 0.3495, 0.3793, 0.3814, 0.3839, 0.3427,
        0.3540, 0.3623, 0.3700, 0.3661, 0.3915, 0.3718, 0.3660, 0.3685, 0.3601,
        0.3597], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>Here <code>nn.Sigmoid</code> is a built-in module that just applies the sigmoid function. Its implementation would look like:</p>
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We could use this to create a network with several hidden layers:</p>
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="94">
<pre><code>tensor([[0.4041],
        [0.4038],
        [0.4038],
        [0.4040],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4041],
        [0.4042],
        [0.4041],
        [0.4038],
        [0.4042],
        [0.4042],
        [0.4039],
        [0.4042],
        [0.4042],
        [0.4040],
        [0.4039],
        [0.4042],
        [0.4042],
        [0.4042],
        [0.4042],
        [0.4041],
        [0.4041],
        [0.4040],
        [0.4041],
        [0.4039],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4042],
        [0.4042],
        [0.4042],
        [0.4039],
        [0.4042],
        [0.4038],
        [0.4040],
        [0.4041],
        [0.4039],
        [0.4040],
        [0.4038],
        [0.4040],
        [0.4042],
        [0.4042],
        [0.4041],
        [0.4042],
        [0.4039],
        [0.4042],
        [0.4038],
        [0.4041],
        [0.4040],
        [0.4040],
        [0.4041],
        [0.4040],
        [0.4038],
        [0.4042],
        [0.4038],
        [0.4041],
        [0.4041],
        [0.4038],
        [0.4041],
        [0.4039],
        [0.4040],
        [0.4039],
        [0.4041],
        [0.4040],
        [0.4041],
        [0.4042],
        [0.4040],
        [0.4040],
        [0.4038],
        [0.4042],
        [0.4039],
        [0.4041],
        [0.4042],
        [0.4038],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4041],
        [0.4042],
        [0.4040],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4039],
        [0.4042],
        [0.4041],
        [0.4041],
        [0.4041],
        [0.4040],
        [0.4040],
        [0.4042],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4038],
        [0.4039],
        [0.4040]], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>PyTorch also provides built-in loss functions. The PyTorch function for the negative log-likelihood for logistic regression is called <code>nn.functional.binary_cross_entropy</code>. It has some sharp edges though.</p>
<p>For one, it expects <code>y</code> to be a float type. We can convert a PyTorch <code>int</code> tensor into a <code>float</code> one by calling the <code>float</code> method.</p>
<p>We also see that our sequential model returns a column vector, so <code>y</code> should match that as well.</p>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>yfloat <span class="op">=</span> y.<span class="bu">float</span>().reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.functional.binary_cross_entropy(predictions, yfloat)</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb85-9"><a href="#cb85-9" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb85-10"><a href="#cb85-10" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb85-11"><a href="#cb85-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.712141752243042
0.7091044783592224
0.7065528631210327
0.7044107913970947
0.7026132941246033
0.7011056542396545
0.6998415589332581
0.6987819671630859
0.6978939771652222
0.6971497535705566
0.696526288986206
0.6960040330886841
0.6955663561820984
0.6951996684074402
0.694892406463623
0.6946350336074829
0.6944191455841064
0.6942383646965027
0.6940867900848389
0.6939594745635986
0.6938527822494507
0.6937631964683533
0.6936879754066467
0.69362473487854
0.6935714483261108
0.6935266852378845
0.6934890151023865
0.6934571266174316
0.6934301853179932
0.6934073567390442
0.693388044834137
0.693371593952179
0.693357527256012
0.6933454871177673
0.6933351755142212
0.6933264136314392
0.6933186054229736
0.6933119297027588
0.693306028842926
0.6933008432388306
0.6932962536811829
0.6932921409606934
0.6932885050773621
0.6932851672172546
0.6932820081710815
0.6932792067527771
0.6932765245437622
0.6932740807533264
0.693271815776825
0.6932694911956787
0.6932673454284668
0.6932653784751892
0.6932634115219116
0.6932615637779236
0.6932596564292908
0.6932578086853027
0.6932560801506042
0.693254292011261
0.6932525634765625
0.693250834941864
0.6932492256164551
0.6932475566864014
0.6932458281517029
0.6932441592216492
0.6932425498962402
0.6932408809661865
0.6932392120361328
0.6932376027107239
0.6932359933853149
0.693234384059906
0.6932327747344971
0.6932311058044434
0.6932294964790344
0.6932278275489807
0.6932263374328613
0.6932246685028076
0.6932230591773987
0.6932214498519897
0.693219780921936
0.6932182312011719
0.6932166218757629
0.693215012550354
0.6932134032249451
0.6932117938995361
0.693210244178772
0.6932085156440735
0.6932069659233093
0.6932052373886108
0.6932037472724915
0.6932021379470825
0.6932004690170288
0.6931988596916199
0.6931973099708557
0.693195641040802
0.6931940317153931
0.6931924223899841
0.6931908130645752
0.693189263343811
0.6931875348091125
0.6931860446929932</code></pre>
</div>
</div>
<p>For convinience, let’s definie a wrapper class for our model.</p>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegressionNeuralNetwork(nn.Module):</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, network):</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> network                   </span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="evaluating-models" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-models">Evaluating models</h2>
<p>We see that we have a <em>lot</em> of options when designing a neural network. So far the choices we’ve seen are: - The number of layers - The number of neurons in each layer - The activation function - The learning rate for gradient descent</p>
<p>And this is just the beginning! As we go on, we’ll learn about many more options that we have.</p>
<p>Let’s take a look at how to make some of these choices. In many real cases, our data will not be a cleanly separated into 2 classes as we’ve seen. For instance, we can look at a noisier version of the dataset we saw before.</p>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-55-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s split this into training and test sets as we’ve seen.</p>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">150</span>:]], y[inds[<span class="dv">150</span>:]]</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-56-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-56-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We’ll start by fitting a logistic regression model as we’ve seen.</p>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, ytrain.flatten())</span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb90-15"><a href="#cb90-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>127.65480041503906
147.39797973632812
99.01510620117188
113.0787582397461
106.0920639038086
161.87832641601562
114.1411361694336
145.89279174804688
98.93666076660156
117.2277603149414
101.8545913696289
140.513427734375
109.4571533203125
145.58836364746094
102.63407897949219
126.65000915527344
102.01583099365234
135.04779052734375
106.65965270996094
142.10926818847656
104.1931381225586
132.24771118164062
102.77326202392578
133.9228973388672
105.18921661376953
139.1488494873047
104.62230682373047
134.86273193359375
103.43161010742188
134.19154357910156
104.5163345336914
137.31463623046875
104.61583709716797
135.83790588378906
103.86148834228516
134.7530059814453
104.25921630859375
136.35430908203125
104.49825286865234
136.070556640625
104.0972900390625
135.22093200683594
104.19034576416016
135.92173767089844
104.38951873779297
136.03619384765625
104.20794677734375
135.51808166503906
104.19206237792969
135.7626190185547
104.31770324707031
135.943603515625
104.25088500976562
135.6765594482422
104.21162414550781
135.72463989257812
104.27830505371094
135.86572265625
104.26254272460938
135.74850463867188
104.2296142578125
135.73023986816406
104.25983428955078
135.81619262695312
104.26236724853516
135.7752227783203
104.24150848388672
135.74542236328125
104.25265502929688
135.78985595703125
104.25914764404297
135.78164672851562
104.24806213378906
135.7581024169922
104.25071716308594
135.77789306640625
104.25615692138672
135.78070068359375
104.2511215209961
135.76658630371094
104.25074005126953
135.7732696533203
104.25418090820312
135.77816772460938
104.2523193359375
135.77090454101562
104.25126647949219
135.77207946777344
104.25308227539062
135.77601623535156
104.2526626586914
135.77297973632812
104.25175476074219
135.7724151611328
104.25259399414062
135.7747802734375
104.25264739990234
135.77366638183594
104.25210571289062
135.77284240722656
104.25238037109375
135.774169921875
104.2525405883789
135.7738494873047
104.25227355957031
135.7733612060547
104.2523422241211
135.773681640625
104.25247192382812
135.7738800048828
104.25233459472656
135.7734832763672
104.25234985351562
135.77362060546875
104.25243377685547
135.77369689941406
104.25235748291016
135.77352905273438
104.25236511230469
135.77362060546875
104.25241088867188
135.773681640625
104.25237274169922
135.7735595703125
104.25234985351562
135.77362060546875
104.2524185180664
135.773681640625
104.25237274169922
135.77354431152344
104.25235748291016
135.77362060546875
104.25240325927734
135.77366638183594
104.25238800048828
135.77357482910156
104.25234985351562
135.77357482910156
104.25241088867188
135.77369689941406
104.25238800048828
135.77359008789062
104.25235748291016
135.77359008789062
104.25239562988281
135.77365112304688
104.25238800048828
135.7736053466797
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875
104.25239562988281
135.77362060546875
104.25237274169922
135.7736053466797
104.25237274169922
135.77362060546875</code></pre>
</div>
</div>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-58-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s compute the accuracy on both the training and the test data</p>
<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, test_acc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.713, Test accuracy: 0.680</code></pre>
</div>
</div>
<div class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">1000</span>),</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">1000</span>, <span class="dv">1000</span>),</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">1000</span>, <span class="dv">1</span>),</span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.00001</span>)</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a>pbar <span class="op">=</span> tqdm.trange(<span class="dv">50000</span>)</span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> pbar:</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, ytrain.flatten())</span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a>    pbar.set_description(<span class="st">'Loss: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> loss.item())</span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(loss.item())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loss: nan:   9%|▊         | 4322/50000 [00:20&lt;03:41, 206.03it/s]   </code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>KeyboardInterrupt: </code></pre>
</div>
</div>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-61-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>