<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 7: PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-pytorch" id="toc-introduction-to-pytorch" class="nav-link active" data-scroll-target="#introduction-to-pytorch">Introduction to PyTorch</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient descent</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn">torch.nn</a></li>
  <li><a href="#evaluating-models" id="toc-evaluating-models" class="nav-link" data-scroll-target="#evaluating-models">Evaluating models</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 7: PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> DecisionBoundaryDisplay</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_boundary(model, X, y):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">xrange</span> <span class="op">=</span> (<span class="op">-</span>X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">+</span> X[:, <span class="dv">0</span>].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    yrange <span class="op">=</span> (<span class="op">-</span>X[:, y].<span class="bu">min</span>() <span class="op">+</span> X[:, y].<span class="bu">max</span>()) <span class="op">/</span> <span class="dv">10</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    feature_1, feature_2 <span class="op">=</span> np.meshgrid(</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="bu">xrange</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="bu">xrange</span>),</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        np.linspace(X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> yrange, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> yrange)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> np.vstack([feature_1.ravel(), feature_2.ravel()]).T</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> np.reshape(model.predict(torch.tensor(grid).<span class="bu">float</span>()).detach().numpy(), feature_1.shape)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    display <span class="op">=</span> DecisionBoundaryDisplay(</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        xx0<span class="op">=</span>feature_1, xx1<span class="op">=</span>feature_2, response<span class="op">=</span>y_pred</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    display.plot()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    display.ax_.scatter(</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, edgecolor<span class="op">=</span><span class="st">"black"</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction-to-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-pytorch">Introduction to PyTorch</h2>
<p>The most basic object in PyTorch is a <code>tensor</code>. Tensor objects behave much like the <code>AutogradValue</code> objects we are creating in the homework! We can create a <code>tensor</code> object with a given value as follows</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor(4.)</code></pre>
</div>
</div>
<p>Performing basic operations on <code>tensor</code> objects gives tensor objects.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>tensor(21.)</code></pre>
</div>
</div>
<p><code>tensor</code> objects also support reverse-mode automatic differentiation! To use this, we must specify that we will want to compute the derivative with respect to a given <code>tensor</code>. We can do this with the <code>requires_grad</code> argument.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">4.</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once we have a <code>tensor</code> that <code>requires_grad</code>, we can perform operations on it to compute a loss.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.log(a) <span class="co"># Functions like log must be called through torch</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor(3.0445, grad_fn=&lt;LogBackward0&gt;)</code></pre>
</div>
</div>
<p>Once we have a loss running the backward pass is done exactly as in the homework. First we call <code>backward()</code> on the loss <code>tensor</code> object, then we can access the derivative through the <code>grad</code> property of <code>x</code>.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor(0.3810)</code></pre>
</div>
</div>
<p>We can also create <code>tensor</code> objects that wrap arrays.</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(np.array([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>We can also just directly create tensors as we would numpy arrays</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([3, 4, 5])</code></pre>
</div>
</div>
<p>Including convienience constructors.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.ones((<span class="dv">5</span>,)))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torch.zeros((<span class="dv">2</span>, <span class="dv">3</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([1., 1., 1., 1., 1.])
tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>Automatic differentiation still works for arrays. In this case it gives use the gradient of the loss (hence the <code>grad</code> property).</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="fl">3.</span>, <span class="fl">4.</span>, <span class="fl">5.</span>], requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> torch.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>L</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor(50., grad_fn=&lt;SumBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>L.backward()</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>x.grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor([ 6.,  8., 10.])</code></pre>
</div>
</div>
<p>We can convert <code>tensor</code> objects back to numpy by calling <code>x.detach().numpy()</code>. (<code>detach</code> removes the variable from any automatic differentiation computations)</p>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>x.detach().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>array([1., 1., 1., 1., 1.], dtype=float32)</code></pre>
</div>
</div>
<p>At this point it’s probably worth remarking on where the name <code>tensor</code> comes from.</p>
<p>So far we’ve discussed 3 kinds of array objects - <strong>Scalars:</strong> which are just single values (0-dimensional) - <strong>Vectors:</strong> 1-dimensional arrays of numbers - <strong>Matrices:</strong> 2-dimensional arrays of numbers</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <strong>tensor</strong> is the generalization of a vector or matrix to <em>any</em> number of dimensions. For example, a 3-dimensional tensor can be seen in multiple ways.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<p>A <code>tensor</code> object can be created with any number of dimensions. For example, we could create a 2x2x2 tensor as:</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.tensor([[[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]]])</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>tensor([[[1, 2],
         [3, 4]],

        [[5, 6],
         [7, 8]]])</code></pre>
</div>
</div>
<p>Or we could create the tensor in the image using <code>arange</code> and <code>reshape</code>.</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.arange(<span class="dv">30</span>).reshape((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([[[ 0,  1,  2,  3,  4],
         [ 5,  6,  7,  8,  9]],

        [[10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]],

        [[20, 21, 22, 23, 24],
         [25, 26, 27, 28, 29]]])</code></pre>
</div>
</div>
<p>4-dimensional tensors can also be visualized</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image-2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Alt text</figcaption><p></p>
</figure>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>t.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>torch.Size([3, 2, 4, 5])</code></pre>
</div>
</div>
<p>There are some notable differences between torch and numpy when it comes to operations. The important one to watch out for at this point is matrix multiplation. In numpy we accomplished with with <code>np.dot</code>:</p>
<div class="cell" data-execution_count="170">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>np.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="170">
<pre><code>array([[5., 5.],
       [5., 5.],
       [5., 5.],
       [5., 5.]])</code></pre>
</div>
</div>
<p>In PyTorch <code>torch.dot</code> only does vector dot products and thus only applies to 1-dimensional <code>tensor</code> objects:</p>
<div class="cell" data-execution_count="171">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones((<span class="dv">5</span>, <span class="dv">2</span>))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>torch.dot(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>RuntimeError: 1D tensors expected, but got 2D and 2D tensors</code></pre>
</div>
</div>
<p>Instead we use the <code>torch.matmul</code> function for this purpose</p>
<div class="cell" data-execution_count="172">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>torch.matmul(x, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="172">
<pre><code>tensor([[5., 5.],
        [5., 5.],
        [5., 5.],
        [5., 5.]])</code></pre>
</div>
</div>
<p>PyTorch also has many handy built-in functions that numpy doesn’t have, such as <code>sigmoid</code>.</p>
<div class="cell" data-execution_count="173">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">50</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> torch.sigmoid(x)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This makes it very easy to implement something like logistic regression.</p>
<div class="cell" data-execution_count="174">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.ones((dims,), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> torch.zeros((), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>        f_X <span class="op">=</span> torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(f_X)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.predict_probability(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s try loading a dataset, converting it to <code>tensor</code> and making predictions</p>
<div class="cell" data-execution_count="175">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(noise<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>When working with PyTorch, it is convention to separate the loss function from the model, where the loss function will just take predictions and labels.</p>
<div class="cell" data-execution_count="176">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NLL(pred, y):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    LL <span class="op">=</span> y <span class="op">*</span> torch.log(pred) <span class="op">+</span> (<span class="fl">1.</span> <span class="op">-</span> y) <span class="op">*</span> torch.log(<span class="fl">1.</span> <span class="op">-</span> pred)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>LL.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h2>
<p>Gradient descent is also implemented in PyTorch in the <code>optim</code> module.</p>
<div class="cell" data-execution_count="177">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Gradient descent works a bit differently in PyTorch than what we’ve seen. We first need to construct a gradient descent <em>object</em> which specifies which values we’re optimizing and what the learning rate will be. We specify the values to optimize by simply passing a list of weights/parameters to the constructor.</p>
<p>In PyTorch, basic gradient descent is encapsulated in the <code>optim.SGD</code> class (<code>SGD</code> stands for <em>stochastic gradient descent</em>, we’ll talk about what <em>stochastic</em> means in this context next week.)</p>
<div class="cell" data-execution_count="178">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([model.weights, model.bias], lr<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that this object doesn’t even take in the function we’re trying to optimize, only the inputs. We need to call the function ourselves <em>and</em> run <code>backward()</code> to compute the gradients.</p>
<div class="cell" data-execution_count="179">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s look at our model <code>weights</code></p>
<div class="cell" data-execution_count="180">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>model.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="180">
<pre><code>tensor([1., 1.], requires_grad=True)</code></pre>
</div>
</div>
<p>We can take a single step of gradient descent using the <code>step</code> method of the optimizer.</p>
<div class="cell" data-execution_count="181">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>model.weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="181">
<pre><code>tensor([ 1.5771, -1.4567], requires_grad=True)</code></pre>
</div>
</div>
<p>We see that this actually updates the weights themselves!</p>
<p>It’s important to note that in PyTorch, calling <code>backward</code> does <strong>not</strong> clear the value stored in grad. So computing the gradient multiple times will result in updates to the gradient.</p>
<div class="cell" data-execution_count="182">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>NLL(model.predict_probability(X), y).backward()</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-5.7714, 24.5667])
tensor([-16.5282,  28.3895])
tensor([-27.2850,  32.2123])</code></pre>
</div>
</div>
<p>We can clear the stored gradients using the optimizer.</p>
<div class="cell" data-execution_count="183">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.weights.grad)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>None</code></pre>
</div>
</div>
<p>So far we’ve only taking a single step of gradient descent. In order to run many steps, we need to write a loop to do everything we just saw.</p>
<div class="cell" data-execution_count="184">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model.predict_probability(X)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>47.09358596801758
43.22157669067383
34.15849304199219
31.948076248168945
30.564128875732422
29.81253433227539
29.332521438598633
29.01232147216797
28.785003662109375
28.61783218383789</code></pre>
</div>
</div>
<p>We should now see that our model has been optimized!</p>
<div class="cell" data-execution_count="185">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn">torch.nn</h2>
<p>While PyTorch as a tool for automatic differentiation and optimization would be useful by itself. It actually gives us a lot more than that!</p>
<p>On of the most important features of PyTorch is its model-building tools in the <code>torch.nn</code> module. This gives us a lot of powerful features that we can use to build complex neural networks!</p>
<div class="cell" data-execution_count="186">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s start by building out logistic regression model in the <code>torch.nn</code> framwork. In order for a model to benefit from <code>torch.nn</code> our model class needs to inheret from <code>nn.Module</code></p>
<div class="cell" data-execution_count="192">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> nn.Parameter(torch.ones((dims,)))</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(()))</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(torch.matmul(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There are 2 changes to note here. The first is that we wrapped our <code>weights</code> and <code>bias</code> terms in <code>nn.Parameter</code>. This tells PyTorch that these are the parameters we will want to optimize. We don’t need to specify <code>requires_grad</code> for parameters, PyTorch will take care of that for us.</p>
<p>The second is that we moved the implmentation of <code>predict_probability</code> to <code>forward</code>. In PyTorch models the <code>forward</code> method is special, it defines the model as a function. If we call the model as a function <code>forward</code> will be called internally.</p>
<div class="cell" data-execution_count="193">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="193">
<pre><code>tensor([0.8061, 0.9062, 0.5295, 0.8793, 0.5760, 0.7457, 0.7801, 0.5451, 0.2699,
        0.6022, 0.8936, 0.7359, 0.8146, 0.7429, 0.3595, 0.6654, 0.8362, 0.5015,
        0.5419, 0.7889, 0.8034, 0.7207, 0.7751, 0.8122, 0.3505, 0.3228, 0.6957,
        0.5209, 0.6943, 0.8058, 0.7483, 0.5522, 0.7572, 0.5012, 0.5702, 0.9059,
        0.8479, 0.5518, 0.7708, 0.7704, 0.5722, 0.6250, 0.4369, 0.8790, 0.5790,
        0.6379, 0.4764, 0.5471, 0.7326, 0.4577, 0.9034, 0.7895, 0.6305, 0.6588,
        0.6457, 0.4482, 0.7797, 0.7687, 0.8219, 0.9444, 0.5676, 0.6338, 0.5766,
        0.8070, 0.3013, 0.7863, 0.6884, 0.7908, 0.7290, 0.7986, 0.5785, 0.6059,
        0.7611, 0.8420, 0.6813, 0.6439, 0.7005, 0.7083, 0.5252, 0.4524, 0.2647,
        0.7873, 0.5357, 0.5840, 0.9021, 0.6080, 0.7858, 0.7382, 0.7141, 0.9162,
        0.5217, 0.7707, 0.7938, 0.6209, 0.4647, 0.6258, 0.3928, 0.3268, 0.5453,
        0.8687], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>This means that we can use instances of <code>nn.Module</code> as parameterized functions. For example, we might create a general linear (technically affine) function in the same way.</p>
<p><span class="math display">\[f(\mathbf{x}) = \mathbf{x}^T\mathbf{W}^T + \mathbf{b},  \quad f: \mathbb{R}^i \rightarrow \mathbb{R}^o\]</span></p>
<p>Note that here we are <strong>not</strong> assuming an augmented representation of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<div class="cell" data-execution_count="195">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Linear(nn.Module):</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weightsT <span class="op">=</span> nn.Parameter(torch.ones((inputs, outputs)))</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros((outputs,)))</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(X, <span class="va">self</span>.weightsT) <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can use this module to implement out logistic regression model above.</p>
<div class="cell" data-execution_count="200">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegression(nn.Module):</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims):</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(dims, <span class="dv">1</span>)                       <span class="co"># Dims input 1 output</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X)).reshape((<span class="op">-</span><span class="dv">1</span>,)) <span class="co"># Turn output into a vector</span></span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="201">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="201">
<pre><code>tensor([0.8061, 0.9062, 0.5295, 0.8793, 0.5760, 0.7457, 0.7801, 0.5451, 0.2699,
        0.6022, 0.8936, 0.7359, 0.8146, 0.7429, 0.3595, 0.6654, 0.8362, 0.5015,
        0.5419, 0.7889, 0.8034, 0.7207, 0.7751, 0.8122, 0.3505, 0.3228, 0.6957,
        0.5209, 0.6943, 0.8058, 0.7483, 0.5522, 0.7572, 0.5012, 0.5702, 0.9059,
        0.8479, 0.5518, 0.7708, 0.7704, 0.5722, 0.6250, 0.4369, 0.8790, 0.5790,
        0.6379, 0.4764, 0.5471, 0.7326, 0.4577, 0.9034, 0.7895, 0.6305, 0.6588,
        0.6457, 0.4482, 0.7797, 0.7687, 0.8219, 0.9444, 0.5676, 0.6338, 0.5766,
        0.8070, 0.3013, 0.7863, 0.6884, 0.7908, 0.7290, 0.7986, 0.5785, 0.6059,
        0.7611, 0.8420, 0.6813, 0.6439, 0.7005, 0.7083, 0.5252, 0.4524, 0.2647,
        0.7873, 0.5357, 0.5840, 0.9021, 0.6080, 0.7858, 0.7382, 0.7141, 0.9162,
        0.5217, 0.7707, 0.7938, 0.6209, 0.4647, 0.6258, 0.3928, 0.3268, 0.5453,
        0.8687], grad_fn=&lt;ReshapeAliasBackward0&gt;)</code></pre>
</div>
</div>
<p>The power here is that because <code>Linear</code> is also an instance of <code>nn.Module</code>, PyTorch knows that it’s weights should also be considered part of our models weights. We can access the weights of a model using the <code>parameters()</code> method.</p>
<div class="cell" data-execution_count="203">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="203">
<pre><code>[Parameter containing:
 tensor([[1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This lets us easily apply gradient descent:</p>
<div class="cell" data-execution_count="204">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>76.70811462402344
47.09358596801758
43.22157669067383
34.15849304199219
31.948076248168945
30.564128875732422
29.81253433227539
29.332521438598633
29.01232147216797
28.785003662109375</code></pre>
</div>
</div>
<div class="cell" data-execution_count="205">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-41-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch unsurprisingly also provides a built-in <code>Linear</code> module. As <code>nn.Linear</code>.</p>
<div class="cell" data-execution_count="206">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="206">
<pre><code>Linear(in_features=2, out_features=1, bias=True)</code></pre>
</div>
</div>
<p>Knowing how to make a parameterized function in PyTorch, let’s consider making a neural network layer with a sigmoid activation function.</p>
<p><span class="math display">\[f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}^T + \mathbf{b})\]</span></p>
<div class="cell" data-execution_count="210">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, outputs):</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(inputs, outputs)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(<span class="va">self</span>.linear(X))</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s create a layer with 10 neurons. (So <span class="math inline">\(\mathbf{W}:\ (10 \times 2)\)</span>)</p>
<div class="cell" data-execution_count="212">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> SigmoidLayer(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>layer(X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([100, 2])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="212">
<pre><code>torch.Size([100, 10])</code></pre>
</div>
</div>
<p>Let’s use this to create a neural network class for binary classification!</p>
<div class="cell" data-execution_count="219">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dims, hidden_size):</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer <span class="op">=</span> SigmoidLayer(dims, hidden_size)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> Linear(hidden_size, <span class="dv">1</span>)                       </span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>        hidden_neurons <span class="op">=</span> <span class="va">self</span>.layer(X)</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.linear(hidden_neurons)</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(output).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We see that PyTorch recognizes both the parameters of the logistic regression and the parameters of our neural network feature transform:</p>
<div class="cell" data-execution_count="220">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork(<span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="220">
<pre><code>[Parameter containing:
 tensor([[ 0.2096, -0.3608],
         [-0.4380, -0.6813],
         [ 0.3196,  0.4796],
         [-0.4256, -0.6939],
         [ 0.4838,  0.3606],
         [-0.0805,  0.5008],
         [-0.2017,  0.4119],
         [ 0.2697,  0.4036],
         [ 0.5785,  0.2659],
         [-0.6341,  0.1310]], requires_grad=True),
 Parameter containing:
 tensor([-0.0678, -0.6714, -0.4546, -0.4149, -0.3568,  0.7036,  0.4120, -0.6844,
          0.1531,  0.3548], requires_grad=True),
 Parameter containing:
 tensor([[1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.],
         [1.]], requires_grad=True),
 Parameter containing:
 tensor([0.], requires_grad=True)]</code></pre>
</div>
</div>
<p>This means that we can easily run our optimization as before.</p>
<div class="cell" data-execution_count="223">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, y)</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>45.445369720458984
43.70785903930664
47.474220275878906
40.63535690307617
42.115936279296875
36.09637451171875
36.04859161376953
32.92196273803711
32.511871337890625
30.93625831604004
30.521907806396484
29.64164924621582
29.293277740478516
28.74832534790039
28.467708587646484
28.101655960083008
27.878223419189453
27.618236541748047
27.442567825317383
27.252187728881836
27.116464614868164
26.97370719909668
26.86859893798828
26.755901336669922
26.669967651367188
26.572307586669922
26.494722366333008
26.40076446533203
26.323572158813477
26.225831985473633
26.144067764282227
26.0379695892334
25.94887924194336
25.831634521484375
25.73374366760254
25.603410720825195
25.4957332611084
25.350324630737305
25.231674194335938
25.068439483642578
24.936925888061523
24.7518253326416
24.604541778564453
24.391887664794922
24.22479820251465
23.97708511352539
23.785263061523438
23.49335289001465
23.272056579589844
22.926254272460938
22.673450469970703
22.265865325927734
21.986351013183594
21.51370620727539
21.222501754760742
20.68614387512207
20.406362533569336
19.806501388549805
19.55826759338379
18.88671112060547
18.671297073364258
17.915258407592773
17.707300186157227
16.870145797729492
16.630151748657227
15.75007438659668
15.45046329498291
14.591230392456055
14.231971740722656
13.452905654907227
13.056207656860352
12.39054012298584
11.986312866210938
11.43923568725586
11.053666114807129
10.611433029174805
10.2611722946167
9.901641845703125
9.592364311218262
9.293046951293945
9.021784782409668
8.764437675476074
8.524192810058594
8.29626750946045
8.080157279968262
7.874066352844238
7.677170753479004
7.488630771636963
7.307856559753418
7.1343464851379395
6.967673301696777
6.807461261749268
6.653371810913086
6.505088806152344
6.3623199462890625
6.224787712097168
6.0922369956970215
5.964423656463623
5.841118812561035
5.722109794616699</code></pre>
</div>
</div>
<div class="cell" data-execution_count="224">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-48-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>PyTorch also gives us an easier (but less flexible) way to define a composition of modules like this. In PyTorch we can define this simple network using <code>nn.Sequential</code></p>
<div class="cell" data-execution_count="229">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>model(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here <code>nn.Sigmoid</code> is a built-in module that just applies the sigmoid function. Its implementation would look like:</p>
<div class="cell" data-execution_count="241">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SigmoidLayer(nn.Module):</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.sigmoid(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We could use this to create a network with several hidden layers:</p>
<div class="cell" data-execution_count="238">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>),</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a>model(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="238">
<pre><code>tensor([[0.5571],
        [0.5571],
        [0.5575],
        [0.5571],
        [0.5574],
        [0.5572],
        [0.5572],
        [0.5574],
        [0.5576],
        [0.5574],
        [0.5571],
        [0.5573],
        [0.5572],
        [0.5573],
        [0.5575],
        [0.5572],
        [0.5571],
        [0.5575],
        [0.5575],
        [0.5571],
        [0.5572],
        [0.5571],
        [0.5573],
        [0.5572],
        [0.5575],
        [0.5575],
        [0.5572],
        [0.5574],
        [0.5572],
        [0.5571],
        [0.5571],
        [0.5573],
        [0.5571],
        [0.5575],
        [0.5574],
        [0.5570],
        [0.5572],
        [0.5574],
        [0.5572],
        [0.5571],
        [0.5573],
        [0.5572],
        [0.5574],
        [0.5571],
        [0.5573],
        [0.5574],
        [0.5574],
        [0.5575],
        [0.5573],
        [0.5575],
        [0.5571],
        [0.5572],
        [0.5574],
        [0.5574],
        [0.5573],
        [0.5574],
        [0.5572],
        [0.5573],
        [0.5571],
        [0.5570],
        [0.5574],
        [0.5574],
        [0.5573],
        [0.5572],
        [0.5576],
        [0.5571],
        [0.5572],
        [0.5571],
        [0.5573],
        [0.5571],
        [0.5573],
        [0.5573],
        [0.5571],
        [0.5572],
        [0.5572],
        [0.5574],
        [0.5573],
        [0.5571],
        [0.5574],
        [0.5575],
        [0.5576],
        [0.5572],
        [0.5574],
        [0.5574],
        [0.5571],
        [0.5574],
        [0.5572],
        [0.5573],
        [0.5573],
        [0.5570],
        [0.5573],
        [0.5572],
        [0.5571],
        [0.5574],
        [0.5574],
        [0.5572],
        [0.5574],
        [0.5575],
        [0.5575],
        [0.5571]], grad_fn=&lt;SigmoidBackward0&gt;)</code></pre>
</div>
</div>
<p>PyTorch also provides built-in loss functions. The PyTorch function for the negative log-likelihood for logistic regression is called <code>nn.functional.binary_cross_entropy</code>. It has some sharp edges though.</p>
<p>For one, it expects <code>y</code> to be a float type. We can convert a PyTorch <code>int</code> tensor into a <code>float</code> one by calling the <code>float</code> method.</p>
<p>We also see that our sequential model returns a column vector, so <code>y</code> should match that as well.</p>
<div class="cell" data-execution_count="240">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>yfloat <span class="op">=</span> y.<span class="bu">float</span>().reshape((<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(X)        <span class="co"># Now we can just call model!</span></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> nn.functional.binary_cross_entropy(predictions, yfloat)</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.6996942162513733
0.6985191106796265
0.6975520253181458
0.6967566013336182
0.6961024403572083
0.6955646276473999
0.6951226592063904
0.6947595477104187
0.6944611072540283
0.6942159533500671
0.6940144896507263
0.6938489675521851
0.6937129497528076
0.6936012506484985
0.6935093402862549
0.6934338212013245
0.6933718323707581
0.6933207511901855
0.6932787299156189
0.6932441592216492
0.6932157278060913
0.6931923031806946
0.6931729316711426
0.6931568384170532
0.6931436061859131
0.6931325793266296
0.693123459815979
0.6931159496307373
0.6931095123291016
0.6931042671203613
0.6930997371673584
0.6930959224700928
0.6930928230285645
0.6930900812149048
0.693087637424469
0.6930855512619019
0.6930838227272034
0.6930822134017944
0.6930808424949646
0.6930795907974243
0.6930783987045288
0.6930773258209229
0.6930763721466064
0.6930754780769348
0.6930745840072632
0.6930738091468811
0.6930729746818542
0.6930721998214722
0.6930714249610901
0.6930708289146423
0.6930701732635498
0.6930694580078125
0.6930687427520752
0.6930680871009827
0.6930674910545349
0.6930667757987976
0.6930661797523499
0.6930655837059021
0.6930650472640991
0.693064272403717
0.6930636763572693
0.6930630207061768
0.693062424659729
0.6930618286132812
0.6930612325668335
0.6930605173110962
0.6930599808692932
0.6930593848228455
0.6930587291717529
0.69305819272995
0.6930574774742126
0.6930568814277649
0.6930561661720276
0.6930555701255798
0.6930549740791321
0.6930543780326843
0.6930537223815918
0.6930531859397888
0.6930525898933411
0.6930519938468933
0.6930513978004456
0.6930506825447083
0.6930500864982605
0.6930494904518127
0.6930487751960754
0.6930482387542725
0.6930476427078247
0.693047046661377
0.6930463314056396
0.6930457353591919
0.6930451393127441
0.6930444836616516
0.6930439472198486
0.6930432915687561
0.6930426955223083
0.6930420398712158
0.6930413842201233
0.6930407881736755
0.693040132522583
0.6930396556854248</code></pre>
</div>
</div>
<p>For convinience, let’s definie a wrapper class for our model.</p>
<div class="cell" data-execution_count="266">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LogisticRegressionNeuralNetwork(nn.Module):</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, network):</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.network <span class="op">=</span> network                   </span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.network(X).reshape((<span class="op">-</span><span class="dv">1</span>,))</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_probability(<span class="va">self</span>, X):</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X)</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb84-13"><a href="#cb84-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="evaluating-models" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-models">Evaluating models</h2>
<p>We see that we have a <em>lot</em> of options when designing a neural network. So far the choices we’ve seen are: - The number of layers - The number of neurons in each layer - The activation function - The learning rate for gradient descent</p>
<p>And this is just the beginning! As we go on, we’ll learn about many more options that we have.</p>
<p>Let’s take a look at how to make some of these choices. In many real cases, our data will not be a cleanly separated into 2 classes as we’ve seen. For instance, we can look at a noisier version of the dataset we saw before.</p>
<div class="cell" data-execution_count="289">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">300</span>, noise<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> torch.tensor(X).<span class="bu">float</span>(), torch.tensor(y)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(<span class="dv">2</span>)</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-54-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s split this into training and test sets as we’ve seen.</p>
<div class="cell" data-execution_count="290">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>inds <span class="op">=</span> np.arange(X.shape[<span class="dv">0</span>])</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(inds)</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a>Xtrain, ytrain <span class="op">=</span> X[inds[:<span class="dv">150</span>]], y[inds[:<span class="dv">150</span>]]</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a>Xtest, ytest <span class="op">=</span> X[inds[<span class="dv">150</span>:]], y[inds[<span class="dv">150</span>:]]</span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtest, ytest)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-55-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-55-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We’ll start by fitting a logistic regression model as we’ve seen.</p>
<div class="cell" data-execution_count="291">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">500</span>):</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, ytrain.flatten())</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>120.94316864013672
127.796875
86.81439971923828
73.72380065917969
72.54862976074219
72.56034088134766
73.04600524902344
75.56434631347656
84.96688842773438
124.48078155517578
118.78359985351562
151.2896728515625
93.05940246582031
83.9063720703125
83.98695373535156
110.54647064208984
113.08613586425781
155.552490234375
96.2688217163086
87.4237060546875
85.07091522216797
111.030029296875
111.80404663085938
152.46817016601562
96.31392669677734
88.99681091308594
86.41571044921875
114.03663635253906
111.0711898803711
147.8506317138672
95.83660888671875
90.59500885009766
88.1241683959961
117.94168090820312
110.042236328125
141.86573791503906
95.25202941894531
93.01756286621094
90.48155212402344
122.63541412353516
108.35640716552734
134.02325439453125
94.65928649902344
97.04625701904297
93.74947357177734
127.61271667480469
105.7712631225586
124.43855285644531
94.2139663696289
103.38475036621094
97.69415283203125
130.93934631347656
102.47567749023438
115.14863586425781
94.21851348876953
111.37113189697266
101.0181884765625
130.2666015625
99.34185028076172
109.46971893310547
95.05870056152344
118.64732360839844
102.36280822753906
125.94175720214844
97.23176574707031
108.67488861083984
96.79950714111328
123.5271987915039
101.78687286376953
120.22323608398438
96.26702880859375
111.5051498413086
98.84413146972656
125.38864135742188
100.23058319091797
115.39372253417969
96.24542999267578
115.82815551757812
100.2905044555664
124.31881713867188
98.64268493652344
113.03353881835938
96.9765625
119.71000671386719
100.64749145507812
121.38877868652344
97.58013916015625
113.34455871582031
98.1272201538086
122.00117492675781
100.09971618652344
118.13880157470703
97.1840591430664
115.40536499023438
99.19065856933594
122.35924530029297
99.17137145996094
115.87261962890625
97.37897491455078
117.93272399902344
99.74707794189453
121.15564727783203
98.3401107788086
115.18673706054688
97.96800231933594
119.88674926757812
99.70308685302734
119.24085235595703
97.86235046386719
115.88703155517578
98.6565933227539
120.73719787597656
99.25977325439453
117.52322387695312
97.78903198242188
117.3037338256836
99.15402221679688
120.4773941040039
98.71568298339844
116.60206604003906
98.0367202758789
118.7052993774414
99.31016540527344
119.50086975097656
98.30267333984375
116.60418701171875
98.4391860961914
119.58499145507812
99.15995788574219
118.36622619628906
98.12686157226562
117.26374816894531
98.80760192871094
119.76678466796875
98.85387420654297
117.54559326171875
98.18272399902344
118.14496612548828
99.00785827636719
119.37471771240234
98.55467224121094
117.26272583007812
98.38886260986328
118.85810089111328
99.00777435302734
118.71200561523438
98.3697509765625
117.4724349975586
98.63040924072266
119.18733978271484
98.86509704589844
118.10199737548828
98.33289337158203
117.95574951171875
98.80648040771484
119.11719512939453
98.67598724365234
117.76033782958984
98.41654968261719
118.45460510253906
98.8672103881836
118.78409576416016
98.52455139160156
117.74071502685547
98.5576171875
118.77909851074219
98.82164764404297
118.38543701171875
98.4555892944336
117.9595718383789
98.6878662109375
118.85733795166016
98.71763610839844
118.08940887451172
98.47119140625
118.2676010131836
98.76087188720703
118.72867584228516
98.61116027832031
117.9787368774414
98.54232788085938
118.52452087402344
98.76475524902344
118.49866485595703
98.54246520996094
118.04289245605469
98.62737274169922
118.6490707397461
98.71753692626953
118.28165435791016
98.52627563476562
118.20873260498047
98.69055938720703
118.63154602050781
98.65156555175781
118.15560913085938
98.55397033691406
118.385498046875
98.71404266357422
118.51820373535156
98.59686279296875
118.1423568725586
98.6031494140625
118.50421142578125
98.7000503540039
118.3780288696289
98.5705795288086
118.21531677246094
98.64945983886719
118.53661346435547
98.6644287109375
118.27118682861328
98.57455444335938
118.32283020019531
98.67626953125
118.49476623535156
98.62669372558594
118.22846221923828
98.5988540649414
118.41502380371094
98.6789321899414
118.4150619506836
98.60150146484375
118.24784088134766
98.6287841796875
118.46170043945312
98.6632080078125
118.33781433105469
98.59473419189453
118.30470275878906
98.65155029296875
118.45823669433594
98.64019775390625
118.29145812988281
98.60377502441406
118.36726379394531
98.66059875488281
118.41978454589844
98.62055206298828
118.28459167480469
98.62083435058594
118.41045379638672
98.65635681152344
118.37055969238281
98.61065673828125
118.30887603759766
98.63729095458984
118.42353057861328
98.64413452148438
118.33201599121094
98.61152648925781
118.34638977050781
98.64715576171875
118.4100570678711
98.63078308105469
118.3156967163086
98.61978912353516
118.37939453125
98.64855194091797
118.38249969482422
98.6216049194336
118.32140350341797
98.63028717041016
118.39678192138672
98.6433334350586
118.35511016845703
98.61885833740234
118.34088134765625
98.63848876953125
118.39653015136719
98.63533020019531
118.33805084228516
98.62176513671875
118.36289978027344
98.6419448852539
118.38346099853516
98.6282730102539
118.33493041992188
98.627685546875
118.37863159179688
98.64068603515625
118.3661880493164
98.62457275390625
118.34294891357422
98.63352966308594
118.38384246826172
98.63650512695312
118.35232543945312
98.62468719482422
118.35601806640625
98.63714599609375
118.37954711914062
98.63177490234375
118.34615325927734
98.62748718261719
118.36784362792969
98.63780975341797
118.37000274658203
98.62845611572266
118.3477783203125
98.63115692138672
118.37418365478516
98.6360855102539
118.36032104492188
98.62736511230469
118.35443115234375
98.63410949707031
118.37451934814453
98.63329315185547
118.35408782958984
98.62828063964844
118.36212921142578
98.63542175292969
118.3700942993164
98.63076782226562
118.3526611328125
98.63032531738281
118.36785888671875
98.63505554199219
118.36405181884766
98.62939453125
118.35536193847656
98.63241577148438
118.36991882324219
98.63362884521484
118.3590087890625
98.62935638427734
118.3598861694336
98.63375854492188
118.36863708496094
98.6319580078125
118.356689453125
98.63031005859375
118.36418151855469
98.63402557373047
118.36526489257812
98.63074493408203
118.35716247558594
98.63162231445312
118.36651611328125
98.63343811035156
118.36174774169922
98.63031768798828
118.35944366455078
98.63267517089844
118.36671447753906
98.63246154785156
118.35950469970703
98.63060760498047
118.36214447021484
98.6331787109375
118.36527252197266
98.6315689086914
118.35890197753906
98.6313247680664
118.36426544189453
98.63307189941406
118.36304473876953
98.63104248046875
118.35980224609375
98.63208770751953
118.3650894165039
98.632568359375
118.36125946044922
98.63101196289062
118.36143493652344
98.632568359375
118.36466217041016
98.6319580078125
118.36034393310547
98.63134765625
118.36296081542969
98.6326904296875
118.36346435546875
98.63151550292969
118.36048889160156
98.63182830810547
118.36389923095703
98.63247680664062
118.36215209960938
98.63134002685547
118.36129760742188
98.63221740722656
118.36400604248047
98.63213348388672
118.36127471923828
98.6314468383789
118.36231994628906
98.63241577148438
118.36347961425781
98.63179016113281
118.36103057861328
98.63170623779297
118.36312866210938
98.63236236572266
118.36260986328125
98.63160705566406
118.36141967773438
98.63201141357422
118.36337280273438
98.63214874267578
118.3619384765625
98.6316146850586
118.36204528808594
98.63218688964844
118.36318969726562
98.63194274902344
118.36162567138672
98.63172149658203
118.36259460449219
98.63220977783203
118.36278533935547
98.63180541992188
118.36174774169922
98.63188934326172
118.36285400390625
98.63212585449219
118.3623275756836
98.63175201416016
118.36198425292969
98.63203430175781
118.3629150390625
98.63201141357422
118.3620834350586
98.63178253173828
118.36233520507812
98.63207244873047
118.36270904541016
98.63191986083984
118.36202239990234
98.63186645507812
118.36255645751953
98.63206481933594
118.36249542236328
98.63185119628906
118.3620834350586
98.6319351196289
118.36263275146484
98.63203430175781
118.36231994628906
98.63185119628906
118.3622817993164
98.63198852539062
118.36259460449219
98.6319580078125
118.36223602294922
98.63187408447266
118.3624038696289
98.63201904296875
118.36248779296875
98.63190460205078
118.36217498779297
98.63190460205078
118.36251068115234
98.63200378417969
118.36241149902344
98.63188934326172
118.36225128173828
98.6319580078125
118.36256408691406
98.63198852539062
118.36231994628906
98.63186645507812
118.36228942871094
98.63198852539062
118.36256408691406
98.6319580078125
118.36225128173828
98.63190460205078
118.3624038696289
98.63200378417969
118.36248016357422
98.63191986083984
118.36226654052734
98.63190460205078
118.36244201660156
98.63198852539062
118.36241912841797
98.63191223144531
118.36229705810547
98.6319351196289
118.36243438720703
98.6319808959961
118.36238098144531
98.63189697265625
118.36228942871094
98.63194274902344
118.36248779296875
98.63197326660156
118.36235046386719
98.63190460205078
118.36233520507812
98.6319580078125
118.36247253417969
98.63192749023438
118.36227416992188
98.63191986083984
118.36241912841797
98.63198852539062
118.36244201660156</code></pre>
</div>
</div>
<div class="cell" data-execution_count="292">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-57-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s compute the accuracy on both the training and the test data</p>
<div class="cell" data-execution_count="293">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> (model.predict(Xtrain) <span class="op">==</span> ytrain).<span class="bu">float</span>().mean()</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> (model.predict(Xtest) <span class="op">==</span> ytest).<span class="bu">float</span>().mean()</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Training accuracy: </span><span class="sc">%.3f</span><span class="st">, Test accuracy: </span><span class="sc">%.3f</span><span class="st">'</span> <span class="op">%</span> (train_acc, test_acc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.713, Test accuracy: 0.760</code></pre>
</div>
</div>
<div class="cell" data-execution_count="313">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> nn.Sequential(</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">2</span>, <span class="dv">250</span>),</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">250</span>, <span class="dv">250</span>),</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">250</span>, <span class="dv">250</span>),</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">250</span>, <span class="dv">250</span>),</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">250</span>, <span class="dv">1</span>),</span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>    nn.Sigmoid(),</span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegressionNeuralNetwork(network)   </span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.00001</span>)</span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-17"><a href="#cb92-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25000</span>):</span>
<span id="cb92-18"><a href="#cb92-18" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(Xtrain)        <span class="co"># Now we can just call model!</span></span>
<span id="cb92-19"><a href="#cb92-19" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> NLL(predictions, ytrain.flatten())</span>
<span id="cb92-20"><a href="#cb92-20" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb92-21"><a href="#cb92-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb92-22"><a href="#cb92-22" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb92-23"><a href="#cb92-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(loss.item())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>KeyboardInterrupt: </code></pre>
</div>
</div>
<div class="cell" data-execution_count="310">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>plot_boundary(model, Xtrain, ytrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="notebook_files/figure-html/cell-60-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>