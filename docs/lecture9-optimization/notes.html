<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.319">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>CS 152: Neural Networks - Lecture 9: Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CS 152: Neural Networks</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../calendar/calendar.html" rel="" target="">
 <span class="menu-text">Calendar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/repositories" rel="" target="">
 <span class="menu-text">Homeworks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://harveymuddcollege.instructure.com/courses/615/" rel="" target="">
 <span class="menu-text">Canvas</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/orgs/CS152-Neural-Networks-Fall-2023/" rel="" target="">
 <span class="menu-text">Github</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#playground" id="toc-playground" class="nav-link active" data-scroll-target="#playground">Playground</a></li>
  <li><a href="#initialization" id="toc-initialization" class="nav-link" data-scroll-target="#initialization">Initialization</a>
  <ul class="collapse">
  <li><a href="#symmetry-breaking" id="toc-symmetry-breaking" class="nav-link" data-scroll-target="#symmetry-breaking">Symmetry-breaking</a></li>
  <li><a href="#visualizing-learning-rates" id="toc-visualizing-learning-rates" class="nav-link" data-scroll-target="#visualizing-learning-rates">Visualizing learning rates</a></li>
  <li><a href="#scaled-initialization" id="toc-scaled-initialization" class="nav-link" data-scroll-target="#scaled-initialization">Scaled initialization</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 9: Optimization</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="playground" class="level1">
<h1>Playground</h1>
<p>Try out the concepts from this lecture in the <a href="https://cs152-neural-networks-fall-2023.github.io/playground">Neural Network Playground!</a></p>
</section>
<section id="initialization" class="level1">
<h1>Initialization</h1>
<p>So far we’ve seen how train neural-networks with gradient descent. Recall that the gradient descent update for a weight <span class="math inline">\(\mathbf{w}\)</span> at step <span class="math inline">\(k\)</span> is: <span class="math display">\[ \mathbf{w}^{(k+1)} \longleftarrow \mathbf{w}^{(k)} - \alpha \nabla_{\mathbf{w}} \textbf{Loss}(\mathbf{w}^{(k)}, \mathbf{X}, \mathbf{y})\]</span></p>
<p>We subtract the gradient of the loss with respect to <span class="math inline">\(\mathbf{w}\)</span> from the current estimate of <span class="math inline">\(\mathbf{w}\)</span>. An important consideration for this algorithm is how to set the initial guess <span class="math inline">\(\mathbf{w}^{(0)}\)</span>. We call this process <strong>initialization</strong>.</p>
<section id="symmetry-breaking" class="level2">
<h2 class="anchored" data-anchor-id="symmetry-breaking">Symmetry-breaking</h2>
<p>In neural networks, we typically initialize parameters <em>randomly</em>. One important reason for random initialization is to make sure that different parameters have different starting values. To see why this is needed, let’s consider the prediction function for a simple neural network that takes in 1-dimensional inputs:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sigma(\mathbf{x}^T\mathbf{W}_1)^T\mathbf{w}_0=\sigma(x_1 w_{11}) w_{01} +\sigma (x_1 w_{12})w_{02}
\]</span></p>
<p>In this case we have 4 parameters: <span class="math inline">\(w_{01}, w_{02}, w_{11}, w_{12}\)</span>. If we initialize all to the same value, say <span class="math inline">\(w_{**} = a\)</span>, let’s see what happens to the derivatives we compute:</p>
<p><span class="math display">\[
\frac{d}{dw_{01}} f(\mathbf{x}) = \sigma(x_1 w_{11}) = \sigma(x_1 a)
\]</span></p>
<p><span class="math display">\[
\frac{d}{dw_{02}} f(\mathbf{x}) = \sigma(x_1 w_{12}) = \sigma(x_1 a)
\]</span></p>
<p>We see that <span class="math inline">\(\frac{d}{dw_{01}} = \frac{d}{dw_{02}}\)</span>! Our gradient descent update will set:</p>
<p><span class="math display">\[
w_{01}^{(1)} \longleftarrow w_{01}^{(0)} - \alpha \frac{d}{dw_{01}} = a - \alpha \sigma(x_1 a)
\]</span></p>
<p><span class="math display">\[
w_{02}^{(1)} \longleftarrow w_{02}^{(0)} - \alpha \frac{d}{dw_{02}} = a - \alpha \sigma(x_1 a)
\]</span></p>
<p>So after each gradient descent update the two values will continue to be the same! The gradient decent algorithm has no way to distinguish between these two weights and so it is stuck finding solutions where <span class="math inline">\(w_{01} = w_{02}\)</span> and <span class="math inline">\(w_{11}=w_{12}\)</span>. We call this the symmetry problem, and it means we no longer get any benefit from having multiple neurons.</p>
<p>We can see this in practice with a simple network:</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img src="images/paste-1.png" class="img-fluid"><br>
When the network is initialized with symmetry, the two neurons will always have the same output and our solution is poor.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><img src="images/paste-2.png" class="img-fluid"></p>
<p>When initialized randomly, the two neurons can create different transforms and a much better solution is found.</p>
</div>
</div>
<p>If we plot the loss as a function of two <span class="math inline">\(w_{01}\)</span> and <span class="math inline">\(w_{02}\)</span> we can see what is happening graphically.</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img src="images/paste-3.png" class="img-fluid"></p>
<p>Initializing the two parameters equal corresponds to sitting on a ridge of the loss surface, there are equally valid solutions on either side, but gradient descent gives us no way to chose between them.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><img src="images/paste-6.png" class="img-fluid"></p>
<p>If we plot the (negative) gradient of the loss we see that the gradient of any point on the ridge always points along the ridge. Gradient descent corresponds to following these arrows to find a minimum.</p>
</div>
</div>
</section>
<section id="visualizing-learning-rates" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-learning-rates">Visualizing learning rates</h2>
<p>As an aside, plotting the gradient as a vector field also gives us an convenient way to visualize the effects of different learning rates. Recall that the learning rate corresponds to how much we <em>scale</em> the gradient each time we take a step.</p>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="images/paste-11.png" class="img-fluid"></p>
<p>A small learning rate means we will move slowly, so It may take a long time to find the minimum.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:30%;">
<p><img src="images/paste-10.png" class="img-fluid"></p>
<p>A well-chosen learning rate lets us find a minimum quickly.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:30%;">
<p><img src="images/paste-13.png" class="img-fluid"></p>
<p>A too-large learning rate means that steps may take us flying past the minimum!</p>
</div>
</div>
</section>
<section id="scaled-initialization" class="level2">
<h2 class="anchored" data-anchor-id="scaled-initialization">Scaled initialization</h2>
<p>Now that we’ve seen the benefits of initializing randomly, we need to consider what distribution to initialize from. An obvious choice might be a standard normal distribution, with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[w_{i} \sim \mathcal{N}(0, 1) \quad \forall\ w_{i} \in \mathbf{w}\]</span>This has a subtle issue though. To see why let’s consider a linear function defined by randomly initialized weights:</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^d x_i w_i
\]</span></p>
<p>Let’s consider the mean and variance of this output with respect to <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\mathbb{E} \big[f(\mathbf{x})\big] = \mathbb{E} \bigg[  \sum_{i=1}^d x_i w_i \bigg] =   \sum_{i=1}^d x_i \mathbb{E} \big[w_i \big] = 0, \quad w_i \sim \mathcal{N}(0, 1)
\]</span></p>
<p><span class="math display">\[
\text{Var} \big[f(\mathbf{x})\big] = \text{Var}  \bigg[  \sum_{i=1}^d x_i w_i \bigg] =   \sum_{i=1}^d \text{Var} \big[ x_i w_i \big] = \sum_{i=1}^d x_i^2 \text{Var} [w_i] = \sum_{i=1}^d x_i^2
\]</span></p>
<p>We see a few things here, the mean is <span class="math inline">\(0\)</span> and the variance depends on <span class="math inline">\(x_i\)</span>, which is reasonable. However we see that the variance also depends on <span class="math inline">\(d\)</span>, the dimensionality of the input. In particular it’s <span class="math inline">\(\mathcal{O}(d)\)</span>. Why is this important? Because it means that if we increase the number of neurons at each layer in our network, the variance of the network’s predictions will also increase!</p>
<p>If our network has many neurons in each layer (large networks can have 1000’s!) the variance of outputs can be extreme, leading to poor initializations that correspond to extremely steep prediction functions. Here we can compare a few intializations from a network with just 8 neurons per layer to a network with 2.</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><img src="images/paste-14.png" class="img-fluid"></p>
<p><img src="images/paste-15.png" class="img-fluid"></p>
<p><img src="images/paste-16.png" class="img-fluid"></p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><img src="images/paste-17.png" class="img-fluid"></p>
<p><img src="images/paste-18.png" class="img-fluid"></p>
<p><img src="images/paste-19.png" class="img-fluid"></p>
</div>
</div>
<p>In practice this can make gradient descent difficult as these initialization are often very far from the minimum and the gradients are typically large, meaning small learning rates are needed to prevent divergence.</p>
<p>A better approach would keep the variance consistent no matter how many inputs there are. We can reduce the variance by dividing our initial weights by some scale factor <span class="math inline">\(s\)</span>.</p>
<p><span class="math display">\[
f(\mathbf{x}) = \sum_{i=1}^d x_i w_i\bigg(\frac{1}{s}\bigg)
\]</span></p>
<p>If we want the variance to be independent of <span class="math inline">\(d\)</span>, then we want:</p>
<p><span class="math display">\[
s = \sqrt{d}
\]</span></p>
<p>We can verify this by computing the variance:</p>
<p><span class="math display">\[
\text{Var}  \bigg[  \sum_{i=1}^d x_i w_i \bigg(\frac{1}{\sqrt{d}}\bigg) \bigg] =   \sum_{i=1}^d \text{Var} \bigg[ x_i w_i \bigg(\frac{1}{\sqrt{d}}\bigg) \bigg] = \sum_{i=1}^d x_i^2 \bigg(\frac{1}{\sqrt{d}}\bigg)^2 \text{Var} [w_i] = \frac{1}{d}\sum_{i=1}^d x_i^2
\]</span></p>
<p>This is equivalent to drawing our initial weights for each layer from a normal distribution with standard deviation equal to 1 over the square root of the number of inputs:</p>
<p><span class="math display">\[w_{i} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \forall\ w_{i} \in \mathbf{w},\ \mathbf{w}\in \mathbb{R}^{d}\]</span></p>
<p>This is known as <strong>Kaiming normal initialization</strong> (sometimes also called <strong>He initialization</strong>, after the inventor Kaiming He).</p>
<p>For neural network layers where the weights are a matrix <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d \times e}\)</span>, this works the same way:</p>
<p><span class="math display">\[w_{ij} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg) \quad \forall\ w_{ij} \in \mathbf{W},\ \mathbf{w}\in \mathbb{R}^{d \times e}\]</span></p>
<p>A popular alternative scales the distribution according to both the number of inputs and outputs of the layer:</p>
<p><span class="math display">\[w_{ij} \sim \mathcal{N}\bigg(0, \sqrt{\frac{2}{d + e}}\bigg) \quad \forall\ w_{ij} \in \mathbf{W},\ \mathbf{w}\in \mathbb{R}^{d \times e}\]</span></p>
<p>This is known as <strong>Xavier initialization</strong> (or <strong>Glorot initialization</strong> after the inventor Xavier Glorot).</p>
<p>We can compare initializations from a standard normal with initializations from a Kaiming normal.</p>
<div class="columns">
<div class="column" style="width:45%;">
<p><strong>Standard normal</strong> <span class="math inline">\(w_{i} \sim \mathcal{N}\bigg(0, 1\bigg)\)</span></p>
<p><img src="images/paste-14.png" class="img-fluid"></p>
<p><img src="images/paste-15.png" class="img-fluid"></p>
<p><img src="images/paste-16.png" class="img-fluid"></p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p><strong>Kaiming normal</strong> <span class="math inline">\(w_{i} \sim \mathcal{N}\bigg(0, \frac{1}{\sqrt{d}}\bigg)\)</span></p>
<p><img src="images/paste-24.png" class="img-fluid"></p>
<p><img src="images/paste-25.png" class="img-fluid"></p>
<p><img src="images/paste-26.png" class="img-fluid"></p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>